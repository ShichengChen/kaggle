{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Models Stacking and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf\n",
    "\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "_stdout = sys.stdout\n",
    "\n",
    "sys.path.append('modules')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from scipy.stats import iqr\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from xgboost import XGBRegressor\n",
    "from xgb_regressor import XGBoostRegressor\n",
    "from stacker import Stacker\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the final part of this capstone, we implement a simple stacker model in which we generate predictions using two models: XGBoost and MLP. Then we combine them in a single dataset and train a second level algorithm to get the final prediction.\n",
    "\n",
    "We are going to do a K-Fold stacking in which we train each model (level 0 model or just L0-model) on a subset of data, generate out-of-fold predictions and train the stacker (level 1 model, L1-model) on these predictions.\n",
    "\n",
    "As usual, use can use pretrained models and skip the heavy computation phase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_PRETRAINED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "Our methodology which I took from [MLWave Ensembling Guide](http://mlwave.com/kaggle-ensembling-guide/) as well as from the winners of [Otto Group Classification Challenge](https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov) is as follows:\n",
    "\n",
    "* **Splitting.** Split the training set into K folds\n",
    "\n",
    "\n",
    "* **Out-of-fold predictions**. Fit each L0-model on K-1 folds, generate predictions for the other fold. Repeat the process for all K folds. In the end, we get predictions for the whole training set (for which we also have labels).\n",
    "\n",
    "\n",
    "* **Fitting on the entire training set**. We fit each L0-model on the whole training set and get predictions for the test set. We combine predictions into a dataset, in which each feature is prediction of a single L0-model.\n",
    "\n",
    "\n",
    "* **Training L1**. We fit L1-model on out-of-fold predictions, while using corresponding labels from the training set as labels for L1. After that, we ask the L1-model get the final prediction using our combine dataset of L0 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one complication: we don't have the test set (we're not going to submit our prediction to Kaggle now), but we need to compare the performance of single L0-models with the stacker, as we'd like to make sure that L1-model works better than any of L0-models.\n",
    "\n",
    "To do this, we split our training dataset into train and test subsets. We will touch the test subset only once when we compare the performance of all the models (L0 vs L1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data\n",
    "We trained our models on the same train set, but the preprocessing phase has been done differently for XGBoost and MLP. We have to replicate the same preprocessing for our ensemble phase. Let's revisit what we have to do for each model:\n",
    "\n",
    "* **XGBoost**: 1) log-transform the target feature, 2) use a label encoding for categorical features.\n",
    "\n",
    "\n",
    "* **MLP**: 1) use an one-hot encoding for categorical features.\n",
    "\n",
    "I'll be verbose in this section and apply all the transformations manually without writing an abstraction (say, a function or a class). We only have two models and there's no need to abstract the logic in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see that we haven't messed up with data split:\n",
      "Training set X: (141238, 130) Y: (141238,)\n",
      "Test set X: (47080, 130) Y: (47080,)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "trainxg = pd.read_csv('train.csv')\n",
    "ntrain = trainxg.shape[0]\n",
    "\n",
    "trainxg['log_loss'] = np.log(trainxg['loss'])    \n",
    "features = [x for x in trainxg.columns if x not in ['id','loss', 'log_loss']]\n",
    "\n",
    "cat_features = [x for x in trainxg.select_dtypes(\n",
    "        include=['object']).columns if x not in ['id','loss', 'log_loss']]\n",
    "\n",
    "for c in range(len(cat_features)):\n",
    "    trainxg[cat_features[c]] = trainxg[cat_features[c]].astype('category').cat.codes\n",
    "\n",
    "trainxg_x = np.array(trainxg[features])\n",
    "trainxg_y = np.array(trainxg['log_loss'])\n",
    "\n",
    "# xg_xte, xg_yte are for the final performance check\n",
    "xg_xtr, xg_xte, xg_ytr, xg_yte = train_test_split(trainxg_x, trainxg_y, test_size=0.25, random_state=31337)\n",
    "\n",
    "print \"Let's see that we haven't messed up with data split:\"\n",
    "print \"Training set X:\", xg_xtr.shape, \"Y:\", xg_ytr.shape\n",
    "print \"Test set X:\", xg_xte.shape, \"Y:\", xg_yte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's see that we haven't messed up with data split:\n",
      "Training set X: (141238, 1153) Y: (141238,)\n",
      "Test set X: (47080, 1153) Y: (47080,)\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "trainmlp = pd.read_csv('train.csv')\n",
    "cat_names = [c for c in trainmlp.columns if 'cat' in c]\n",
    "\n",
    "trainmlp = pd.get_dummies(data=trainmlp, columns=cat_names)\n",
    "\n",
    "features_mlp = [x for x in trainmlp.columns if x not in ['id','loss']]\n",
    "\n",
    "trainmlp_x = np.array(trainmlp[features_mlp])\n",
    "trainmlp_y = np.array(trainmlp['loss'])\n",
    "\n",
    "mlp_xtr, mlp_xte, mlp_ytr, mlp_yte = train_test_split(trainmlp_x, trainmlp_y, test_size=0.25, random_state=31337)\n",
    "\n",
    "print \"Let's see that we haven't messed up with data split:\"\n",
    "print \"Training set X:\", mlp_xtr.shape, \"Y:\", mlp_ytr.shape\n",
    "print \"Test set X:\", mlp_xte.shape, \"Y:\", mlp_yte.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the L0-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we just take our final models for XGBoost and MLP. Of course, we could have found more models to add to the stacker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "reg_xgb = XGBoostRegressor(num_boost_round=200, eta=0.07, gamma=0.2, max_depth=8, min_child_weight=6,\n",
    "                colsample_bytree=0.6, subsample=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "\n",
    "def hyper_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(351, input_dim=len(features_mlp), init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.578947))\n",
    "    \n",
    "    model.add(Dense(293, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.26666))\n",
    "    \n",
    "    model.add(Dense(46, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.188888))\n",
    "    \n",
    "    model.add(Dense(1, init='glorot_normal'))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-fold predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we divide our training set `xg_xtr, xg_ytr` (xgboost), `mlp_xtr, mlp_ytr` (mlp) into k-folds (3), train on 2/3 folds and predict the third fold. We persist the results to files to get back to them later on.\n",
    "\n",
    "We also save test labels (`*_test_fold_*`) to make sure that the fold generator splits data the same fashion for XGBoost and MLP and we can stack their predictions side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "if not USE_PRETRAINED:\n",
    "    folds = KFold(len(xg_ytr), shuffle=False, n_folds=3)\n",
    "\n",
    "    for k, (train_index, test_index) in enumerate(folds):\n",
    "        xtr = xg_xtr[train_index]\n",
    "        ytr = xg_ytr[train_index]\n",
    "        xte, yte = xg_xtr[test_index], xg_ytr[test_index]\n",
    "        reg_xgb = XGBoostRegressor(num_boost_round=200, eta=0.07, gamma=0.2, max_depth=8, min_child_weight=6,\n",
    "                        colsample_bytree=0.6, subsample=0.9)\n",
    "        reg_xgb.fit(xtr, ytr)\n",
    "        np.savetxt('ensemble/xgb_pred_fold_{}.txt'.format(k), np.exp(reg_xgb.predict(xte)))\n",
    "        np.savetxt('ensemble/xgb_test_fold_{}.txt'.format(k), yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "if not USE_PRETRAINED:\n",
    "    folds = KFold(len(mlp_ytr), shuffle=False, n_folds=3)\n",
    "    for k, (train_index, test_index) in enumerate(folds):\n",
    "        xtr = mlp_xtr[train_index]\n",
    "        ytr = mlp_ytr[train_index]\n",
    "        xte, yte = mlp_xtr[test_index], mlp_ytr[test_index]\n",
    "        reg_mlp = hyper_model()\n",
    "        fit = reg_mlp.fit(xtr, ytr, batch_size=128, nb_epoch=30, verbose=0)\n",
    "        pred = reg_mlp.predict(xte, batch_size=256)\n",
    "        np.savetxt('ensemble/mlp_pred_fold_{}.txt'.format(k), pred)\n",
    "        np.savetxt('ensemble/mlp_test_fold_{}.txt'.format(k), yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the same models on the whole training set (`xg_xtr, mlp_xtr` and corresponding labels) and generate predictions for the test set (`xg_xte, mlp_xte`). Remember that we do have labels for the test set, but we don't allow our L0-model see them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "if not USE_PRETRAINED:\n",
    "    reg_xgb = XGBoostRegressor(num_boost_round=200, eta=0.07, gamma=0.2, max_depth=8, min_child_weight=6,\n",
    "                  colsample_bytree=0.6, subsample=0.9)\n",
    "    reg_xgb.fit(xg_xtr, xg_ytr)\n",
    "    np.savetxt('ensemble/xgb_pred_test.txt'.format(k), np.exp(reg_xgb.predict(xg_xte)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP\n",
    "if not USE_PRETRAINED:\n",
    "    reg_mlp = hyper_model()\n",
    "    fit = reg_mlp.fit(mlp_xtr, mlp_ytr, batch_size=128, nb_epoch=30, verbose=0)\n",
    "    pred = reg_mlp.predict(mlp_xte, batch_size=256)\n",
    "    np.savetxt('ensemble/mlp_pred_test.txt'.format(k), pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1-model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the previous stage is completed, we have generated out-of-fold and test set predictions, which we can now use to train the stacker.\n",
    "\n",
    "First, we load predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_xgb1 = np.loadtxt('ensemble/xgb_pred_fold_0.txt')\n",
    "train_xgb2 = np.loadtxt('ensemble/xgb_pred_fold_1.txt')\n",
    "train_xgb3 = np.loadtxt('ensemble/xgb_pred_fold_2.txt')\n",
    "\n",
    "train_mlp1 = np.loadtxt('ensemble/mlp_pred_fold_0.txt')\n",
    "train_mlp2 = np.loadtxt('ensemble/mlp_pred_fold_1.txt')\n",
    "train_mlp3 = np.loadtxt('ensemble/mlp_pred_fold_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check \\#1\n",
    "We load labels to check that we haven't messed up with folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgb_test_fold1 = np.exp(np.loadtxt('ensemble/xgb_test_fold_0.txt'))\n",
    "xgb_test_fold2 = np.exp(np.loadtxt('ensemble/xgb_test_fold_1.txt'))\n",
    "xgb_test_fold3 = np.exp(np.loadtxt('ensemble/xgb_test_fold_2.txt'))\n",
    "\n",
    "mlp_test_fold1 = np.loadtxt('ensemble/mlp_test_fold_0.txt')\n",
    "mlp_test_fold2 = np.loadtxt('ensemble/mlp_test_fold_1.txt')\n",
    "mlp_test_fold3 = np.loadtxt('ensemble/mlp_test_fold_2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreating the original set of training set labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_test_fold = np.hstack((xgb_test_fold1, xgb_test_fold2, xgb_test_fold3))\n",
    "mlp_test_fold = np.hstack((mlp_test_fold1, mlp_test_fold2, mlp_test_fold3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And testing that these labels completely match (there's still a little rounding error due to log-exp conversion):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.118142715102431e-12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(xgb_test_fold, mlp_test_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basically zero. Labels from two fold generators match and we can go on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check \\#2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the predictions for the whole test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_xgb = np.loadtxt('ensemble/xgb_pred_test.txt')\n",
    "test_mlp = np.loadtxt('ensemble/mlp_pred_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should check that MAE from combined out-of-fold predictions is reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combined out-of-fold predictions for XGBoost and MLP\n",
    "\n",
    "train_xgb_folds = np.hstack((train_xgb1, train_xgb2, train_xgb3))\n",
    "train_mlp_folds = np.hstack((train_mlp1, train_mlp2, train_mlp3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152.2760489657664"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE of XGBoost combined predictions\n",
    "\n",
    "mean_absolute_error(np.exp(xg_ytr), train_xgb_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1146.5263643278126"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE of MLP combined predictions\n",
    "\n",
    "mean_absolute_error(mlp_ytr, train_mlp_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are around 1150-1155: this is exactly what we expect from these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for the test set and get scores for each single model. The score of the best single model is our baseline score for the stacker (our stacker should perform better than any given single model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE which we need to improve with stacking, XGB: 1149.19888471; MLP: 1145.49726607.\n"
     ]
    }
   ],
   "source": [
    "mae_xgb_test = mean_absolute_error(np.exp(xg_yte), test_xgb)\n",
    "mae_mlp_test = mean_absolute_error(mlp_yte, test_mlp)\n",
    "print \"Baseline MAE which we need to improve with stacking, XGB: {}; MLP: {}.\".format(mae_xgb_test, mae_mlp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions on test should be close to 1145-1150."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training L1-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to join the predictions of L0-models and train a stacker over them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_train_x = np.vstack((train_xgb_folds, train_mlp_folds)).T\n",
    "l1_test_x = np.vstack((test_xgb, test_mlp)).T\n",
    "l1_train_y = mlp_ytr\n",
    "l1_test_y = mlp_yte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape: (141238, 2)\n",
      "ytrain shape: (141238,)\n",
      "Xtest shape: (47080, 2)\n",
      "ytest shape: (47080,)\n"
     ]
    }
   ],
   "source": [
    "# Just a sanity check\n",
    "print \"Xtrain shape:\", l1_train_x.shape\n",
    "print \"ytrain shape:\", l1_train_y.shape\n",
    "print \"Xtest shape:\", l1_test_x.shape\n",
    "print \"ytest shape:\", l1_test_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of the algorithm for L1 is crucial. The problem is that we don't want it overfit on the training set, and it can be very easily done. To reduce the possibility of overfitting, we should take a very simple regressor, and `LinearRegression` is the ideal candidate for us.\n",
    "\n",
    "We now fit a very basic linear regression and get the predictions for the final test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "\n",
    "# Note that normalizing the data in case of linear models is very important\n",
    "reg.fit(np.log(l1_train_x), np.log(l1_train_y))\n",
    "pred = reg.predict(np.log(l1_test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for XGB: 1149.19888471\n",
      "MAE for MLP: 1145.49726607\n",
      "MAE for stacker: 1136.21813333\n"
     ]
    }
   ],
   "source": [
    "mae_stacker = mean_absolute_error(l1_test_y, np.exp(pred))\n",
    "\n",
    "print \"MAE for XGB:\", mae_xgb_test\n",
    "print \"MAE for MLP:\", mae_mlp_test\n",
    "print \"MAE for stacker:\", mae_stacker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the following results:\n",
    "\n",
    "`MAE for XGB: 1149.19888471\n",
    "MAE for MLP: 1145.49726607\n",
    "MAE for stacker: 1136.21813333`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we improved our results dramatically. Thus, our score for a single stacker is `MAE=1136.2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to note, the coefficients of the linear combination, provided by our linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59121805,  0.41705347])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be interpreted this way:\n",
    "\n",
    "PREDICTION = 0.59 \\* XGB_PREDICTION + 0.41 \\* MLP_PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should validate our final model. We may encounter the problem that the model generalizes well in the specific case of our train-test split, but fails to generalize if it was trained on a different subset of data.\n",
    "\n",
    "In this section, we train several stackers on different splits of data and make sure that all of them provide a consistent, stable performance and this performance is considerably better than the score of single L0-models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate the process, we use mostly the same stacking technique, but I abstracted the stacker in a class `Stacker`. There are several issues to be highlighted:\n",
    "\n",
    "* We pass functions of our models instead of the instances of models themselves. This is needed since the models should be initialized clean several times during training (e.g., for out-of-fold predictions) and I'd like to abstract the logic of the stacker class from the logic of single models' initialization.\n",
    "\n",
    "\n",
    "* We may choose a different seed during stacker initialization. Thus we can shuffle the train-test split to see how our models generalize on different parts of dataset.\n",
    "\n",
    "\n",
    "* The purpose of this class is solely to automate the evaluation and generalization of our stacker. This is done to be DRY and not to introduce more excessive code in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We'll initialize the stacker on five different seeds and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seeds = [1,2,4,8,16]\n",
    "final_scores = []\n",
    "\n",
    "# Preparing XGBoost model function\n",
    "xgbst = lambda _: XGBoostRegressor(num_boost_round=200, eta=0.07, gamma=0.2, max_depth=8, min_child_weight=6,\n",
    "                colsample_bytree=0.6, subsample=0.9)\n",
    "\n",
    "# Preparing MLP model function. We need to pass input dimension into\n",
    "# this function, which is done in Stacker class\n",
    "def hyper_model(dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(351, input_dim=dim, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.578947))\n",
    "    \n",
    "    model.add(Dense(293, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.26666))\n",
    "    \n",
    "    model.add(Dense(46, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.188888))\n",
    "    \n",
    "    model.add(Dense(1, init='glorot_normal'))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also prepare parameters for fitting MLP and predictions (amount of epochs, batch sizes) and pipeline training output to `stacker_validation.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('stacker_validation.txt', 'a')\n",
    "_stdout = sys.stdout\n",
    "sys.stdout = f\n",
    "\n",
    "mlp_fit_params = {'nb_epoch': 30, 'batch_size': 128, 'verbose': 1}\n",
    "mlp_pred_params = {'batch_size': 256, 'verbose': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`stack_and_compare` returns a dictionary of final scores for both single L0-models as well as for the stacker. We initialize stackers with seeds we prepared and run stacking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not USE_PRETRAINED:\n",
    "    for seed in seeds:\n",
    "        stacker = Stacker(xgbst, hyper_model, train_path='train.csv', seed=seed,\n",
    "                      mlp_fit_kwargs=mlp_fit_params, mlp_predict_kwargs=mlp_pred_params)\n",
    "        score = stacker.stack_and_compare()\n",
    "        final_scores.append(score)\n",
    "        print \"Seed {} completed with scores :: {}\".format(seed, score)\n",
    "else:\n",
    "    with open('pretrained/stacker_seeds_scores.pkl', 'rb') as f:\n",
    "        final_scores = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.stdout = _stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED: 1\n",
      "SCORE: {'mlp': 1137.4518808596861, 'xgb': 1141.3274823341596, 'stacker': 1128.5183831793099}\n",
      "\n",
      "SEED: 2\n",
      "SCORE: {'mlp': 1138.605777626057, 'xgb': 1145.4124181185377, 'stacker': 1131.0467572871137}\n",
      "\n",
      "SEED: 4\n",
      "SCORE: {'mlp': 1139.5123466472771, 'xgb': 1145.2325510240682, 'stacker': 1130.3417696000593}\n",
      "\n",
      "SEED: 8\n",
      "SCORE: {'mlp': 1141.0668336039034, 'xgb': 1145.1971588199588, 'stacker': 1132.1655095762851}\n",
      "\n",
      "SEED: 16\n",
      "SCORE: {'mlp': 1136.586960351832, 'xgb': 1140.7780777321607, 'stacker': 1127.505237791496}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seed, score in zip(seeds, final_scores):\n",
    "    print \"SEED: {}\".format(seed)\n",
    "    print \"SCORE: {}\".format(score)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the L0-models provide very close results from seed to seed. L1-model is better in all cases: its MAE are lower, the scores are tighter clustered. To measure this fact more precisely, we calculate means and standard deviations for L0 and L1-models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean and standard deviation calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(final_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mlp</th>\n",
       "      <th>stacker</th>\n",
       "      <th>xgb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1138.644760</td>\n",
       "      <td>1129.915531</td>\n",
       "      <td>1143.589538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.752011</td>\n",
       "      <td>1.889794</td>\n",
       "      <td>2.325297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1136.586960</td>\n",
       "      <td>1127.505238</td>\n",
       "      <td>1140.778078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1137.451881</td>\n",
       "      <td>1128.518383</td>\n",
       "      <td>1141.327482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1138.605778</td>\n",
       "      <td>1130.341770</td>\n",
       "      <td>1145.197159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1139.512347</td>\n",
       "      <td>1131.046757</td>\n",
       "      <td>1145.232551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1141.066834</td>\n",
       "      <td>1132.165510</td>\n",
       "      <td>1145.412418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mlp      stacker          xgb\n",
       "count     5.000000     5.000000     5.000000\n",
       "mean   1138.644760  1129.915531  1143.589538\n",
       "std       1.752011     1.889794     2.325297\n",
       "min    1136.586960  1127.505238  1140.778078\n",
       "25%    1137.451881  1128.518383  1141.327482\n",
       "50%    1138.605778  1130.341770  1145.197159\n",
       "75%    1139.512347  1131.046757  1145.232551\n",
       "max    1141.066834  1132.165510  1145.412418"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "\n",
    "* The score of all models as well as the stacker's is very consistent and stable (std are low).\n",
    "\n",
    "\n",
    "* Stacker's std is lower than std of XGBoost, and just a little higher than the std of MLP. Since stacker's performance depends on the performance of single models, its tendency to keep a low standard deviation is a very good feature.\n",
    "\n",
    "\n",
    "* Stacker's scores are better for each seed, its mean is lower. The worst stacker's score (1132) is still better than the best score of a single model (MLP, 1136.58).\n",
    "\n",
    "\n",
    "We may now conclude that the stacker indeed outperforms any of our two models: XGBoost and MLP. We take `MAE=1129.9` (average of five stackers) as our final score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement over baseline scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remember that we have the following baseline scores:\n",
    "\n",
    "* `MAE=1217.52` — the baseline score of a Random Forest trained by Allstate competition arrangers. With our final model, we got **7.2%** score improvement.\n",
    "\n",
    "* `MAE=1190.73` — the baseline score of a single simple model we trained (MLP).\n",
    "\n",
    "To understand, if we got a significant improvement over those scores, let's add each of those scores to our stacker scores list and find out whether it can be called an outlier. Thus, if this baseline score can be considered an outlier, the difference between our final scores and the baseline is significant.\n",
    "\n",
    "To run this test, we can calclucate [IQR](https://en.wikipedia.org/wiki/Interquartile_range) which is used for anomaly or outlier detection. We then calculate third quantile of data (Q3) and use the formula `Q3 + 1.5 * IQR` to set the upper score margin. Values higher than this margin are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE = 1217.52 is considered an outlier.\n",
      "MAE = 1190.73 is considered an outlier.\n"
     ]
    }
   ],
   "source": [
    "for baseline in [1217.52, 1190.73]:\n",
    "    stacker_scores = list(scores.stacker)\n",
    "    stacker_scores.append(baseline)\n",
    "    max_margin = np.percentile(stacker_scores, 75) + 1.5*iqr(stacker_scores)\n",
    "    if baseline - max_margin > 0:\n",
    "        print 'MAE =', baseline, 'is considered an outlier.'\n",
    "    else:\n",
    "        print 'MAE = ', baseline, 'is NOT an outlier.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, all baseline scores are considered outliers — we may say that our final model indeed provided a visible score improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
