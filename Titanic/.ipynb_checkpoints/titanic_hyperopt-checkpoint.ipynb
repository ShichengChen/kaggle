{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "14630296-b1aa-759e-bafa-b6a73f3896ed",
    "_execution_state": "idle",
    "_uuid": "2e37a274400cfeb472b6405d524325245588dd66"
   },
   "outputs": [],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "import warnings\n",
    "from matplotlib.pyplot import ylim\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "import hpsklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=2,random_state=42)\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "  \n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Survived', u'Sex', u'Fare', u'SibSp0', u'SibSp1', u'SibSp2',\n",
      "       u'Parch0', u'Parch1', u'Parch2', u'Parch3', u'Age_(-10, 0]',\n",
      "       u'Age_(0, 10]', u'Fsize0', u'Fsize1', u'Fsize2', u'Fsize3', u'Fsize4',\n",
      "       u'name_(10, 20]', u'name_(30, 40]', u'name_(40, 50]', u'name_(50, 100]',\n",
      "       u'Title_Master', u'Title_girl', u'Title_men', u'Title_women',\n",
      "       u'Cabin_B', u'Cabin_C', u'Cabin_D', u'Cabin_E', u'Cabin_X', u'T_1',\n",
      "       u'T_2', u'T_3', u'T_A', u'T_P', u'Em_C', u'Em_S', u'Pc_1', u'Pc_2',\n",
      "       u'Pc_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('featureEngineering.csv',index_col=0)\n",
    "test = pd.read_csv('test.csv')\n",
    "PassengerId = test['PassengerId']\n",
    "train_len = len(pd.read_csv('train.csv'))\n",
    "print dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "968cbd7f-80b2-7f8d-2ad6-b68b3aeae671",
    "_uuid": "95fdd1e8cd9f23cfcef3bed92511da084a323c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 39)\n",
      "(891,)\n",
      "(418, 39)\n"
     ]
    }
   ],
   "source": [
    "pdtest = dataset[train_len:].drop(labels=[\"Survived\"],axis = 1)\n",
    "y_train = dataset[:train_len][\"Survived\"].values.astype(int)\n",
    "pdx_train = dataset[:train_len].drop(labels = [\"Survived\"],axis = 1)\n",
    "\n",
    "test = pdtest.values\n",
    "x_train = pdx_train.values\n",
    "\n",
    "#x_train, test, y_train, ytest = train_test_split(x_train, y_train, test_size=0.33, random_state=42)\n",
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print test.shape\n",
    "#print xtrain.shape\n",
    "#print ytrain.shape\n",
    "#print xtest.shape\n",
    "#print ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hpsklearn import HyperoptEstimator, any_sparse_classifier, tfidf\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn import metrics\n",
    "from hyperopt import tpe\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.pyll.stochastic\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: 0.810323978435 {'n_estimators': 573, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 13, 'max_depth': 12, 'min_samples_leaf': 8}\n",
      "new best: 0.812573688719 {'n_estimators': 578, 'max_features': 9, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 6, 'max_depth': 5, 'min_samples_leaf': 2}\n",
      "new best: 0.81480072555 {'n_estimators': 532, 'max_features': 9, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 17, 'max_depth': 3, 'min_samples_leaf': 6}\n",
      "new best: 0.817042878017 {'n_estimators': 509, 'max_features': 9, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 17, 'max_depth': 3, 'min_samples_leaf': 6}\n",
      "new best: 0.818168992795 {'n_estimators': 514, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 7, 'max_depth': 3, 'min_samples_leaf': 4}\n",
      "new best: 0.819290069028 {'n_estimators': 577, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 7, 'max_depth': 3, 'min_samples_leaf': 3}\n",
      "new best: 0.8192925883 {'n_estimators': 514, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 7, 'max_depth': 3, 'min_samples_leaf': 3}\n",
      "new best: 0.821537260039 {'n_estimators': 591, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 7, 'max_depth': 3, 'min_samples_leaf': 3}\n",
      "{'n_jobs': 0, 'min_samples_leaf': 1, 'n_estimators': 91, 'min_samples_split': 5, 'criterion': 0, 'max_features': 6, 'max_depth': 2}\n",
      "1070.90807104\n"
     ]
    }
   ],
   "source": [
    "time0= time.time()\n",
    "cv = StratifiedKFold(n_splits=2,random_state=42)\n",
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(4,12)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf',range(2,20)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(500,600)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\"]),\n",
    "    'n_jobs': hp.choice('n_jobs', [-1]), \n",
    "    'min_samples_split': hp.choice('min_samples_split',range(2,20))\n",
    "}\n",
    "maxacc=0\n",
    "def f(params):\n",
    "    clf = RandomForestClassifier(**params)\n",
    "    acc = cross_val_score(clf, x_train, y_train,cv=cv,n_jobs=-1).mean()\n",
    "    global maxacc\n",
    "    if acc > maxacc:\n",
    "        maxacc = acc\n",
    "        print 'new best:', maxacc, params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "print fmin(f, space4rf, algo=tpe.suggest, max_evals=500, trials=trials)\n",
    "print time.time()-time0\n",
    "#10 - 1min\n",
    "#600 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: 0.796860986547 {'n_estimators': 500, 'max_features': 4, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 16, 'max_depth': 11, 'min_samples_leaf': 12}\n",
      "new best: 0.797987101325 {'n_estimators': 503, 'max_features': 8, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 19, 'max_depth': 18, 'min_samples_leaf': 14}\n",
      "new best: 0.806963269008 {'n_estimators': 503, 'max_features': 5, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 4, 'max_depth': 9, 'min_samples_leaf': 2}\n",
      "new best: 0.809205421474 {'n_estimators': 505, 'max_features': 7, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 5, 'max_depth': 12, 'min_samples_leaf': 3}\n",
      "new best: 0.813699803497 {'n_estimators': 501, 'max_features': 11, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 11, 'max_depth': 14, 'min_samples_leaf': 7}\n",
      "new best: 0.813702322769 {'n_estimators': 509, 'max_features': 11, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 5, 'max_depth': 12, 'min_samples_leaf': 7}\n",
      "new best: 0.814815841185 {'n_estimators': 509, 'max_features': 11, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 5, 'max_depth': 12, 'min_samples_leaf': 16}\n",
      "new best: 0.815936917418 {'n_estimators': 500, 'max_features': 11, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 12, 'max_depth': 8, 'min_samples_leaf': 16}\n",
      "new best: 0.817060512924 {'n_estimators': 503, 'max_features': 11, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 17, 'max_depth': 18, 'min_samples_leaf': 12}\n",
      "new best: 0.817070590014 {'n_estimators': 506, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 6, 'max_depth': 6, 'min_samples_leaf': 4}\n",
      "new best: 0.820418703079 {'n_estimators': 508, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 3, 'min_samples_leaf': 5}\n",
      "new best: 0.82715775684 {'n_estimators': 508, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'min_samples_leaf': 5}\n",
      "{'n_jobs': 0, 'min_samples_leaf': 3, 'n_estimators': 8, 'min_samples_split': 0, 'criterion': 0, 'max_features': 6, 'max_depth': 4}\n",
      "1943.75710917\n"
     ]
    }
   ],
   "source": [
    "time0= time.time()\n",
    "space4et = {\n",
    "    'max_features': hp.choice('max_features', range(4,12)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(500,510)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\"]),\n",
    "    \"max_depth\": hp.choice('max_depth', range(1,20)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf',range(2,20)),\n",
    "     'n_jobs': hp.choice('n_jobs', [-1]),  \n",
    "    'min_samples_split': hp.choice('min_samples_split',range(2,20))\n",
    "}\n",
    "maxacc=0\n",
    "def f(params):\n",
    "    clf = ExtraTreesClassifier(**params)\n",
    "    acc = cross_val_score(clf, x_train, y_train,cv=cv,n_jobs=-1).mean()\n",
    "    global maxacc\n",
    "    if acc > maxacc:\n",
    "        maxacc = acc\n",
    "        print 'new best:', maxacc, params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best= fmin(f, space4et, algo=tpe.suggest, max_evals=500, trials=trials)\n",
    "print best\n",
    "print time.time()-time0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: 0.790132009876 {'n_estimators': 637, 'loss': 'deviance', 'max_features': 5, 'min_samples_split': 14, 'learning_rate': 0.0019306977288832496, 'max_depth': 3, 'min_samples_leaf': 17}\n",
      "new best: 0.815954552325 {'n_estimators': 675, 'loss': 'deviance', 'max_features': 11, 'min_samples_split': 8, 'learning_rate': 0.012648552168552958, 'max_depth': 16, 'min_samples_leaf': 18}\n",
      "new best: 0.817073109286 {'n_estimators': 651, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 8, 'learning_rate': 0.0086851137375135203, 'max_depth': 13, 'min_samples_leaf': 14}\n",
      "new best: 0.821547337129 {'n_estimators': 604, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 12, 'learning_rate': 0.0086851137375135203, 'max_depth': 13, 'min_samples_leaf': 9}\n",
      "new best: 0.821549856401 {'n_estimators': 787, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 12, 'learning_rate': 0.0086851137375135203, 'max_depth': 13, 'min_samples_leaf': 9}\n",
      "new best: 0.822668413362 {'n_estimators': 684, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 14, 'learning_rate': 0.0086851137375135203, 'max_depth': 9, 'min_samples_leaf': 9}\n",
      "new best: 0.822670932635 {'n_estimators': 688, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 10, 'learning_rate': 0.0071968567300115215, 'max_depth': 13, 'min_samples_leaf': 9}\n",
      "new best: 0.823792008868 {'n_estimators': 547, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 12, 'learning_rate': 0.010481131341546858, 'max_depth': 13, 'min_samples_leaf': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process PoolWorker-14192:\n",
      "Traceback (most recent call last):\n",
      "Process PoolWorker-14191:\n",
      "  File \"/home/csc/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/csc/anaconda2/lib/python2.7/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/csc/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "  File \"/home/csc/anaconda2/lib/python2.7/multiprocessing/process.py\", line 114, in run\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b16906a0600e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace4gb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtime0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/fmin.pyc\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/hyperopt/base.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-b16906a0600e>\u001b[0m in \u001b[0;36mf\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mmaxacc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxacc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m    319\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             return_times=True)\n\u001b[0;32m--> 195\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/multiprocessing/pool.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csc/anaconda2/lib/python2.7/threading.pyc\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m__debug__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_note\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.wait(): got it\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time0= time.time()\n",
    "space4gb = {\n",
    "    'max_features': hp.choice('max_features', range(4,12)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(500,800)),\n",
    "    'learning_rate': hp.choice('learning_rate',np.logspace(-3,-1)),\n",
    "    \"max_depth\": hp.choice('max_depth', range(1,20)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf',range(2,20)),\n",
    "    'loss' : hp.choice('loss',[\"deviance\"]),\n",
    "    'min_samples_split': hp.choice('min_samples_split',range(2,20))\n",
    "}\n",
    "maxacc=0\n",
    "def f(params):\n",
    "    clf = GradientBoostingClassifier(**params)\n",
    "    acc = cross_val_score(clf, x_train, y_train,cv=cv,n_jobs=-1).mean()\n",
    "    global maxacc\n",
    "    if acc > maxacc:\n",
    "        maxacc = acc\n",
    "        print 'new best:', maxacc, params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best =fmin(f, space4gb, algo=tpe.suggest, max_evals=500, trials=trials)\n",
    "print best\n",
    "print time.time()-time0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(gamma=np.logspace(-3, 2, 100), \n",
    "                  C=np.logspace(0.1, 3, 1000),\n",
    "                  kernel=['rbf'])\n",
    "\n",
    "gridsvc2 = GridSearchCV(SVC(), param_grid=param_grid, scoring='accuracy',cv=cv,n_jobs=-1, verbose = 1)\n",
    "gridsvc2.fit(x_train, y_train)\n",
    "print(\"The best parameters are %s with a score of %0.8f\"% \\\n",
    "      (gridsvc2.best_params_, gridsvc2.best_score_))\n",
    "\n",
    "#print gridsvc2.cv_results_['param_C']\n",
    "#print gridsvc2.cv_results_['param_gamma']\n",
    "#print gridsvc2.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = sns.barplot(x='param_C',y='mean_test_score',hue='param_gamma',data=gridsvc2.cv_results_)\n",
    "for tick in g.get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "g.legend(loc=\"lower right\", bbox_to_anchor=(1.5, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "adaDTC = AdaBoostClassifier(DecisionTreeClassifier(), random_state=7)\n",
    "ada_param_grid = {\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[400, 700, 1000],\n",
    "              \"learning_rate\":  [0.0001,0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, \n",
    "                        cv=cv, \n",
    "                        scoring='accuracy', n_jobs= -1, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(x_train,y_train)\n",
    "\n",
    "print gsadaDTC.best_score_\n",
    "print gsadaDTC.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "406d0494-1d0c-3126-19d9-bc53127c4249",
    "_uuid": "46a93dc062e973832cecd50246d0d7581aafb02b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "n_splits = 2\n",
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((x_train.shape[0],))\n",
    "    skf = StratifiedKFold(n_splits=n_splits,random_state=42)\n",
    "    skf.get_n_splits(x_train,y_train)\n",
    "    oof_test = np.zeros((x_test.shape[0],))\n",
    "\n",
    "    for train_index, test_index in skf.split(x_train,y_train):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test += clf.predict(x_test)\n",
    "        \n",
    "\n",
    "    oof_test = oof_test / n_splits\n",
    "    print accuracy_score(y_train,oof_train.reshape(-1))\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79bd2a86-82e2-648a-e816-9660e89794ad",
    "_uuid": "114750e2d5e4fdd234ccd8647fc349463a56fa09"
   },
   "outputs": [],
   "source": [
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':500,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "et_params={'n_estimators': 549, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 18, \n",
    "           'max_depth': 4, 'min_samples_leaf': 17}\n",
    "et_params={'n_estimators': 508, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 2, 'max_depth': 5, 'min_samples_leaf': 5}\n",
    "et = ExtraTreesClassifier(**et_params)\n",
    "et_oof_train, et_oof_test = get_oof(et,x_train,y_train,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params1={'n_estimators': 503, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 19, 'max_depth': 4, 'min_samples_leaf': 2}\n",
    "et1 = ExtraTreesClassifier(**et_params1)\n",
    "et_oof_train1, et_oof_test1 = get_oof(et1,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot_learning_curve(et,\"et learning curves\",x_train,y_train,cv=cv,ylim=ylim(0.75,0.9),\n",
    "                       train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "rf_params = {'n_estimators': 591, 'max_features': 10, 'n_jobs': -1, 'criterion': 'gini', 'min_samples_split': 7, 'max_depth': 3, 'min_samples_leaf': 3}\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params1={'max_features': 14, 'n_estimators': 500, 'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 6}\n",
    "rf1 = RandomForestClassifier(**rf_params1)\n",
    "rf_oof_train1, rf_oof_test1 = get_oof(rf1,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params2={'max_features': 12, 'n_estimators': 502, 'criterion': 'gini', 'max_depth': 7, 'min_samples_leaf': 4}\n",
    "rf2 = RandomForestClassifier(**rf_params2)\n",
    "rf_oof_train2, rf_oof_test2 = get_oof(rf2,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot_learning_curve(rf,\"RF mearning curves\",x_train,y_train,cv=cv,train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "'''ada_params = {'n_estimators': 700, 'learning_rate': 0.2, \n",
    " 'algorithm': 'SAMME'}'''\n",
    "ada = AdaBoostClassifier(**ada_params)\n",
    "ada_oof_train, ada_oof_test = get_oof(ada,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot_learning_curve(ada,\"AdaBoost learning curves\",x_train,y_train,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params = {\n",
    "    'n_estimators': 500,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 0\n",
    "}\n",
    "#More trees is always better with diminishing returns. Deeper trees are almost always better\n",
    "#subject to requiring more trees for similar performance.\n",
    "\n",
    "#The above two points are directly a result of the bias-variance tradeoff. \n",
    "#Deeper trees reduces the bias; more trees reduces the variance.\n",
    "gb_params={'n_estimators': 503, 'loss': 'deviance', 'max_features': 13, 'learning_rate': 0.018420699693267154, 'max_depth': 3, 'min_samples_leaf': 2}\n",
    "gb_params={'n_estimators': 547, 'loss': 'deviance', 'max_features': 10, 'min_samples_split': 12, 'learning_rate': 0.010481131341546858, 'max_depth': 13, 'min_samples_leaf': 9}\n",
    "gb = GradientBoostingClassifier(**gb_params)\n",
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_params1={'n_estimators': 501, 'loss': 'deviance', 'max_features': 15, 'learning_rate': 0.0015998587196060573, 'max_depth': 9, 'min_samples_leaf': 10}\n",
    "gb1 = GradientBoostingClassifier(**gb_params1)\n",
    "gb_oof_train1, gb_oof_test1 = get_oof(gb1,x_train,y_train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = plot_learning_curve(gb,\"GradientBoosting learning curves\",x_train,y_train,cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {'kernel': 'rbf', 'C': 13.219411484660288, 'gamma': 0.0030538555088334154}\n",
    "svc_params = {'kernel': 'rbf', 'C': 1.8947460938388148, 'gamma': 0.086851137375135293}\n",
    "svc = SVC(**svc_params)\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,x_train,y_train,test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot_learning_curve(svc,\"SVC learning curves\",x_train,y_train,cv=cv,\n",
    "                        train_sizes=np.linspace(.1, 1.0, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3f292e65-fe8a-d662-6ace-41a19866d671",
    "_uuid": "4cf41b3d9a541c9d39b645a66c8f1116eaf76861"
   },
   "source": [
    "**Feature importances generated from the different classifiers**\n",
    "\n",
    "Now having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n",
    "\n",
    "As per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ed9cf8b5-95a4-d974-fb11-592214949d1f",
    "_uuid": "b3b0356c8bef0dceb5fcfa7fb7a11359010b2098"
   },
   "outputs": [],
   "source": [
    "rf_features = rf.feature_importances_\n",
    "et_features = et.feature_importances_\n",
    "ada_features = ada.feature_importances_\n",
    "gb_features = gb.feature_importances_\n",
    "'''print len(gb_features)\n",
    "print gb_features\n",
    "print len(ada_features)\n",
    "print et_features\n",
    "print rf_features'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "635a063f-281d-66d4-6572-587ebecd6b4b",
    "_uuid": "6f68b3033a8f185f61d83e80323c2486024f5d4d"
   },
   "outputs": [],
   "source": [
    "cols = dataset.columns.values[1:]\n",
    "# Create a dataframe with features\n",
    "feature_dataframe = pd.DataFrame( {'features': cols,\n",
    "     'rf': rf_features,\n",
    "     'et': et_features,\n",
    "      'ada': ada_features,\n",
    "    'gb': gb_features\n",
    "    })\n",
    "feature_dataframe = feature_dataframe[['features','rf','et','ada','gb']]\n",
    "#feature_dataframe = feature_dataframe[['features','rf','et','gb']]\n",
    "feature_dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06b9f410-f93e-0206-b029-24df035eea2b",
    "_uuid": "7e25675f239b0ab008e0264917abff497795681a"
   },
   "source": [
    "**Interactive feature importances via Plotly scatterplots**\n",
    "\n",
    "I'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "553828f0-c994-5ee1-695f-9373f11a1a7b",
    "_uuid": "9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74"
   },
   "source": [
    "Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06847850-a829-0858-b12c-7b66e53e030a",
    "_uuid": "f611812e2c9de3773df2264dfb2b13c0995807ac"
   },
   "outputs": [],
   "source": [
    "# Create the new column containing the average of values\n",
    "\n",
    "feature_dataframe['mean'] = feature_dataframe.mean(axis= 1)\n",
    "feature_dataframe.sort_values('et',ascending=False)\n",
    "important = feature_dataframe.sort_values('mean',ascending=False)\n",
    "#print important\n",
    "important['features'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax=plt.subplots(4,1,figsize=(30,60))\n",
    "_.set_size_inches(10, 60)\n",
    "for i,classify in enumerate(important.columns.values[1:5]):\n",
    "    sns.barplot(y='features',x=classify,data=important,ax=ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6685fa11-497f-3fc2-ab1f-97f92d6eca61",
    "_uuid": "fef365199854ca3fff754399b4699d941b7e43b8"
   },
   "outputs": [],
   "source": [
    "print et_oof_train.shape\n",
    "x_train_stack = np.concatenate(( et_oof_train,et_oof_train1, \n",
    "                                rf_oof_train,rf_oof_train1,rf_oof_train2,\n",
    "                                gb_oof_train,gb_oof_train1,\n",
    "                               ada_oof_train,svc_oof_train), axis=1)\n",
    "x_test_stack = np.concatenate(( et_oof_test,et_oof_test1, \n",
    "                               rf_oof_test,rf_oof_test1,rf_oof_test2,\n",
    "                               gb_oof_test,gb_oof_test1,\n",
    "                              ada_oof_test,svc_oof_test), axis=1)\n",
    "print x_train_stack.shape\n",
    "print x_test_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6685fa11-497f-3fc2-ab1f-97f92d6eca61",
    "_uuid": "fef365199854ca3fff754399b4699d941b7e43b8"
   },
   "outputs": [],
   "source": [
    "print et_oof_train.shape\n",
    "x_train_stack = np.concatenate(( et_oof_train, \n",
    "                                rf_oof_train,\n",
    "                                gb_oof_train,\n",
    "                               ada_oof_train,svc_oof_train), axis=1)\n",
    "x_test_stack = np.concatenate(( et_oof_test, \n",
    "                               rf_oof_test,\n",
    "                               gb_oof_test,\n",
    "                              ada_oof_test,svc_oof_test), axis=1)\n",
    "print x_train_stack.shape\n",
    "print x_test_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print et_oof_train.shape\n",
    "x_train_stack = np.concatenate(( et_oof_train, \n",
    "                                rf_oof_train,gb_oof_train), axis=1)\n",
    "x_test_stack = np.concatenate(( et_oof_test, \n",
    "                               rf_oof_test,gb_oof_test), axis=1)\n",
    "print x_train_stack.shape\n",
    "print x_test_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space4xgb = {\n",
    "    #'max_features': hp.choice('max_features', range(4,20)),\n",
    "    'n_estimators': hp.choice('n_estimators', [2000]),\n",
    "    'learning_rate': hp.choice('learning_rate',np.logspace(-2,-1)),\n",
    "    \"max_depth\": hp.choice('max_depth', range(3,10)),\n",
    "    'min_child_weight': hp.choice('min_samples_leaf',range(2,5)),\n",
    "    'gamma':hp.choice('gamma',[0.8,0.9,1.0]),\n",
    "    'colsample_bytree':hp.choice('colsample_bytree',[0.8]),\n",
    "    'subsample':hp.choice('subsample',[0.8]),\n",
    "    'objective':hp.choice('objective',['binary:logistic']),\n",
    "     'nthread':hp.choice('nthread',[-1]),\n",
    "     'scale_pos_weight':hp.choice('scale_pos_weight',[1])\n",
    "}\n",
    "\n",
    "best = 0\n",
    "def f(params):\n",
    "    global best\n",
    "    clf = xgb.XGBClassifier(**params)\n",
    "    acc = cross_val_score(clf, x_train_stack, y_train,cv=cv,n_jobs=-1).mean()\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "        print 'new best:', best, params\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, space4xgb, algo=tpe.suggest, max_evals=180, trials=trials)\n",
    "print 'best:'\n",
    "print best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramxgb={'colsample_bytree': 0.8, 'scale_pos_weight': 1, 'learning_rate': 0.0517947467923121, 'nthread': -1, 'min_child_weight': 9, 'n_estimators': 2000, 'subsample': 0.8, 'objective': 'binary:logistic', 'max_depth': 3, 'gamma': 0.9}\n",
    "gbm = xgb.XGBClassifier(**paramxgb)\n",
    "gbm.fit(x_train_stack, y_train)\n",
    "predictions = gbm.predict(x_test_stack)\n",
    "StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': predictions })\n",
    "StackingSubmission.to_csv(\"titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "3a7c7517-b9a3-3a21-3a7b-299ca37c6843",
    "_uuid": "5155d370069fe6de0fe5105309342ce55130dae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833894500561\n"
     ]
    }
   ],
   "source": [
    "gbm = xgb.XGBClassifier(\n",
    " learning_rate = 0.1,\n",
    " n_estimators= 2000,\n",
    " max_depth= 4,\n",
    " min_child_weight= 2,\n",
    " #gamma=1,\n",
    " gamma=0.9,                        \n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread= -1,\n",
    " scale_pos_weight=1).fit(x_train_stack, y_train)\n",
    "print accuracy_score(gbm.predict(x_train_stack),y_train)\n",
    "predictions = gbm.predict(x_test_stack)\n",
    "StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': predictions })\n",
    "StackingSubmission.to_csv(\"titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print accuracy_score(ytest,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 500,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 0\n",
    "}\n",
    "rf = RandomForestClassifier(**rf_params)\n",
    "rf.fit(x_train,y_train)\n",
    "rfpredict = rf.predict(test)\n",
    "\n",
    "StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': rf_oof_test.reshape(-1).astype(int)})\n",
    "StackingSubmission.to_csv(\"titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': rf_oof_test.reshape(-1).astype(int)})\n",
    "StackingSubmission.to_csv(\"titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e56c738-b8f3-95e4-d642-c483f9757ed8",
    "_uuid": "9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5"
   },
   "source": [
    "**Steps for Further Improvement**\n",
    "\n",
    "As a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n",
    "\n",
    "Some additional steps that may be taken to improve one's score could be:\n",
    "\n",
    " 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n",
    " 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score."
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
