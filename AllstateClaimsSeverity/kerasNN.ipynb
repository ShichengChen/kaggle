{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3, Multilayer Perceptron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csc/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/csc/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "'''gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))'''\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1153)\n",
      "(188318,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((188318, 1153), (188318,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "cat_names = [c for c in train.columns if 'cat' in c]\n",
    "\n",
    "train = pd.get_dummies(data=train, columns=cat_names)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['id','loss']]\n",
    "\n",
    "train_x = np.array(train[features])\n",
    "\n",
    "ntrain = train_x.shape[0]\n",
    "\n",
    "train_y = np.log(train['loss'] + 200)\n",
    "\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)\n",
    "train_x.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=30, verbose=1, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtr, xte, ytr, yte = train_test_split(train_x, train_y, test_size=0.25, random_state=31337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cdn.rawgit.com/dnkirill/allstate_capstone/master/images/mlp3.svg\"></td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y), np.exp(yhat))\n",
    "\n",
    "def mae_score(y_true, y_pred):\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "mae_scorer = make_scorer(mae_score, greater_is_better=False)\n",
    "\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.exp(y_true) - K.exp(y_pred)))\n",
    "\n",
    "def mean_pred_metrics(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.exp(y_true) - K.exp(y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=100, verbose=1, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyper_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(351, input_dim=train_x.shape[1], init='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.578947))\n",
    "    \n",
    "    model.add(Dense(293, init='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.26666))\n",
    "    \n",
    "    model.add(Dense(46, init='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.188888))\n",
    "    \n",
    "    model.add(Dense(1, init='he_normal'))\n",
    "    model.compile(optimizer='adam',\n",
    "              loss=mean_pred,metrics=[mean_pred_metrics])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(351, input_dim=1153, kernel_initializer=\"he_normal\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/csc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(293, kernel_initializer=\"he_normal\")`\n",
      "  \n",
      "/home/csc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(46, kernel_initializer=\"he_normal\")`\n",
      "  del sys.path[0]\n",
      "/home/csc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"he_normal\")`\n",
      "/home/csc/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100436 samples, validate on 25109 samples\n",
      "Epoch 1/100\n",
      "100436/100436 [==============================] - 9s - loss: 3210.1322 - mean_pred_metrics: 3210.1322 - val_loss: 3095.4986 - val_mean_pred_metrics: 3095.4986\n",
      "Epoch 2/100\n",
      "100436/100436 [==============================] - 7s - loss: 2724.3428 - mean_pred_metrics: 2724.3428 - val_loss: 1932.3458 - val_mean_pred_metrics: 1932.3458\n",
      "Epoch 3/100\n",
      "100436/100436 [==============================] - 7s - loss: 2205.4983 - mean_pred_metrics: 2205.4983 - val_loss: 1854.8780 - val_mean_pred_metrics: 1854.8780\n",
      "Epoch 4/100\n",
      "100436/100436 [==============================] - 7s - loss: 2081.3217 - mean_pred_metrics: 2081.3217 - val_loss: 1695.6310 - val_mean_pred_metrics: 1695.6310\n",
      "Epoch 5/100\n",
      "100436/100436 [==============================] - 8s - loss: 1942.1458 - mean_pred_metrics: 1942.1458 - val_loss: 1565.2848 - val_mean_pred_metrics: 1565.2848\n",
      "Epoch 6/100\n",
      "100436/100436 [==============================] - 7s - loss: 1860.2303 - mean_pred_metrics: 1860.2303 - val_loss: 1476.8009 - val_mean_pred_metrics: 1476.8009\n",
      "Epoch 7/100\n",
      "100436/100436 [==============================] - 7s - loss: 1808.3640 - mean_pred_metrics: 1808.3640 - val_loss: 1599.0924 - val_mean_pred_metrics: 1599.0924\n",
      "Epoch 8/100\n",
      "100436/100436 [==============================] - 7s - loss: 1772.5211 - mean_pred_metrics: 1772.5211 - val_loss: 1449.6119 - val_mean_pred_metrics: 1449.6119\n",
      "Epoch 9/100\n",
      "100436/100436 [==============================] - 7s - loss: 1753.5202 - mean_pred_metrics: 1753.5202 - val_loss: 1401.8683 - val_mean_pred_metrics: 1401.8683\n",
      "Epoch 10/100\n",
      "100436/100436 [==============================] - 7s - loss: 1716.8362 - mean_pred_metrics: 1716.8362 - val_loss: 1407.0938 - val_mean_pred_metrics: 1407.0938\n",
      "Epoch 11/100\n",
      "100436/100436 [==============================] - 7s - loss: 1677.5695 - mean_pred_metrics: 1677.5695 - val_loss: 1359.0393 - val_mean_pred_metrics: 1359.0393\n",
      "Epoch 12/100\n",
      "100436/100436 [==============================] - 7s - loss: 1659.6933 - mean_pred_metrics: 1659.6933 - val_loss: 1248.7220 - val_mean_pred_metrics: 1248.7220\n",
      "Epoch 13/100\n",
      "100436/100436 [==============================] - 7s - loss: 1635.1861 - mean_pred_metrics: 1635.1861 - val_loss: 1393.8967 - val_mean_pred_metrics: 1393.8967\n",
      "Epoch 14/100\n",
      "100436/100436 [==============================] - 7s - loss: 1613.7112 - mean_pred_metrics: 1613.7112 - val_loss: 1312.2167 - val_mean_pred_metrics: 1312.2167\n",
      "Epoch 15/100\n",
      "100436/100436 [==============================] - 7s - loss: 1590.5231 - mean_pred_metrics: 1590.5231 - val_loss: 1372.2427 - val_mean_pred_metrics: 1372.2427\n",
      "Epoch 16/100\n",
      "100436/100436 [==============================] - 7s - loss: 1573.6608 - mean_pred_metrics: 1573.6608 - val_loss: 1373.6602 - val_mean_pred_metrics: 1373.6602\n",
      "Epoch 17/100\n",
      "100436/100436 [==============================] - 7s - loss: 1555.0730 - mean_pred_metrics: 1555.0730 - val_loss: 1325.7523 - val_mean_pred_metrics: 1325.7523\n",
      "Epoch 18/100\n",
      "100436/100436 [==============================] - 7s - loss: 1531.5387 - mean_pred_metrics: 1531.5387 - val_loss: 1262.3367 - val_mean_pred_metrics: 1262.3367\n",
      "Epoch 19/100\n",
      "100436/100436 [==============================] - 7s - loss: 1504.8109 - mean_pred_metrics: 1504.8109 - val_loss: 1314.9789 - val_mean_pred_metrics: 1314.9789\n",
      "Epoch 20/100\n",
      "100436/100436 [==============================] - 7s - loss: 1489.2571 - mean_pred_metrics: 1489.2571 - val_loss: 1201.5892 - val_mean_pred_metrics: 1201.5892\n",
      "Epoch 21/100\n",
      "100436/100436 [==============================] - 7s - loss: 1471.5419 - mean_pred_metrics: 1471.5419 - val_loss: 1204.9153 - val_mean_pred_metrics: 1204.9153\n",
      "Epoch 22/100\n",
      "100436/100436 [==============================] - 7s - loss: 1443.9332 - mean_pred_metrics: 1443.9332 - val_loss: 1282.6272 - val_mean_pred_metrics: 1282.6272\n",
      "Epoch 23/100\n",
      "100436/100436 [==============================] - 7s - loss: 1428.9428 - mean_pred_metrics: 1428.9428 - val_loss: 1255.3266 - val_mean_pred_metrics: 1255.3266\n",
      "Epoch 24/100\n",
      "100436/100436 [==============================] - 7s - loss: 1407.5693 - mean_pred_metrics: 1407.5693 - val_loss: 1202.6771 - val_mean_pred_metrics: 1202.6771\n",
      "Epoch 25/100\n",
      "100436/100436 [==============================] - 7s - loss: 1397.3190 - mean_pred_metrics: 1397.3190 - val_loss: 1255.9866 - val_mean_pred_metrics: 1255.9866\n",
      "Epoch 26/100\n",
      "100436/100436 [==============================] - 7s - loss: 1384.8554 - mean_pred_metrics: 1384.8554 - val_loss: 1227.0145 - val_mean_pred_metrics: 1227.0145\n",
      "Epoch 27/100\n",
      "100436/100436 [==============================] - 7s - loss: 1358.4705 - mean_pred_metrics: 1358.4705 - val_loss: 1174.7189 - val_mean_pred_metrics: 1174.7189\n",
      "Epoch 28/100\n",
      "100436/100436 [==============================] - 7s - loss: 1345.4836 - mean_pred_metrics: 1345.4836 - val_loss: 1183.3940 - val_mean_pred_metrics: 1183.3940\n",
      "Epoch 29/100\n",
      "100436/100436 [==============================] - 7s - loss: 1332.1338 - mean_pred_metrics: 1332.1338 - val_loss: 1209.9642 - val_mean_pred_metrics: 1209.9642\n",
      "Epoch 30/100\n",
      "100436/100436 [==============================] - 7s - loss: 1314.2266 - mean_pred_metrics: 1314.2266 - val_loss: 1171.0790 - val_mean_pred_metrics: 1171.0790\n",
      "Epoch 31/100\n",
      "100436/100436 [==============================] - 7s - loss: 1298.9838 - mean_pred_metrics: 1298.9838 - val_loss: 1179.3215 - val_mean_pred_metrics: 1179.3215\n",
      "Epoch 32/100\n",
      "100436/100436 [==============================] - 7s - loss: 1280.6861 - mean_pred_metrics: 1280.6861 - val_loss: 1237.2707 - val_mean_pred_metrics: 1237.2707\n",
      "Epoch 33/100\n",
      "100436/100436 [==============================] - 7s - loss: 1274.4782 - mean_pred_metrics: 1274.4782 - val_loss: 1144.3932 - val_mean_pred_metrics: 1144.3932\n",
      "Epoch 34/100\n",
      "100436/100436 [==============================] - 7s - loss: 1259.8631 - mean_pred_metrics: 1259.8631 - val_loss: 1156.7561 - val_mean_pred_metrics: 1156.7561\n",
      "Epoch 35/100\n",
      "100436/100436 [==============================] - 7s - loss: 1247.4042 - mean_pred_metrics: 1247.4042 - val_loss: 1150.5423 - val_mean_pred_metrics: 1150.5423\n",
      "Epoch 36/100\n",
      "100436/100436 [==============================] - 7s - loss: 1233.3850 - mean_pred_metrics: 1233.3850 - val_loss: 1180.5249 - val_mean_pred_metrics: 1180.5249\n",
      "Epoch 37/100\n",
      "100436/100436 [==============================] - 7s - loss: 1222.7030 - mean_pred_metrics: 1222.7030 - val_loss: 1148.9286 - val_mean_pred_metrics: 1148.9286\n",
      "Epoch 38/100\n",
      "100436/100436 [==============================] - 7s - loss: 1212.3901 - mean_pred_metrics: 1212.3901 - val_loss: 1146.4709 - val_mean_pred_metrics: 1146.4709\n",
      "Epoch 39/100\n",
      "100436/100436 [==============================] - 7s - loss: 1203.6436 - mean_pred_metrics: 1203.6436 - val_loss: 1156.2996 - val_mean_pred_metrics: 1156.2996\n",
      "Epoch 40/100\n",
      "100436/100436 [==============================] - 7s - loss: 1196.9744 - mean_pred_metrics: 1196.9744 - val_loss: 1140.7932 - val_mean_pred_metrics: 1140.7932\n",
      "Epoch 41/100\n",
      "100436/100436 [==============================] - 7s - loss: 1184.9413 - mean_pred_metrics: 1184.9413 - val_loss: 1143.3520 - val_mean_pred_metrics: 1143.3520\n",
      "Epoch 42/100\n",
      "100436/100436 [==============================] - 7s - loss: 1175.2839 - mean_pred_metrics: 1175.2839 - val_loss: 1147.1644 - val_mean_pred_metrics: 1147.1644\n",
      "Epoch 43/100\n",
      "100436/100436 [==============================] - 7s - loss: 1167.6440 - mean_pred_metrics: 1167.6440 - val_loss: 1158.1981 - val_mean_pred_metrics: 1158.1981\n",
      "Epoch 44/100\n",
      "100436/100436 [==============================] - 7s - loss: 1161.5907 - mean_pred_metrics: 1161.5907 - val_loss: 1144.6931 - val_mean_pred_metrics: 1144.6931\n",
      "Epoch 45/100\n",
      "100436/100436 [==============================] - 7s - loss: 1152.7022 - mean_pred_metrics: 1152.7022 - val_loss: 1151.9788 - val_mean_pred_metrics: 1151.9788\n",
      "Epoch 46/100\n",
      "100436/100436 [==============================] - 7s - loss: 1148.7159 - mean_pred_metrics: 1148.7159 - val_loss: 1144.6867 - val_mean_pred_metrics: 1144.6867\n",
      "Epoch 47/100\n",
      "100436/100436 [==============================] - 7s - loss: 1140.9877 - mean_pred_metrics: 1140.9877 - val_loss: 1142.9985 - val_mean_pred_metrics: 1142.9985\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100436/100436 [==============================] - 7s - loss: 1136.6872 - mean_pred_metrics: 1136.6872 - val_loss: 1151.2071 - val_mean_pred_metrics: 1151.2071\n",
      "Epoch 49/100\n",
      "100436/100436 [==============================] - 7s - loss: 1125.8715 - mean_pred_metrics: 1125.8715 - val_loss: 1160.0568 - val_mean_pred_metrics: 1160.0568\n",
      "Epoch 50/100\n",
      "100436/100436 [==============================] - 7s - loss: 1124.8675 - mean_pred_metrics: 1124.8675 - val_loss: 1157.1427 - val_mean_pred_metrics: 1157.1427\n",
      "Epoch 51/100\n",
      "100436/100436 [==============================] - 7s - loss: 1116.8743 - mean_pred_metrics: 1116.8743 - val_loss: 1147.1448 - val_mean_pred_metrics: 1147.1448\n",
      "Fold 0, MAE: 0.37658946331957105\n",
      "Train on 100436 samples, validate on 25109 samples\n",
      "Epoch 1/100\n",
      "100436/100436 [==============================] - 9s - loss: 3213.2251 - mean_pred_metrics: 3213.2251 - val_loss: 3130.6405 - val_mean_pred_metrics: 3130.6405\n",
      "Epoch 2/100\n",
      "100436/100436 [==============================] - 7s - loss: 2734.1874 - mean_pred_metrics: 2734.1874 - val_loss: 2595.9772 - val_mean_pred_metrics: 2595.9772\n",
      "Epoch 3/100\n",
      "100436/100436 [==============================] - 7s - loss: 2384.1952 - mean_pred_metrics: 2384.1952 - val_loss: 2119.2749 - val_mean_pred_metrics: 2119.2749\n",
      "Epoch 4/100\n",
      "100436/100436 [==============================] - 7s - loss: 2212.3364 - mean_pred_metrics: 2212.3364 - val_loss: 1901.8821 - val_mean_pred_metrics: 1901.8821\n",
      "Epoch 5/100\n",
      "100436/100436 [==============================] - 7s - loss: 2072.5000 - mean_pred_metrics: 2072.5000 - val_loss: 1668.1689 - val_mean_pred_metrics: 1668.1689\n",
      "Epoch 6/100\n",
      "100436/100436 [==============================] - 7s - loss: 1984.5311 - mean_pred_metrics: 1984.5311 - val_loss: 1642.0227 - val_mean_pred_metrics: 1642.0227\n",
      "Epoch 7/100\n",
      "100436/100436 [==============================] - 7s - loss: 1926.0909 - mean_pred_metrics: 1926.0909 - val_loss: 1432.0780 - val_mean_pred_metrics: 1432.0780\n",
      "Epoch 8/100\n",
      "100436/100436 [==============================] - 7s - loss: 1879.3539 - mean_pred_metrics: 1879.3539 - val_loss: 1596.1522 - val_mean_pred_metrics: 1596.1522\n",
      "Epoch 9/100\n",
      "100436/100436 [==============================] - 7s - loss: 1843.2504 - mean_pred_metrics: 1843.2504 - val_loss: 1451.9196 - val_mean_pred_metrics: 1451.9196\n",
      "Epoch 10/100\n",
      "100436/100436 [==============================] - 7s - loss: 1800.1520 - mean_pred_metrics: 1800.1520 - val_loss: 1460.6583 - val_mean_pred_metrics: 1460.6583\n",
      "Epoch 11/100\n",
      "100436/100436 [==============================] - 7s - loss: 1759.4222 - mean_pred_metrics: 1759.4222 - val_loss: 1443.7795 - val_mean_pred_metrics: 1443.7795\n",
      "Epoch 12/100\n",
      "100436/100436 [==============================] - 7s - loss: 1727.9528 - mean_pred_metrics: 1727.9528 - val_loss: 1470.5793 - val_mean_pred_metrics: 1470.5793\n",
      "Epoch 13/100\n",
      "100436/100436 [==============================] - 7s - loss: 1694.7601 - mean_pred_metrics: 1694.7601 - val_loss: 1321.6277 - val_mean_pred_metrics: 1321.6277\n",
      "Epoch 14/100\n",
      "100436/100436 [==============================] - 7s - loss: 1677.6900 - mean_pred_metrics: 1677.6900 - val_loss: 1407.0700 - val_mean_pred_metrics: 1407.0700\n",
      "Epoch 15/100\n",
      "100436/100436 [==============================] - 7s - loss: 1642.4341 - mean_pred_metrics: 1642.4341 - val_loss: 1352.5068 - val_mean_pred_metrics: 1352.5068\n",
      "Epoch 16/100\n",
      "100436/100436 [==============================] - 7s - loss: 1626.6786 - mean_pred_metrics: 1626.6786 - val_loss: 1314.4664 - val_mean_pred_metrics: 1314.4664\n",
      "Epoch 17/100\n",
      "100436/100436 [==============================] - 7s - loss: 1593.4556 - mean_pred_metrics: 1593.4556 - val_loss: 1244.4776 - val_mean_pred_metrics: 1244.4776\n",
      "Epoch 18/100\n",
      "100436/100436 [==============================] - 7s - loss: 1575.8056 - mean_pred_metrics: 1575.8056 - val_loss: 1372.9465 - val_mean_pred_metrics: 1372.9465\n",
      "Epoch 19/100\n",
      "100436/100436 [==============================] - 7s - loss: 1553.7421 - mean_pred_metrics: 1553.7421 - val_loss: 1278.4182 - val_mean_pred_metrics: 1278.4182\n",
      "Epoch 20/100\n",
      "100436/100436 [==============================] - 7s - loss: 1526.3857 - mean_pred_metrics: 1526.3857 - val_loss: 1282.1802 - val_mean_pred_metrics: 1282.1802\n",
      "Epoch 21/100\n",
      "100436/100436 [==============================] - 7s - loss: 1501.2586 - mean_pred_metrics: 1501.2586 - val_loss: 1250.5146 - val_mean_pred_metrics: 1250.5146\n",
      "Epoch 22/100\n",
      "100436/100436 [==============================] - 7s - loss: 1482.8639 - mean_pred_metrics: 1482.8639 - val_loss: 1202.8865 - val_mean_pred_metrics: 1202.8865\n",
      "Epoch 23/100\n",
      "100436/100436 [==============================] - 7s - loss: 1459.8491 - mean_pred_metrics: 1459.8491 - val_loss: 1183.0911 - val_mean_pred_metrics: 1183.0911\n",
      "Epoch 24/100\n",
      "100436/100436 [==============================] - 7s - loss: 1442.0085 - mean_pred_metrics: 1442.0085 - val_loss: 1216.7193 - val_mean_pred_metrics: 1216.7193\n",
      "Epoch 25/100\n",
      "100436/100436 [==============================] - 7s - loss: 1415.5680 - mean_pred_metrics: 1415.5680 - val_loss: 1220.7145 - val_mean_pred_metrics: 1220.7145\n",
      "Epoch 26/100\n",
      "100436/100436 [==============================] - 7s - loss: 1402.0632 - mean_pred_metrics: 1402.0632 - val_loss: 1207.2496 - val_mean_pred_metrics: 1207.2496\n",
      "Epoch 27/100\n",
      "100436/100436 [==============================] - 7s - loss: 1378.3878 - mean_pred_metrics: 1378.3878 - val_loss: 1163.7796 - val_mean_pred_metrics: 1163.7796\n",
      "Epoch 28/100\n",
      "100436/100436 [==============================] - 7s - loss: 1363.3142 - mean_pred_metrics: 1363.3142 - val_loss: 1190.0052 - val_mean_pred_metrics: 1190.0052\n",
      "Epoch 29/100\n",
      "100436/100436 [==============================] - 7s - loss: 1337.9867 - mean_pred_metrics: 1337.9867 - val_loss: 1153.0151 - val_mean_pred_metrics: 1153.0151\n",
      "Epoch 30/100\n",
      "100436/100436 [==============================] - 7s - loss: 1326.5103 - mean_pred_metrics: 1326.5103 - val_loss: 1179.0950 - val_mean_pred_metrics: 1179.0950\n",
      "Epoch 31/100\n",
      "100436/100436 [==============================] - 7s - loss: 1308.7931 - mean_pred_metrics: 1308.7931 - val_loss: 1181.0552 - val_mean_pred_metrics: 1181.0552\n",
      "Epoch 32/100\n",
      "100436/100436 [==============================] - 7s - loss: 1292.1094 - mean_pred_metrics: 1292.1094 - val_loss: 1177.7163 - val_mean_pred_metrics: 1177.7163\n",
      "Epoch 33/100\n",
      "100436/100436 [==============================] - 7s - loss: 1278.1987 - mean_pred_metrics: 1278.1987 - val_loss: 1145.6265 - val_mean_pred_metrics: 1145.6265\n",
      "Epoch 34/100\n",
      "100436/100436 [==============================] - 7s - loss: 1261.9277 - mean_pred_metrics: 1261.9277 - val_loss: 1152.3613 - val_mean_pred_metrics: 1152.3613\n",
      "Epoch 35/100\n",
      "100436/100436 [==============================] - 7s - loss: 1248.8796 - mean_pred_metrics: 1248.8796 - val_loss: 1148.4832 - val_mean_pred_metrics: 1148.4832\n",
      "Epoch 36/100\n",
      "100436/100436 [==============================] - 7s - loss: 1233.6191 - mean_pred_metrics: 1233.6191 - val_loss: 1139.7905 - val_mean_pred_metrics: 1139.7905\n",
      "Epoch 37/100\n",
      "100436/100436 [==============================] - 7s - loss: 1227.4770 - mean_pred_metrics: 1227.4770 - val_loss: 1138.7469 - val_mean_pred_metrics: 1138.7469\n",
      "Epoch 38/100\n",
      "100436/100436 [==============================] - 7s - loss: 1219.2977 - mean_pred_metrics: 1219.2977 - val_loss: 1158.2665 - val_mean_pred_metrics: 1158.2665\n",
      "Epoch 39/100\n",
      "100436/100436 [==============================] - 6s - loss: 1202.2825 - mean_pred_metrics: 1202.2825 - val_loss: 1138.0802 - val_mean_pred_metrics: 1138.0802\n",
      "Epoch 40/100\n",
      "100436/100436 [==============================] - 6s - loss: 1196.8004 - mean_pred_metrics: 1196.8004 - val_loss: 1184.1114 - val_mean_pred_metrics: 1184.1114\n",
      "Epoch 41/100\n",
      "100436/100436 [==============================] - 6s - loss: 1185.2667 - mean_pred_metrics: 1185.2667 - val_loss: 1138.8475 - val_mean_pred_metrics: 1138.8475\n",
      "Epoch 42/100\n",
      "100436/100436 [==============================] - 6s - loss: 1181.0093 - mean_pred_metrics: 1181.0093 - val_loss: 1144.4251 - val_mean_pred_metrics: 1144.4251\n",
      "Epoch 43/100\n",
      "100436/100436 [==============================] - 6s - loss: 1168.9901 - mean_pred_metrics: 1168.9901 - val_loss: 1142.4293 - val_mean_pred_metrics: 1142.4293\n",
      "Epoch 44/100\n"
      "Epoch 1/30\n",
      "100436/100436 [==============================] - 19s 185us/step - loss: 3225.7151 - mean_absolute_error: 7.0840 - mean_pred_metrics: 3225.7151 - val_loss: 3180.2746 - val_mean_absolute_error: 6.7966 - val_mean_pred_metrics: 3180.2746\n",
      "Epoch 2/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 2816.3621 - mean_absolute_error: 4.2023 - mean_pred_metrics: 2816.3621 - val_loss: 2037.0752 - val_mean_absolute_error: 1.1061 - val_mean_pred_metrics: 2037.0752\n",
      "Epoch 3/30\n",
      "100436/100436 [==============================] - 10s 99us/step - loss: 2085.1639 - mean_absolute_error: 0.9580 - mean_pred_metrics: 2085.1639 - val_loss: 1654.8122 - val_mean_absolute_error: 0.5962 - val_mean_pred_metrics: 1654.8122\n",
      "Epoch 4/30\n",
      "100436/100436 [==============================] - 10s 101us/step - loss: 1967.2538 - mean_absolute_error: 0.8401 - mean_pred_metrics: 1967.2538 - val_loss: 1621.5311 - val_mean_absolute_error: 0.6085 - val_mean_pred_metrics: 1621.5311\n",
      "Epoch 5/30\n",
      "100436/100436 [==============================] - 10s 100us/step - loss: 1896.8837 - mean_absolute_error: 0.7810 - mean_pred_metrics: 1896.8837 - val_loss: 1462.3234 - val_mean_absolute_error: 0.5410 - val_mean_pred_metrics: 1462.3234\n",
      "Epoch 6/30\n",
      "100436/100436 [==============================] - 10s 98us/step - loss: 1841.4296 - mean_absolute_error: 0.7426 - mean_pred_metrics: 1841.4296 - val_loss: 1420.6821 - val_mean_absolute_error: 0.4822 - val_mean_pred_metrics: 1420.6821\n",
      "Epoch 7/30\n",
      "100436/100436 [==============================] - 10s 98us/step - loss: 1780.0215 - mean_absolute_error: 0.7031 - mean_pred_metrics: 1780.0215 - val_loss: 1366.0216 - val_mean_absolute_error: 0.4960 - val_mean_pred_metrics: 1366.0216\n",
      "Epoch 8/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1742.1456 - mean_absolute_error: 0.6783 - mean_pred_metrics: 1742.1456 - val_loss: 1281.0043 - val_mean_absolute_error: 0.4619 - val_mean_pred_metrics: 1281.0043\n",
      "Epoch 9/30\n",
      "100436/100436 [==============================] - 9s 94us/step - loss: 1698.3276 - mean_absolute_error: 0.6490 - mean_pred_metrics: 1698.3276 - val_loss: 1421.3704 - val_mean_absolute_error: 0.4727 - val_mean_pred_metrics: 1421.3704\n",
      "Epoch 10/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1668.5058 - mean_absolute_error: 0.6291 - mean_pred_metrics: 1668.5058 - val_loss: 1466.0231 - val_mean_absolute_error: 0.4916 - val_mean_pred_metrics: 1466.0231\n",
      "Epoch 11/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1634.1041 - mean_absolute_error: 0.6059 - mean_pred_metrics: 1634.1041 - val_loss: 1377.0999 - val_mean_absolute_error: 0.4715 - val_mean_pred_metrics: 1377.0999\n",
      "Epoch 12/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1600.4670 - mean_absolute_error: 0.5854 - mean_pred_metrics: 1600.4670 - val_loss: 1289.9137 - val_mean_absolute_error: 0.4295 - val_mean_pred_metrics: 1289.9137\n",
      "Epoch 13/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1578.5898 - mean_absolute_error: 0.5707 - mean_pred_metrics: 1578.5898 - val_loss: 1235.6598 - val_mean_absolute_error: 0.4079 - val_mean_pred_metrics: 1235.6598\n",
      "Epoch 14/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1557.2660 - mean_absolute_error: 0.5558 - mean_pred_metrics: 1557.2660 - val_loss: 1300.6645 - val_mean_absolute_error: 0.4307 - val_mean_pred_metrics: 1300.6645\n",
      "Epoch 15/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1525.7913 - mean_absolute_error: 0.5403 - mean_pred_metrics: 1525.7913 - val_loss: 1310.1213 - val_mean_absolute_error: 0.4299 - val_mean_pred_metrics: 1310.1213\n",
      "Epoch 16/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1502.6640 - mean_absolute_error: 0.5235 - mean_pred_metrics: 1502.6640 - val_loss: 1228.8090 - val_mean_absolute_error: 0.3979 - val_mean_pred_metrics: 1228.8090\n",
      "Epoch 17/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1481.1512 - mean_absolute_error: 0.5133 - mean_pred_metrics: 1481.1512 - val_loss: 1209.6038 - val_mean_absolute_error: 0.3950 - val_mean_pred_metrics: 1209.6038\n",
      "Epoch 18/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1458.5969 - mean_absolute_error: 0.5002 - mean_pred_metrics: 1458.5969 - val_loss: 1227.8175 - val_mean_absolute_error: 0.3990 - val_mean_pred_metrics: 1227.8175\n",
      "Epoch 19/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1439.6998 - mean_absolute_error: 0.4905 - mean_pred_metrics: 1439.6998 - val_loss: 1198.6220 - val_mean_absolute_error: 0.3914 - val_mean_pred_metrics: 1198.6220\n",
      "Epoch 20/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1419.5396 - mean_absolute_error: 0.4797 - mean_pred_metrics: 1419.5396 - val_loss: 1183.8292 - val_mean_absolute_error: 0.3850 - val_mean_pred_metrics: 1183.8292\n",
      "Epoch 21/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1401.6004 - mean_absolute_error: 0.4721 - mean_pred_metrics: 1401.6004 - val_loss: 1236.2229 - val_mean_absolute_error: 0.3991 - val_mean_pred_metrics: 1236.2229\n",
      "Epoch 22/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1381.0307 - mean_absolute_error: 0.4631 - mean_pred_metrics: 1381.0307 - val_loss: 1172.6044 - val_mean_absolute_error: 0.3814 - val_mean_pred_metrics: 1172.6044\n",
      "Epoch 23/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1362.4176 - mean_absolute_error: 0.4546 - mean_pred_metrics: 1362.4176 - val_loss: 1166.8301 - val_mean_absolute_error: 0.3808 - val_mean_pred_metrics: 1166.8301\n",
      "Epoch 24/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1344.8619 - mean_absolute_error: 0.4466 - mean_pred_metrics: 1344.8619 - val_loss: 1183.6780 - val_mean_absolute_error: 0.3859 - val_mean_pred_metrics: 1183.6780\n",
      "Epoch 25/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1328.8521 - mean_absolute_error: 0.4401 - mean_pred_metrics: 1328.8521 - val_loss: 1178.7331 - val_mean_absolute_error: 0.3840 - val_mean_pred_metrics: 1178.7331\n",
      "Epoch 26/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1313.3579 - mean_absolute_error: 0.4333 - mean_pred_metrics: 1313.3579 - val_loss: 1186.9510 - val_mean_absolute_error: 0.3846 - val_mean_pred_metrics: 1186.9510\n",
      "Epoch 27/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1294.6866 - mean_absolute_error: 0.4256 - mean_pred_metrics: 1294.6866 - val_loss: 1145.7572 - val_mean_absolute_error: 0.3747 - val_mean_pred_metrics: 1145.7572\n",
      "Epoch 28/30\n",
      "100436/100436 [==============================] - 9s 91us/step - loss: 1285.1687 - mean_absolute_error: 0.4221 - mean_pred_metrics: 1285.1687 - val_loss: 1204.8454 - val_mean_absolute_error: 0.3874 - val_mean_pred_metrics: 1204.8454\n",
      "Epoch 29/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1272.3220 - mean_absolute_error: 0.4157 - mean_pred_metrics: 1272.3220 - val_loss: 1152.9344 - val_mean_absolute_error: 0.3767 - val_mean_pred_metrics: 1152.9344\n",
      "Epoch 30/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1257.9494 - mean_absolute_error: 0.4117 - mean_pred_metrics: 1257.9494 - val_loss: 1173.9713 - val_mean_absolute_error: 0.3812 - val_mean_pred_metrics: 1173.9713\n",
      "Fold 0, MAE: 0.382326654651738\n",
      "Train on 100436 samples, validate on 25109 samples\n",
      "Epoch 1/30\n",
      "100436/100436 [==============================] - 9s 95us/step - loss: 3080.9145 - mean_absolute_error: 6.3088 - mean_pred_metrics: 3080.9145 - val_loss: 2867.0915 - val_mean_absolute_error: 5.7247 - val_mean_pred_metrics: 2867.0915\n",
      "Epoch 2/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 2440.7502 - mean_absolute_error: 3.0312 - mean_pred_metrics: 2440.7502 - val_loss: 2048.1291 - val_mean_absolute_error: 1.2670 - val_mean_pred_metrics: 2048.1291\n",
      "Epoch 3/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 2058.7136 - mean_absolute_error: 0.9637 - mean_pred_metrics: 2058.7136 - val_loss: 1629.6158 - val_mean_absolute_error: 0.5893 - val_mean_pred_metrics: 1629.6158\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100436/100436 [==============================] - 6s - loss: 1162.7145 - mean_pred_metrics: 1162.7145 - val_loss: 1141.0320 - val_mean_pred_metrics: 1141.0320\n",
      "Epoch 45/100\n",
      "100436/100436 [==============================] - 6s - loss: 1152.8343 - mean_pred_metrics: 1152.8343 - val_loss: 1141.9597 - val_mean_pred_metrics: 1141.9597\n",
      "Epoch 46/100\n",
      "100436/100436 [==============================] - 6s - loss: 1146.4018 - mean_pred_metrics: 1146.4018 - val_loss: 1143.4258 - val_mean_pred_metrics: 1143.4258\n",
      "Epoch 47/100\n",
      "100436/100436 [==============================] - 6s - loss: 1135.1554 - mean_pred_metrics: 1135.1554 - val_loss: 1146.0390 - val_mean_pred_metrics: 1146.0390\n",
      "Epoch 48/100\n",
      "100436/100436 [==============================] - 6s - loss: 1133.1676 - mean_pred_metrics: 1133.1676 - val_loss: 1142.4095 - val_mean_pred_metrics: 1142.4095\n",
      "Epoch 49/100\n",
      "100436/100436 [==============================] - 6s - loss: 1127.3076 - mean_pred_metrics: 1127.3076 - val_loss: 1147.7889 - val_mean_pred_metrics: 1147.7889\n",
      "Epoch 50/100\n",
      "100436/100436 [==============================] - 6s - loss: 1120.7744 - mean_pred_metrics: 1120.7744 - val_loss: 1146.1406 - val_mean_pred_metrics: 1146.1406\n",
      "Fold 1, MAE: 0.3765862143452659\n",
      "Train on 100436 samples, validate on 25110 samples\n",
      "Epoch 1/100\n",
      "100436/100436 [==============================] - 8s - loss: 2902.3228 - mean_pred_metrics: 2902.3228 - val_loss: 2223.8122 - val_mean_pred_metrics: 2223.8122\n",
      "Epoch 2/100\n",
      "100436/100436 [==============================] - 6s - loss: 2123.5234 - mean_pred_metrics: 2123.5234 - val_loss: 1690.4882 - val_mean_pred_metrics: 1690.4882\n",
      "Epoch 3/100\n",
      "100436/100436 [==============================] - 6s - loss: 1984.2888 - mean_pred_metrics: 1984.2888 - val_loss: 1609.6049 - val_mean_pred_metrics: 1609.6049\n",
      "Epoch 4/100\n",
      "100436/100436 [==============================] - 6s - loss: 1920.0652 - mean_pred_metrics: 1920.0652 - val_loss: 1534.6062 - val_mean_pred_metrics: 1534.6062\n",
      "Epoch 5/100\n",
      "100436/100436 [==============================] - 6s - loss: 1874.8589 - mean_pred_metrics: 1874.8589 - val_loss: 1503.4772 - val_mean_pred_metrics: 1503.4772\n",
      "Epoch 6/100\n",
      "100436/100436 [==============================] - 6s - loss: 1821.5298 - mean_pred_metrics: 1821.5298 - val_loss: 1540.9977 - val_mean_pred_metrics: 1540.9977\n",
      "Epoch 7/100\n",
      "100436/100436 [==============================] - 6s - loss: 1775.2193 - mean_pred_metrics: 1775.2193 - val_loss: 1451.8208 - val_mean_pred_metrics: 1451.8208\n",
      "Epoch 8/100\n",
      "100436/100436 [==============================] - 6s - loss: 1734.7795 - mean_pred_metrics: 1734.7795 - val_loss: 1500.2708 - val_mean_pred_metrics: 1500.2708\n",
      "Epoch 9/100\n",
      "100436/100436 [==============================] - 6s - loss: 1696.9892 - mean_pred_metrics: 1696.9892 - val_loss: 1360.2473 - val_mean_pred_metrics: 1360.2473\n",
      "Epoch 10/100\n",
      "100436/100436 [==============================] - 6s - loss: 1672.8470 - mean_pred_metrics: 1672.8470 - val_loss: 1370.4835 - val_mean_pred_metrics: 1370.4835\n",
      "Epoch 11/100\n",
      "100436/100436 [==============================] - 6s - loss: 1634.1106 - mean_pred_metrics: 1634.1106 - val_loss: 1294.5571 - val_mean_pred_metrics: 1294.5571\n",
      "Epoch 12/100\n",
      "100436/100436 [==============================] - 6s - loss: 1610.7867 - mean_pred_metrics: 1610.7867 - val_loss: 1289.7414 - val_mean_pred_metrics: 1289.7414\n",
      "Epoch 13/100\n",
      "100436/100436 [==============================] - 6s - loss: 1571.0746 - mean_pred_metrics: 1571.0746 - val_loss: 1253.1364 - val_mean_pred_metrics: 1253.1364\n",
      "Epoch 14/100\n",
      "100436/100436 [==============================] - 6s - loss: 1553.3554 - mean_pred_metrics: 1553.3554 - val_loss: 1226.0576 - val_mean_pred_metrics: 1226.0576\n",
      "Epoch 15/100\n",
      "100436/100436 [==============================] - 6s - loss: 1518.7110 - mean_pred_metrics: 1518.7110 - val_loss: 1283.9163 - val_mean_pred_metrics: 1283.9163\n",
      "Epoch 16/100\n",
      "100436/100436 [==============================] - 6s - loss: 1491.9825 - mean_pred_metrics: 1491.9825 - val_loss: 1239.3411 - val_mean_pred_metrics: 1239.3411\n",
      "Epoch 17/100\n",
      "100436/100436 [==============================] - 6s - loss: 1474.0383 - mean_pred_metrics: 1474.0383 - val_loss: 1240.7251 - val_mean_pred_metrics: 1240.7251\n",
      "Epoch 18/100\n",
      "100436/100436 [==============================] - 6s - loss: 1449.5413 - mean_pred_metrics: 1449.5413 - val_loss: 1211.4488 - val_mean_pred_metrics: 1211.4488\n",
      "Epoch 19/100\n",
      "100436/100436 [==============================] - 6s - loss: 1419.8147 - mean_pred_metrics: 1419.8147 - val_loss: 1200.8089 - val_mean_pred_metrics: 1200.8089\n",
      "Epoch 20/100\n",
      "100436/100436 [==============================] - 6s - loss: 1396.4460 - mean_pred_metrics: 1396.4460 - val_loss: 1170.8017 - val_mean_pred_metrics: 1170.8017\n",
      "Epoch 21/100\n",
      "100436/100436 [==============================] - 6s - loss: 1381.7543 - mean_pred_metrics: 1381.7543 - val_loss: 1170.4527 - val_mean_pred_metrics: 1170.4527\n",
      "Epoch 22/100\n",
      "100436/100436 [==============================] - 6s - loss: 1363.4591 - mean_pred_metrics: 1363.4591 - val_loss: 1210.8599 - val_mean_pred_metrics: 1210.8599\n",
      "Epoch 23/100\n",
      "100436/100436 [==============================] - 6s - loss: 1348.5135 - mean_pred_metrics: 1348.5135 - val_loss: 1201.4748 - val_mean_pred_metrics: 1201.4748\n",
      "Epoch 24/100\n",
      "100436/100436 [==============================] - 6s - loss: 1329.7250 - mean_pred_metrics: 1329.7250 - val_loss: 1169.6318 - val_mean_pred_metrics: 1169.6318\n",
      "Epoch 25/100\n",
      "100436/100436 [==============================] - 6s - loss: 1316.4640 - mean_pred_metrics: 1316.4640 - val_loss: 1261.7528 - val_mean_pred_metrics: 1261.7528\n",
      "Epoch 26/100\n",
      "100436/100436 [==============================] - 6s - loss: 1300.0172 - mean_pred_metrics: 1300.0172 - val_loss: 1173.3877 - val_mean_pred_metrics: 1173.3877\n",
      "Epoch 27/100\n",
      "100436/100436 [==============================] - 6s - loss: 1282.5220 - mean_pred_metrics: 1282.5220 - val_loss: 1198.0617 - val_mean_pred_metrics: 1198.0617\n",
      "Epoch 28/100\n",
      "100436/100436 [==============================] - 6s - loss: 1273.4313 - mean_pred_metrics: 1273.4313 - val_loss: 1155.0296 - val_mean_pred_metrics: 1155.0296\n",
      "Epoch 29/100\n",
      "100436/100436 [==============================] - 6s - loss: 1265.3906 - mean_pred_metrics: 1265.3906 - val_loss: 1156.8373 - val_mean_pred_metrics: 1156.8373\n",
      "Epoch 30/100\n",
      "100436/100436 [==============================] - 6s - loss: 1253.2444 - mean_pred_metrics: 1253.2444 - val_loss: 1164.3546 - val_mean_pred_metrics: 1164.3546\n",
      "Epoch 31/100\n",
      "100436/100436 [==============================] - 6s - loss: 1242.3700 - mean_pred_metrics: 1242.3700 - val_loss: 1151.3072 - val_mean_pred_metrics: 1151.3072\n",
      "Epoch 32/100\n",
      "100436/100436 [==============================] - 6s - loss: 1227.3591 - mean_pred_metrics: 1227.3591 - val_loss: 1147.7174 - val_mean_pred_metrics: 1147.7174\n",
      "Epoch 33/100\n",
      "100436/100436 [==============================] - 7s - loss: 1218.2284 - mean_pred_metrics: 1218.2284 - val_loss: 1160.6936 - val_mean_pred_metrics: 1160.6936\n",
      "Epoch 34/100\n",
      "100436/100436 [==============================] - 6s - loss: 1202.8821 - mean_pred_metrics: 1202.8821 - val_loss: 1148.7022 - val_mean_pred_metrics: 1148.7022\n",
      "Epoch 35/100\n",
      "100436/100436 [==============================] - 6s - loss: 1198.5602 - mean_pred_metrics: 1198.5602 - val_loss: 1151.3442 - val_mean_pred_metrics: 1151.3442\n",
      "Epoch 36/100\n",
      "100436/100436 [==============================] - 6s - loss: 1186.5893 - mean_pred_metrics: 1186.5893 - val_loss: 1155.5804 - val_mean_pred_metrics: 1155.5804\n",
      "Epoch 37/100\n",
      "100436/100436 [==============================] - 6s - loss: 1178.7782 - mean_pred_metrics: 1178.7782 - val_loss: 1149.8229 - val_mean_pred_metrics: 1149.8229\n",
      "Epoch 38/100\n",
      "100436/100436 [==============================] - 6s - loss: 1173.4765 - mean_pred_metrics: 1173.4765 - val_loss: 1148.5505 - val_mean_pred_metrics: 1148.5505\n",
      "Epoch 39/100\n",
      "100436/100436 [==============================] - 6s - loss: 1166.1170 - mean_pred_metrics: 1166.1170 - val_loss: 1151.5271 - val_mean_pred_metrics: 1151.5271\n",
      "Epoch 40/100\n",
      "100436/100436 [==============================] - 6s - loss: 1156.3728 - mean_pred_metrics: 1156.3728 - val_loss: 1156.4786 - val_mean_pred_metrics: 1156.4786\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100436/100436 [==============================] - 6s - loss: 1148.9452 - mean_pred_metrics: 1148.9452 - val_loss: 1152.0684 - val_mean_pred_metrics: 1152.0684\n",
      "Epoch 42/100\n",
      "100436/100436 [==============================] - 6s - loss: 1149.1742 - mean_pred_metrics: 1149.1742 - val_loss: 1152.5814 - val_mean_pred_metrics: 1152.5814\n",
      "Epoch 43/100\n",
      "100436/100436 [==============================] - 6s - loss: 1140.7909 - mean_pred_metrics: 1140.7909 - val_loss: 1159.4557 - val_mean_pred_metrics: 1159.4557\n",
      "Fold 2, MAE: 0.37785441310995893\n",
      "3-fold CV score: 0.3770100302582653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('CV score for the final model:', 0.37701003025826529)"
      ]
     },
     "execution_count": 75,
=======
      "100436/100436 [==============================] - 9s 92us/step - loss: 1969.1457 - mean_absolute_error: 0.8450 - mean_pred_metrics: 1969.1457 - val_loss: 1578.8081 - val_mean_absolute_error: 0.5782 - val_mean_pred_metrics: 1578.8081\n",
      "Epoch 5/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1904.3249 - mean_absolute_error: 0.7939 - mean_pred_metrics: 1904.3249 - val_loss: 1572.2104 - val_mean_absolute_error: 0.5620 - val_mean_pred_metrics: 1572.2104\n",
      "Epoch 6/30\n",
      "100436/100436 [==============================] - 10s 100us/step - loss: 1868.3429 - mean_absolute_error: 0.7580 - mean_pred_metrics: 1868.3429 - val_loss: 1605.6478 - val_mean_absolute_error: 0.5616 - val_mean_pred_metrics: 1605.6478\n",
      "Epoch 7/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1830.5336 - mean_absolute_error: 0.7296 - mean_pred_metrics: 1830.5336 - val_loss: 1467.8644 - val_mean_absolute_error: 0.5176 - val_mean_pred_metrics: 1467.8644\n",
      "Epoch 8/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1790.3655 - mean_absolute_error: 0.6988 - mean_pred_metrics: 1790.3655 - val_loss: 1445.0242 - val_mean_absolute_error: 0.5061 - val_mean_pred_metrics: 1445.0242\n",
      "Epoch 9/30\n",
      "100436/100436 [==============================] - 10s 101us/step - loss: 1765.1532 - mean_absolute_error: 0.6793 - mean_pred_metrics: 1765.1532 - val_loss: 1387.5749 - val_mean_absolute_error: 0.4657 - val_mean_pred_metrics: 1387.5749\n",
      "Epoch 10/30\n",
      "100436/100436 [==============================] - 10s 99us/step - loss: 1729.2739 - mean_absolute_error: 0.6532 - mean_pred_metrics: 1729.2739 - val_loss: 1373.5861 - val_mean_absolute_error: 0.4668 - val_mean_pred_metrics: 1373.5861\n",
      "Epoch 11/30\n",
      "100436/100436 [==============================] - 10s 101us/step - loss: 1684.5067 - mean_absolute_error: 0.6275 - mean_pred_metrics: 1684.5067 - val_loss: 1313.1172 - val_mean_absolute_error: 0.4495 - val_mean_pred_metrics: 1313.1172\n",
      "Epoch 12/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1648.8197 - mean_absolute_error: 0.6048 - mean_pred_metrics: 1648.8197 - val_loss: 1254.1643 - val_mean_absolute_error: 0.4106 - val_mean_pred_metrics: 1254.1643\n",
      "Epoch 13/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1614.4531 - mean_absolute_error: 0.5864 - mean_pred_metrics: 1614.4531 - val_loss: 1228.7458 - val_mean_absolute_error: 0.4096 - val_mean_pred_metrics: 1228.7458\n",
      "Epoch 14/30\n",
      "100436/100436 [==============================] - 10s 95us/step - loss: 1590.2515 - mean_absolute_error: 0.5683 - mean_pred_metrics: 1590.2515 - val_loss: 1272.8373 - val_mean_absolute_error: 0.4173 - val_mean_pred_metrics: 1272.8373\n",
      "Epoch 15/30\n",
      "100436/100436 [==============================] - 9s 94us/step - loss: 1566.4109 - mean_absolute_error: 0.5552 - mean_pred_metrics: 1566.4109 - val_loss: 1255.1324 - val_mean_absolute_error: 0.4113 - val_mean_pred_metrics: 1255.1324\n",
      "Epoch 16/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1528.4041 - mean_absolute_error: 0.5355 - mean_pred_metrics: 1528.4041 - val_loss: 1178.0726 - val_mean_absolute_error: 0.3901 - val_mean_pred_metrics: 1178.0726\n",
      "Epoch 17/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1502.5504 - mean_absolute_error: 0.5229 - mean_pred_metrics: 1502.5504 - val_loss: 1252.1554 - val_mean_absolute_error: 0.4130 - val_mean_pred_metrics: 1252.1554\n",
      "Epoch 18/30\n",
      "100436/100436 [==============================] - 9s 94us/step - loss: 1486.0183 - mean_absolute_error: 0.5118 - mean_pred_metrics: 1486.0183 - val_loss: 1240.2024 - val_mean_absolute_error: 0.4066 - val_mean_pred_metrics: 1240.2024\n",
      "Epoch 19/30\n",
      "100436/100436 [==============================] - 9s 94us/step - loss: 1451.6278 - mean_absolute_error: 0.4970 - mean_pred_metrics: 1451.6278 - val_loss: 1163.6224 - val_mean_absolute_error: 0.3843 - val_mean_pred_metrics: 1163.6224\n",
      "Epoch 20/30\n",
      "100436/100436 [==============================] - 10s 96us/step - loss: 1440.3419 - mean_absolute_error: 0.4882 - mean_pred_metrics: 1440.3419 - val_loss: 1211.7694 - val_mean_absolute_error: 0.3914 - val_mean_pred_metrics: 1211.7694\n",
      "Epoch 21/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1414.3519 - mean_absolute_error: 0.4749 - mean_pred_metrics: 1414.3519 - val_loss: 1176.6111 - val_mean_absolute_error: 0.3847 - val_mean_pred_metrics: 1176.6111\n",
      "Epoch 22/30\n",
      "100436/100436 [==============================] - 10s 104us/step - loss: 1396.7660 - mean_absolute_error: 0.4673 - mean_pred_metrics: 1396.7660 - val_loss: 1159.7447 - val_mean_absolute_error: 0.3771 - val_mean_pred_metrics: 1159.7447\n",
      "Epoch 23/30\n",
      "100436/100436 [==============================] - 10s 95us/step - loss: 1382.2535 - mean_absolute_error: 0.4592 - mean_pred_metrics: 1382.2535 - val_loss: 1213.4982 - val_mean_absolute_error: 0.3917 - val_mean_pred_metrics: 1213.4982\n",
      "Epoch 24/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1360.0148 - mean_absolute_error: 0.4508 - mean_pred_metrics: 1360.0148 - val_loss: 1162.9807 - val_mean_absolute_error: 0.3801 - val_mean_pred_metrics: 1162.9807\n",
      "Epoch 25/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1340.7826 - mean_absolute_error: 0.4414 - mean_pred_metrics: 1340.7826 - val_loss: 1190.3354 - val_mean_absolute_error: 0.3847 - val_mean_pred_metrics: 1190.3354\n",
      "Epoch 26/30\n",
      "100436/100436 [==============================] - 10s 96us/step - loss: 1326.2578 - mean_absolute_error: 0.4360 - mean_pred_metrics: 1326.2578 - val_loss: 1155.6241 - val_mean_absolute_error: 0.3756 - val_mean_pred_metrics: 1155.6241\n",
      "Epoch 27/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1310.7670 - mean_absolute_error: 0.4287 - mean_pred_metrics: 1310.7670 - val_loss: 1139.3800 - val_mean_absolute_error: 0.3747 - val_mean_pred_metrics: 1139.3800\n",
      "Epoch 28/30\n",
      "100436/100436 [==============================] - 10s 96us/step - loss: 1295.1779 - mean_absolute_error: 0.4241 - mean_pred_metrics: 1295.1779 - val_loss: 1165.8912 - val_mean_absolute_error: 0.3785 - val_mean_pred_metrics: 1165.8912\n",
      "Epoch 29/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1280.5611 - mean_absolute_error: 0.4182 - mean_pred_metrics: 1280.5611 - val_loss: 1152.6403 - val_mean_absolute_error: 0.3752 - val_mean_pred_metrics: 1152.6403\n",
      "Epoch 30/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1265.2016 - mean_absolute_error: 0.4129 - mean_pred_metrics: 1265.2016 - val_loss: 1159.3377 - val_mean_absolute_error: 0.3762 - val_mean_pred_metrics: 1159.3377\n",
      "Fold 1, MAE: 0.3798251808969431\n",
      "Train on 100436 samples, validate on 25110 samples\n",
      "Epoch 1/30\n",
      "100436/100436 [==============================] - 10s 100us/step - loss: 3000.9452 - mean_absolute_error: 5.0477 - mean_pred_metrics: 3000.9452 - val_loss: 2459.1464 - val_mean_absolute_error: 2.4933 - val_mean_pred_metrics: 2459.1464\n",
      "Epoch 2/30\n",
      "100436/100436 [==============================] - 10s 96us/step - loss: 2179.0740 - mean_absolute_error: 1.1944 - mean_pred_metrics: 2179.0740 - val_loss: 1595.6270 - val_mean_absolute_error: 0.6762 - val_mean_pred_metrics: 1595.6270\n",
      "Epoch 3/30\n",
      "100436/100436 [==============================] - 9s 95us/step - loss: 1944.9927 - mean_absolute_error: 0.8208 - mean_pred_metrics: 1944.9927 - val_loss: 1675.8707 - val_mean_absolute_error: 0.6017 - val_mean_pred_metrics: 1675.8707\n",
      "Epoch 4/30\n",
      "100436/100436 [==============================] - 9s 92us/step - loss: 1852.9051 - mean_absolute_error: 0.7489 - mean_pred_metrics: 1852.9051 - val_loss: 1504.5061 - val_mean_absolute_error: 0.5648 - val_mean_pred_metrics: 1504.5061\n",
      "Epoch 5/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1797.0752 - mean_absolute_error: 0.7101 - mean_pred_metrics: 1797.0752 - val_loss: 1485.2883 - val_mean_absolute_error: 0.5012 - val_mean_pred_metrics: 1485.2883\n",
      "Epoch 6/30\n",
      "100436/100436 [==============================] - 9s 93us/step - loss: 1757.6097 - mean_absolute_error: 0.6829 - mean_pred_metrics: 1757.6097 - val_loss: 1332.5663 - val_mean_absolute_error: 0.4629 - val_mean_pred_metrics: 1332.5663\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100436/100436 [==============================] - 9s 94us/step - loss: 1732.2703 - mean_absolute_error: 0.6607 - mean_pred_metrics: 1732.2703 - val_loss: 1339.6569 - val_mean_absolute_error: 0.4383 - val_mean_pred_metrics: 1339.6569\n",
      "Epoch 8/30\n",
      "100436/100436 [==============================] - 10s 96us/step - loss: 1690.5047 - mean_absolute_error: 0.6324 - mean_pred_metrics: 1690.5047 - val_loss: 1295.5609 - val_mean_absolute_error: 0.4446 - val_mean_pred_metrics: 1295.5609\n",
      "Epoch 9/30\n",
      "100436/100436 [==============================] - 10s 101us/step - loss: 1657.3362 - mean_absolute_error: 0.6106 - mean_pred_metrics: 1657.3362 - val_loss: 1259.1574 - val_mean_absolute_error: 0.4141 - val_mean_pred_metrics: 1259.1574\n",
      "Epoch 10/30\n",
      "100436/100436 [==============================] - 10s 101us/step - loss: 1634.9820 - mean_absolute_error: 0.5939 - mean_pred_metrics: 1634.9820 - val_loss: 1315.4953 - val_mean_absolute_error: 0.4390 - val_mean_pred_metrics: 1315.4953\n",
      "Epoch 11/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1606.8590 - mean_absolute_error: 0.5769 - mean_pred_metrics: 1606.8590 - val_loss: 1234.8398 - val_mean_absolute_error: 0.4117 - val_mean_pred_metrics: 1234.8398\n",
      "Epoch 12/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1574.8844 - mean_absolute_error: 0.5582 - mean_pred_metrics: 1574.8844 - val_loss: 1304.6121 - val_mean_absolute_error: 0.4231 - val_mean_pred_metrics: 1304.6121\n",
      "Epoch 13/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1558.9244 - mean_absolute_error: 0.5442 - mean_pred_metrics: 1558.9244 - val_loss: 1261.2292 - val_mean_absolute_error: 0.4075 - val_mean_pred_metrics: 1261.2292\n",
      "Epoch 14/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1521.2963 - mean_absolute_error: 0.5282 - mean_pred_metrics: 1521.2963 - val_loss: 1205.0296 - val_mean_absolute_error: 0.3936 - val_mean_pred_metrics: 1205.0296\n",
      "Epoch 15/30\n",
      "100436/100436 [==============================] - 10s 100us/step - loss: 1505.8795 - mean_absolute_error: 0.5174 - mean_pred_metrics: 1505.8795 - val_loss: 1264.6362 - val_mean_absolute_error: 0.4061 - val_mean_pred_metrics: 1264.6362\n",
      "Epoch 16/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1475.3269 - mean_absolute_error: 0.5041 - mean_pred_metrics: 1475.3269 - val_loss: 1177.3532 - val_mean_absolute_error: 0.3823 - val_mean_pred_metrics: 1177.3532\n",
      "Epoch 17/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1460.0264 - mean_absolute_error: 0.4960 - mean_pred_metrics: 1460.0264 - val_loss: 1191.1394 - val_mean_absolute_error: 0.3893 - val_mean_pred_metrics: 1191.1394\n",
      "Epoch 18/30\n",
      "100436/100436 [==============================] - 10s 97us/step - loss: 1439.7954 - mean_absolute_error: 0.4843 - mean_pred_metrics: 1439.7954 - val_loss: 1252.2655 - val_mean_absolute_error: 0.4063 - val_mean_pred_metrics: 1252.2655\n",
      "Epoch 19/30\n",
      "100436/100436 [==============================] - 10s 101us/step - loss: 1419.1505 - mean_absolute_error: 0.4747 - mean_pred_metrics: 1419.1505 - val_loss: 1186.2000 - val_mean_absolute_error: 0.3855 - val_mean_pred_metrics: 1186.2000\n",
      "Epoch 20/30\n",
      "100436/100436 [==============================] - 10s 95us/step - loss: 1394.9898 - mean_absolute_error: 0.4648 - mean_pred_metrics: 1394.9898 - val_loss: 1205.7036 - val_mean_absolute_error: 0.3880 - val_mean_pred_metrics: 1205.7036\n",
      "Epoch 21/30\n",
      "100436/100436 [==============================] - 10s 95us/step - loss: 1380.1194 - mean_absolute_error: 0.4575 - mean_pred_metrics: 1380.1194 - val_loss: 1203.4310 - val_mean_absolute_error: 0.3943 - val_mean_pred_metrics: 1203.4310\n",
      "Fold 2, MAE: 0.39608245792484953\n",
      "3-fold CV score: 0.3860780978245102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('CV score for the final model:', 0.38607809782451019)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_score = cross_validate_mlp(hyper_model)\n",
    "(\"CV score for the final model:\", cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22, 37], [10, 16], [25, 26], [39, 16], [45], [29], [8, 26], [6, 10], [8, 16], [47, 25, 37, 36]]\n",
      "[[22 37  0  0]\n",
      " [10 16  0  0]\n",
      " [25 26  0  0]\n",
      " [39 16  0  0]\n",
      " [45  0  0  0]\n",
      " [29  0  0  0]\n",
      " [ 8 26  0  0]\n",
      " [ 6 10  0  0]\n",
      " [ 8 16  0  0]\n",
      " [47 25 37 36]]\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_3 (Embedding)          (None, 4, 8)          400         embedding_input_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)              (None, 32)            0           embedding_3[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_135 (Dense)                (None, 1)             33          flatten_3[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, nb_epoch=100, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN_with_EntityEmbedding(Model):\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "        super().__init__()\n",
    "        self.nb_epoch = 10\n",
    "        self.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "        self.max_log_y = max(numpy.max(numpy.log(y_train)), numpy.max(numpy.log(y_val)))\n",
    "        self.__build_keras_model()\n",
    "        self.fit(X_train, y_train, X_val, y_val)\n",
    "\n",
    "    def preprocessing(self, X):\n",
    "        X_list = split_features(X)\n",
    "        return X_list\n",
    "\n",
    "    def __build_keras_model(self):\n",
    "        models = []\n",
    "\n",
    "        model_store = Sequential()\n",
    "        model_store.add(Embedding(1115, 10, input_length=1))\n",
    "        model_store.add(Reshape(target_shape=(10,)))\n",
    "        models.append(model_store)\n",
    "\n",
    "        model_dow = Sequential()\n",
    "        model_dow.add(Embedding(7, 6, input_length=1))\n",
    "        model_dow.add(Reshape(target_shape=(6,)))\n",
    "        models.append(model_dow)\n",
    "\n",
    "        model_promo = Sequential()\n",
    "        model_promo.add(Dense(1, input_dim=1))\n",
    "        models.append(model_promo)\n",
    "\n",
    "        model_year = Sequential()\n",
    "        model_year.add(Embedding(3, 2, input_length=1))\n",
    "        model_year.add(Reshape(target_shape=(2,)))\n",
    "        models.append(model_year)\n",
    "\n",
    "        model_month = Sequential()\n",
    "        model_month.add(Embedding(12, 6, input_length=1))\n",
    "        model_month.add(Reshape(target_shape=(6,)))\n",
    "        models.append(model_month)\n",
    "\n",
    "        model_day = Sequential()\n",
    "        model_day.add(Embedding(31, 10, input_length=1))\n",
    "        model_day.add(Reshape(target_shape=(10,)))\n",
    "        models.append(model_day)\n",
    "\n",
    "        model_germanstate = Sequential()\n",
    "        model_germanstate.add(Embedding(12, 6, input_length=1))\n",
    "        model_germanstate.add(Reshape(target_shape=(6,)))\n",
    "        models.append(model_germanstate)\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Merge(models, mode='concat'))\n",
    "        self.model.add(Dense(1000, init='uniform'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(500, init='uniform'))\n",
    "        self.model.add(Activation('relu'))\n",
    "        self.model.add(Dense(1))\n",
    "        self.model.add(Activation('sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "\n",
    "    def _val_for_fit(self, val):\n",
    "        val = numpy.log(val) / self.max_log_y\n",
    "        return val\n",
    "\n",
    "    def _val_for_pred(self, val):\n",
    "        return numpy.exp(val * self.max_log_y)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "        self.model.fit(self.preprocessing(X_train), self._val_for_fit(y_train),\n",
    "                       validation_data=(self.preprocessing(X_val), self._val_for_fit(y_val)),\n",
    "                       nb_epoch=self.nb_epoch, batch_size=128,\n",
    "                       # callbacks=[self.checkpointer],\n",
    "                       )\n",
    "        # self.model.load_weights('best_model_weights.hdf5')\n",
    "        print(\"Result on validation data: \", self.evaluate(X_val, y_val))\n",
    "\n",
    "    def guess(self, features):\n",
    "        features = self.preprocessing(features)\n",
    "        result = self.model.predict(features).flatten()\n",
    "        return self._val_for_pred(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
