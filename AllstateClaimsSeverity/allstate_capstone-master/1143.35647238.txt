from keras import regularizers
from keras.initializers import he_normal
from keras.regularizers import l2
from keras.activations import selu
from keras.optimizers import Adam
def hyper_model(seed = None):
    model = Sequential()
    model.add(Dense(437, input_dim=train_x.shape[1], kernel_initializer=he_normal(seed = seed)
                    ,kernel_regularizer=l2(0.002)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.536))
    
    model.add(Dense(182, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.4))
    
    model.add(Dense(73, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(0.233))
    
    model.add(Dense(1, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))
    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
    #model.compile(optimizer='adadelta',loss = 'mae',metrics = [mae_score])
    model.compile(optimizer='adadelta',loss = 'mae')
    return model


nnmodel={}
nnmodel.clear()
def cross_validate_mlp(mlp_func, nfolds=5,nbags=1):
    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)
    val_scores = np.zeros((nbags,))
    stack_train = np.zeros((nbags,len(train_y)))
    stack_test = np.zeros((nbags,len(test)))
    for k,(train_index, test_index) in enumerate(folds):
        xtr = train_x[train_index]
        ytr = train_y[train_index]
        xte = train_x[test_index]
        yte = train_y[test_index]
        for bag in range(nbags):
            nnmodel['nn%d',k*10+bag*1] = mlp_func(seed = k*10+bag*1)
            early_stopping = EarlyStopping(monitor='val_loss', patience=5)
            fit = nnmodel['nn%d',k*10+bag*1].fit(xtr, ytr, validation_split=0.2, batch_size=128,
                          epochs=30, verbose=1, callbacks=[ExponentialMovingAverage(save_mv_ave_model=False),
                                                          early_stopping])
            pred = nnmodel['nn%d',k*10+bag*1].predict(xte, batch_size=256)
            #score = mean_absolute_error(np.exp(yte), np.exp(pred))
            score = mean_absolute_error(yte, pred)
            val_scores[bag] += score
            #stack_train[bag][test_index] = pred[:,0]
            print ("nfold:{},bag:{}".format(k,bag),score)
    for bag in range(nbags):
        val_scores[bag] = val_scores[bag] / float(nfolds)
    
    
    return val_scores

cv_score = cross_validate_mlp(hyper_model)
print ("CV score for the final model:", cv_score)


Train on 120523 samples, validate on 30131 samples
Epoch 1/30
120523/120523 [==============================] - 18s 151us/step - loss: 2220.7055 - val_loss: 1240.2123
Epoch 2/30
120523/120523 [==============================] - 17s 144us/step - loss: 1383.0595 - val_loss: 1179.4340
Epoch 3/30
120523/120523 [==============================] - 17s 143us/step - loss: 1343.8729 - val_loss: 1170.8472
Epoch 4/30
120523/120523 [==============================] - 18s 145us/step - loss: 1335.4071 - val_loss: 1160.8068
Epoch 5/30
120523/120523 [==============================] - 18s 148us/step - loss: 1326.3428 - val_loss: 1159.4791
Epoch 6/30
120523/120523 [==============================] - 17s 145us/step - loss: 1320.6481 - val_loss: 1156.3107
Epoch 7/30
120523/120523 [==============================] - 17s 144us/step - loss: 1309.6932 - val_loss: 1151.1042
Epoch 8/30
120523/120523 [==============================] - 17s 144us/step - loss: 1304.2346 - val_loss: 1153.3945
Epoch 9/30
120523/120523 [==============================] - 17s 144us/step - loss: 1300.9895 - val_loss: 1154.6922
Epoch 10/30
120523/120523 [==============================] - 18s 147us/step - loss: 1292.3884 - val_loss: 1146.4252
Epoch 11/30
120523/120523 [==============================] - 17s 145us/step - loss: 1285.4468 - val_loss: 1146.5292
Epoch 12/30
120523/120523 [==============================] - 17s 143us/step - loss: 1283.5604 - val_loss: 1146.4775
Epoch 13/30
120523/120523 [==============================] - 17s 144us/step - loss: 1278.4509 - val_loss: 1144.0429
Epoch 14/30
120523/120523 [==============================] - 17s 145us/step - loss: 1267.9910 - val_loss: 1147.8762
Epoch 15/30
120523/120523 [==============================] - 17s 143us/step - loss: 1262.1824 - val_loss: 1145.8711
Epoch 16/30
120523/120523 [==============================] - 18s 147us/step - loss: 1258.6666 - val_loss: 1141.7195
Epoch 17/30
120523/120523 [==============================] - 18s 150us/step - loss: 1257.5281 - val_loss: 1140.6500
Epoch 18/30
120523/120523 [==============================] - 18s 146us/step - loss: 1250.9173 - val_loss: 1142.3087
Epoch 19/30
120523/120523 [==============================] - 18s 146us/step - loss: 1246.4278 - val_loss: 1142.7809
Epoch 20/30
120523/120523 [==============================] - 18s 149us/step - loss: 1239.0933 - val_loss: 1141.4450
Epoch 21/30
120523/120523 [==============================] - 17s 144us/step - loss: 1234.6676 - val_loss: 1142.6470
Epoch 22/30
120523/120523 [==============================] - 18s 150us/step - loss: 1229.2469 - val_loss: 1139.3260
Epoch 23/30
120523/120523 [==============================] - 17s 145us/step - loss: 1225.4607 - val_loss: 1141.6334
Epoch 24/30
120523/120523 [==============================] - 17s 145us/step - loss: 1219.1531 - val_loss: 1143.0691
Epoch 25/30
120523/120523 [==============================] - 18s 151us/step - loss: 1215.6541 - val_loss: 1142.4367
Epoch 26/30
120523/120523 [==============================] - 18s 146us/step - loss: 1213.3739 - val_loss: 1140.5451
Epoch 27/30
120523/120523 [==============================] - 17s 143us/step - loss: 1212.4373 - val_loss: 1141.2941
nfold:0,bag:0 1150.93866139
Train on 120523 samples, validate on 30131 samples
Epoch 1/30
120523/120523 [==============================] - 18s 153us/step - loss: 2156.7897 - val_loss: 1240.8666
Epoch 2/30
120523/120523 [==============================] - 18s 147us/step - loss: 1352.6270 - val_loss: 1178.2444
Epoch 3/30
120523/120523 [==============================] - 18s 152us/step - loss: 1329.1122 - val_loss: 1163.4105
Epoch 4/30
120523/120523 [==============================] - 18s 146us/step - loss: 1317.7670 - val_loss: 1158.1583
Epoch 5/30
120523/120523 [==============================] - 17s 144us/step - loss: 1311.2702 - val_loss: 1154.0003
Epoch 6/30
120523/120523 [==============================] - 18s 152us/step - loss: 1305.6887 - val_loss: 1150.0340
Epoch 7/30
120523/120523 [==============================] - 20s 162us/step - loss: 1300.8328 - val_loss: 1148.8937
Epoch 8/30
120523/120523 [==============================] - 20s 165us/step - loss: 1294.2257 - val_loss: 1148.1912
Epoch 9/30
120523/120523 [==============================] - 19s 161us/step - loss: 1289.6478 - val_loss: 1146.7017
Epoch 10/30
120523/120523 [==============================] - 20s 162us/step - loss: 1286.6153 - val_loss: 1142.8037
Epoch 11/30
120523/120523 [==============================] - 20s 163us/step - loss: 1278.4921 - val_loss: 1141.9567
Epoch 12/30
120523/120523 [==============================] - 20s 170us/step - loss: 1271.9114 - val_loss: 1141.7722
Epoch 13/30
120523/120523 [==============================] - 19s 162us/step - loss: 1269.0241 - val_loss: 1140.1195
Epoch 14/30
120523/120523 [==============================] - 19s 161us/step - loss: 1260.3249 - val_loss: 1141.2965
Epoch 15/30
120523/120523 [==============================] - 19s 160us/step - loss: 1259.1349 - val_loss: 1138.0889
Epoch 16/30
120523/120523 [==============================] - 19s 160us/step - loss: 1256.6295 - val_loss: 1140.5910
Epoch 17/30
120523/120523 [==============================] - 19s 161us/step - loss: 1248.9829 - val_loss: 1139.5392
Epoch 18/30
120523/120523 [==============================] - 20s 165us/step - loss: 1248.4914 - val_loss: 1139.3333
Epoch 19/30
120523/120523 [==============================] - 19s 160us/step - loss: 1242.8248 - val_loss: 1137.5459
Epoch 20/30
120523/120523 [==============================] - 20s 162us/step - loss: 1238.3190 - val_loss: 1137.2074
Epoch 21/30
120523/120523 [==============================] - 19s 162us/step - loss: 1231.4430 - val_loss: 1134.3223
Epoch 22/30
120523/120523 [==============================] - 19s 160us/step - loss: 1231.8428 - val_loss: 1136.3040
Epoch 23/30
120523/120523 [==============================] - 20s 162us/step - loss: 1225.7966 - val_loss: 1134.9105
Epoch 24/30
120523/120523 [==============================] - 20s 164us/step - loss: 1221.7529 - val_loss: 1136.7496
Epoch 25/30
120523/120523 [==============================] - 20s 163us/step - loss: 1216.3134 - val_loss: 1134.5557
Epoch 26/30
120523/120523 [==============================] - 20s 163us/step - loss: 1213.0992 - val_loss: 1138.0729
nfold:1,bag:0 1143.8277897
Train on 120523 samples, validate on 30131 samples
Epoch 1/30
120523/120523 [==============================] - 21s 171us/step - loss: 2116.7592 - val_loss: 1201.1712
Epoch 2/30
120523/120523 [==============================] - 20s 165us/step - loss: 1339.9314 - val_loss: 1171.5482
Epoch 3/30
120523/120523 [==============================] - 20s 168us/step - loss: 1319.3720 - val_loss: 1163.9439
Epoch 4/30
120523/120523 [==============================] - 20s 169us/step - loss: 1311.2813 - val_loss: 1162.2898
Epoch 5/30
120523/120523 [==============================] - 21s 175us/step - loss: 1307.8476 - val_loss: 1156.1174
Epoch 6/30
120523/120523 [==============================] - 20s 166us/step - loss: 1298.3534 - val_loss: 1153.6526
Epoch 7/30
120523/120523 [==============================] - 20s 163us/step - loss: 1292.7517 - val_loss: 1151.1617
Epoch 8/30
120523/120523 [==============================] - 20s 167us/step - loss: 1284.2404 - val_loss: 1150.8852
Epoch 9/30
120523/120523 [==============================] - 20s 164us/step - loss: 1279.3807 - val_loss: 1149.7054
Epoch 10/30
120523/120523 [==============================] - 20s 165us/step - loss: 1273.0901 - val_loss: 1147.0088
Epoch 11/30
120523/120523 [==============================] - 20s 168us/step - loss: 1272.8376 - val_loss: 1146.9109
Epoch 12/30
120523/120523 [==============================] - 20s 168us/step - loss: 1266.4569 - val_loss: 1146.6232
Epoch 13/30
120523/120523 [==============================] - 20s 166us/step - loss: 1256.7026 - val_loss: 1144.7972
Epoch 14/30
120523/120523 [==============================] - 20s 167us/step - loss: 1256.2467 - val_loss: 1147.1039
Epoch 15/30
120523/120523 [==============================] - 20s 168us/step - loss: 1249.9372 - val_loss: 1144.6351
Epoch 16/30
120523/120523 [==============================] - 21s 170us/step - loss: 1251.9283 - val_loss: 1143.5465
Epoch 17/30
120523/120523 [==============================] - 20s 167us/step - loss: 1241.4961 - val_loss: 1144.5366
Epoch 18/30
120523/120523 [==============================] - 20s 168us/step - loss: 1243.5102 - val_loss: 1143.3620
Epoch 19/30
120523/120523 [==============================] - 20s 168us/step - loss: 1236.6626 - val_loss: 1141.0594
Epoch 20/30
120523/120523 [==============================] - 20s 168us/step - loss: 1233.4897 - val_loss: 1143.1258
Epoch 21/30
120523/120523 [==============================] - 20s 170us/step - loss: 1230.5490 - val_loss: 1141.3692
Epoch 22/30
120523/120523 [==============================] - 20s 170us/step - loss: 1225.9896 - val_loss: 1141.6736
Epoch 23/30
120523/120523 [==============================] - 20s 169us/step - loss: 1224.5863 - val_loss: 1139.0325
Epoch 24/30
120523/120523 [==============================] - 20s 168us/step - loss: 1219.8429 - val_loss: 1139.1794
Epoch 25/30
120523/120523 [==============================] - 20s 167us/step - loss: 1213.0620 - val_loss: 1139.6385
Epoch 26/30
120523/120523 [==============================] - 20s 170us/step - loss: 1214.8577 - val_loss: 1142.5067
Epoch 27/30
120523/120523 [==============================] - 20s 168us/step - loss: 1208.4839 - val_loss: 1140.5615
Epoch 28/30
120523/120523 [==============================] - 20s 166us/step - loss: 1203.2854 - val_loss: 1138.4463
Epoch 29/30
120523/120523 [==============================] - 20s 167us/step - loss: 1200.8883 - val_loss: 1140.3566
Epoch 30/30
120523/120523 [==============================] - 21s 171us/step - loss: 1195.3204 - val_loss: 1138.7327
nfold:2,bag:0 1144.17148029
Train on 120524 samples, validate on 30131 samples
Epoch 1/30
120524/120524 [==============================] - 21s 171us/step - loss: 2252.3712 - val_loss: 1247.0992
Epoch 2/30
120524/120524 [==============================] - 20s 167us/step - loss: 1335.0113 - val_loss: 1171.3165
Epoch 3/30
120524/120524 [==============================] - 20s 166us/step - loss: 1310.0871 - val_loss: 1161.5819
Epoch 4/30
120524/120524 [==============================] - 20s 168us/step - loss: 1299.0243 - val_loss: 1159.0659
Epoch 5/30
120524/120524 [==============================] - 20s 168us/step - loss: 1291.3868 - val_loss: 1157.5722
Epoch 6/30
120524/120524 [==============================] - 20s 167us/step - loss: 1288.6129 - val_loss: 1151.0975
Epoch 7/30
120524/120524 [==============================] - 20s 169us/step - loss: 1280.6480 - val_loss: 1150.8108
Epoch 8/30
120524/120524 [==============================] - 20s 170us/step - loss: 1278.8053 - val_loss: 1146.2809
Epoch 9/30
120524/120524 [==============================] - 21s 171us/step - loss: 1272.2090 - val_loss: 1146.3238
Epoch 10/30
120524/120524 [==============================] - 22s 180us/step - loss: 1268.8373 - val_loss: 1143.6507
Epoch 11/30
120524/120524 [==============================] - 20s 169us/step - loss: 1262.2791 - val_loss: 1142.3134
Epoch 12/30
120524/120524 [==============================] - 21s 172us/step - loss: 1256.0082 - val_loss: 1143.0342
Epoch 13/30
120524/120524 [==============================] - 21s 174us/step - loss: 1252.2415 - val_loss: 1140.5687
Epoch 14/30
120524/120524 [==============================] - 21s 173us/step - loss: 1247.5447 - val_loss: 1138.0235
Epoch 15/30
120524/120524 [==============================] - 21s 171us/step - loss: 1245.1319 - val_loss: 1140.4057
Epoch 16/30
120524/120524 [==============================] - 21s 174us/step - loss: 1240.7457 - val_loss: 1138.6340
Epoch 17/30
120524/120524 [==============================] - 20s 170us/step - loss: 1237.6633 - val_loss: 1137.1535
Epoch 18/30
120524/120524 [==============================] - 20s 169us/step - loss: 1233.2991 - val_loss: 1138.2296
Epoch 19/30
120524/120524 [==============================] - 20s 170us/step - loss: 1226.8618 - val_loss: 1137.2696
Epoch 20/30
120524/120524 [==============================] - 21s 171us/step - loss: 1226.1558 - val_loss: 1139.1023
Epoch 21/30
120524/120524 [==============================] - 21s 172us/step - loss: 1222.8797 - val_loss: 1136.6749
Epoch 22/30
120524/120524 [==============================] - 21s 172us/step - loss: 1219.2526 - val_loss: 1137.4358
Epoch 23/30
120524/120524 [==============================] - 20s 164us/step - loss: 1214.8315 - val_loss: 1136.3191
Epoch 24/30
120524/120524 [==============================] - 21s 173us/step - loss: 1212.1019 - val_loss: 1136.5715
Epoch 25/30
120524/120524 [==============================] - 21s 173us/step - loss: 1203.7829 - val_loss: 1136.4320
Epoch 26/30
120524/120524 [==============================] - 21s 175us/step - loss: 1207.3643 - val_loss: 1137.4302
Epoch 27/30
120524/120524 [==============================] - 20s 167us/step - loss: 1199.0724 - val_loss: 1140.9104
Epoch 28/30
120524/120524 [==============================] - 20s 168us/step - loss: 1198.5937 - val_loss: 1137.6353
nfold:3,bag:0 1142.18858671
Train on 120524 samples, validate on 30131 samples
Epoch 1/30
120524/120524 [==============================] - 22s 184us/step - loss: 2195.9309 - val_loss: 1263.3946
Epoch 2/30
120524/120524 [==============================] - 20s 167us/step - loss: 1418.8403 - val_loss: 1196.0856
Epoch 3/30
120524/120524 [==============================] - 21s 171us/step - loss: 1386.2099 - val_loss: 1182.4641
Epoch 4/30
120524/120524 [==============================] - 20s 164us/step - loss: 1376.9616 - val_loss: 1173.5396
Epoch 5/30
120524/120524 [==============================] - 20s 170us/step - loss: 1367.4061 - val_loss: 1173.4109
Epoch 6/30
120524/120524 [==============================] - 21s 171us/step - loss: 1356.6327 - val_loss: 1165.2303
Epoch 7/30
120524/120524 [==============================] - 20s 168us/step - loss: 1346.9800 - val_loss: 1166.1347
Epoch 8/30
120524/120524 [==============================] - 20s 169us/step - loss: 1341.6068 - val_loss: 1158.8954
Epoch 9/30
120524/120524 [==============================] - 20s 169us/step - loss: 1333.8590 - val_loss: 1163.3096
Epoch 10/30
120524/120524 [==============================] - 20s 166us/step - loss: 1322.7317 - val_loss: 1161.0441
Epoch 11/30
120524/120524 [==============================] - 20s 167us/step - loss: 1320.9522 - val_loss: 1156.9133
Epoch 12/30
120524/120524 [==============================] - 20s 165us/step - loss: 1311.7515 - val_loss: 1154.2991
Epoch 13/30
120524/120524 [==============================] - 20s 165us/step - loss: 1303.4217 - val_loss: 1157.2863
Epoch 14/30
120524/120524 [==============================] - 20s 165us/step - loss: 1299.7339 - val_loss: 1152.0497
Epoch 15/30
120524/120524 [==============================] - 20s 169us/step - loss: 1289.7482 - val_loss: 1152.8028
Epoch 16/30
120524/120524 [==============================] - 20s 165us/step - loss: 1282.1680 - val_loss: 1152.7805
Epoch 17/30
120524/120524 [==============================] - 20s 165us/step - loss: 1276.4178 - val_loss: 1150.7471
Epoch 18/30
120524/120524 [==============================] - 20s 168us/step - loss: 1271.5583 - val_loss: 1151.5797
Epoch 19/30
120524/120524 [==============================] - 20s 167us/step - loss: 1266.3405 - val_loss: 1151.1535
Epoch 20/30
120524/120524 [==============================] - 20s 167us/step - loss: 1258.7990 - val_loss: 1150.0107
Epoch 21/30
120524/120524 [==============================] - 20s 166us/step - loss: 1255.2877 - val_loss: 1150.7993
Epoch 22/30
120524/120524 [==============================] - 20s 166us/step - loss: 1248.3481 - val_loss: 1153.1796
Epoch 23/30
120524/120524 [==============================] - 20s 169us/step - loss: 1238.6474 - val_loss: 1150.2670
Epoch 24/30
120524/120524 [==============================] - 20s 169us/step - loss: 1237.0756 - val_loss: 1151.0837
Epoch 25/30
120524/120524 [==============================] - 20s 167us/step - loss: 1233.3567 - val_loss: 1149.7543
Epoch 26/30
120524/120524 [==============================] - 20s 165us/step - loss: 1225.7192 - val_loss: 1148.6977
Epoch 27/30
120524/120524 [==============================] - 20s 168us/step - loss: 1218.9868 - val_loss: 1147.6546
Epoch 28/30
120524/120524 [==============================] - 20s 167us/step - loss: 1216.2719 - val_loss: 1149.4474
Epoch 29/30
120524/120524 [==============================] - 20s 168us/step - loss: 1212.7738 - val_loss: 1149.6970
Epoch 30/30
120524/120524 [==============================] - 20s 164us/step - loss: 1212.7833 - val_loss: 1148.3528
nfold:4,bag:0 1135.65584381
CV score for the final model: [ 1143.35647238]