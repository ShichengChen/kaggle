{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'Vladimir Iglovikov'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import pickle\n",
    "import hyperopt.pyll.stochastic\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santhosh SharmaExploratory study on ML algorithms?.ipynb  params2.txt\r\n",
      "Vladimir Iglovikovxgb 1114?.py                            sub_v.csv\r\n",
      "\u001b[0m\u001b[01;34mallstate_capstone-master\u001b[0m/                                 submission.csv\r\n",
      "handtune.ipynb                                            test.csv\r\n",
      "hyperopt.ipynb                                            train.csv\r\n",
      "params.txt\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socre:1143.8243166, params:{'reg_alpha': 16.769964407548855, 'colsample_bytree': 0.5349474195365941, 'scale_pos_weight': 3.8594086177000153, 'learning_rate': 0.03312809141914759, 'nthread': 10, 'min_child_weight': 3, 'subsample': 0.9471583516376643, 'seed': 0, 'max_depth': 6, 'gamma': 0.37515714143957835}\r\n"
     ]
    }
   ],
   "source": [
    "%cat params2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test['loss'] = np.nan\n",
    "joined = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in list(train.select_dtypes(include=['object']).columns):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = (remove_train|remove_test)\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "        joined[column] = joined[column].apply(lambda x: np.nan if x in remove else x, 1)\n",
    "\n",
    "    joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n",
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "\n",
    "shift = 700\n",
    "y = np.log(train['loss'] + shift)\n",
    "ids = test['id']\n",
    "X = train.drop(['loss', 'id'], 1)\n",
    "X_test = test.drop(['loss', 'id'], 1)\n",
    "xgtrain = xgb.DMatrix(X, label=y)\n",
    "xgtest = xgb.DMatrix(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time0= time.time()\n",
    "space4xgb = {\n",
    "    'max_depth': hp.choice('max_depth', range(3,20)),\n",
    "    'min_child_weight':hp.choice('min_child_weight',range(1,10)),\n",
    "    \n",
    "    'learning_rate': hp.loguniform('learning_rate', -3*np.log(10), -1*np.log(10)),\n",
    "    \n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0,10), \n",
    "    'gamma': hp.uniform('gamma',0,0.5),\n",
    "    \n",
    "    \n",
    "    'subsample': hp.uniform('subsample',0.1,1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree',0.1,1),\n",
    "    'reg_alpha':hp.uniform('reg_alpha',0,1000),\n",
    "    \n",
    "    'nthread': hp.choice('nthread', [10]),\n",
    "    'seed':hp.choice('seed', [0])\n",
    "}\n",
    "minacc=1160\n",
    "#{'reg_alpha': 0.8673861702351379, 'colsample_bytree': 0.8759854865376723, 'scale_pos_weight': 1, 'learning_rate': 0.06925571303753118, 'nthread': 20, 'min_child_weight': 1, 'subsample': 0.629375884543008, 'max_depth': 7, 'gamma': 0.19254932932610697}\n",
    "def optf(params): \n",
    "    params['nthread']=10\n",
    "    #clf = xgb.XGBRegressor(**params)\n",
    "    #acc = cross_val_score(clf, X, y,cv=cv,n_jobs=1,scoring=evalerror).mean()\n",
    "    bst = xgb.cv(params, xgtrain, num_boost_round=1000, nfold=5, seed=0, \n",
    "                    feval=evalerror,early_stopping_rounds=40)\n",
    "    acc = bst.iloc[-1,:]['test-mae-mean']\n",
    "    #print time.time()-time0\n",
    "    print acc\n",
    "    global minacc\n",
    "    if acc < minacc:\n",
    "        minacc = acc\n",
    "        print 'new best:', minacc, params\n",
    "        \n",
    "        with open(\"params2.txt\", \"a\") as text_file:\n",
    "            text_file.write('socre:{}, params:{}\\n'.format(minacc,params))\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning from 174 trials to 175 (+1) trials\n",
      "1158.9513672\n",
      "new best: 1158.9513672 {'reg_alpha': 51.17342991232758, 'colsample_bytree': 0.8746115066199959, 'scale_pos_weight': 2.687076986237154, 'learning_rate': 0.014590391007893385, 'nthread': 10, 'min_child_weight': 4, 'subsample': 0.8228501992826736, 'seed': 0, 'max_depth': 10, 'gamma': 0.07453877132173166}\n",
      "Rerunning from 175 trials to 176 (+1) trials\n",
      "1271.6265624\n",
      "Rerunning from 176 trials to 177 (+1) trials\n",
      "1559.0780274\n",
      "Rerunning from 177 trials to 178 (+1) trials\n",
      "1882.4093752\n",
      "Rerunning from 178 trials to 179 (+1) trials\n",
      "3541.7228518\n",
      "Rerunning from 179 trials to 180 (+1) trials\n",
      "3403.0651856\n",
      "Rerunning from 180 trials to 181 (+1) trials\n",
      "3048.5229492\n",
      "Rerunning from 181 trials to 182 (+1) trials\n",
      "1424.4821288\n",
      "Rerunning from 182 trials to 183 (+1) trials\n",
      "3477.7983888\n",
      "Rerunning from 183 trials to 184 (+1) trials\n",
      "3339.2915526\n",
      "Rerunning from 184 trials to 185 (+1) trials\n",
      "2554.9480956\n",
      "Rerunning from 185 trials to 186 (+1) trials\n",
      "1217.6481692\n",
      "Rerunning from 186 trials to 187 (+1) trials\n",
      "3188.2808106\n",
      "Rerunning from 187 trials to 188 (+1) trials\n",
      "1520.749292\n",
      "Rerunning from 188 trials to 189 (+1) trials\n",
      "2037.0508058\n",
      "Rerunning from 189 trials to 190 (+1) trials\n",
      "1402.6996338\n",
      "Rerunning from 190 trials to 191 (+1) trials\n"
     ]
    }
   ],
   "source": [
    "def run_trials():\n",
    "\n",
    "    trials_step = 1  # how many additional trials to do after loading saved trials. 1 = save after iteration\n",
    "    max_trials = 1  # initial max_trials. put something small to not have to wait\n",
    "\n",
    "    \n",
    "    try:  # try to load an already saved trials object, and increase the max\n",
    "        trials = pickle.load(open(\"/home/phe002/shicheng/dataset/hyperoptModel\", \"rb\"))\n",
    "        max_trials = len(trials.trials) + trials_step\n",
    "        print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "    except:  # create a new trials object and start searching\n",
    "        trials = Trials()\n",
    "\n",
    "    best = fmin(fn=optf, space=space4xgb, algo=tpe.suggest, max_evals=max_trials, trials=trials)\n",
    "    # save the trials object\n",
    "    with open(\"/home/phe002/shicheng/dataset/hyperoptModel\", \"wb\") as f:\n",
    "        pickle.dump(trials, f)\n",
    "\n",
    "# loop indefinitely and stop whenever you like\n",
    "while True:\n",
    "    run_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1148.9639894 {'reg_alpha': 0.13614510960026047, 'colsample_bytree': 0.48613283826428166, 'scale_pos_weight': 1, 'learning_rate': 0.018415349849039475, 'nthread': 10, 'min_child_weight': 3, 'subsample': 0.8045758877474857, 'max_depth': 16, 'gamma': 3.738381824865164}\n",
    "'''RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.01,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'subsample': 0.8,\n",
    "    'alpha': 1,\n",
    "    'gamma': 1,\n",
    "    'silent': 1,\n",
    "    'verbose_eval': True,\n",
    "    'seed': RANDOM_STATE,\n",
    "    'nthread':10\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X, label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "model = xgb.train(params, xgtrain, int(2012 / 0.9), feval=evalerror)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.save_model('1114.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.exp(model.predict(xgtest)) - shift\n",
    "submission = pd.DataFrame()\n",
    "submission['loss'] = prediction\n",
    "submission['id'] = ids\n",
    "submission.to_csv('sub_v.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print \"a+b\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
