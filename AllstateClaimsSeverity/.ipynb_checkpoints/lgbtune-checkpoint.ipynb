{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold,GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import hyperopt.pyll.stochastic\n",
    "import time\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test['loss'] = np.nan\n",
    "joined = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in train.columns if x not in ['id','loss', 'log_loss']]\n",
    "\n",
    "cat_features = [x for x in train.select_dtypes(\n",
    "        include=['object']).columns if x not in ['id','loss', 'log_loss']]\n",
    "num_features = [x for x in train.select_dtypes(\n",
    "        exclude=['object']).columns if x not in ['id','loss', 'log_loss']]\n",
    "\n",
    "print \"Categorical features:\", len(cat_features)\n",
    "print \"Numerical features:\", len(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train[num_features] = train[num_features].astype(float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    preds = np.exp(preds.clip(min=0))\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "\n",
    "def xg_eval_mae(yhat, dtrain):\n",
    "    y = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(y), np.exp(yhat))\n",
    "\n",
    "def mae_score(y_true, y_pred):\n",
    "    return mean_absolute_error(np.exp(y_true), np.exp(y_pred))\n",
    "\n",
    "mae_scorer = make_scorer(mae_score, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1]\n",
      "[0 1 0]\n",
      "[0 1 2]\n",
      "[0 1 1]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'A':['type2','type1','type2'],\n",
    "                   'B':['type1','type2','type3'],\n",
    "                   'C':['type1','type3','type3']})\n",
    "print pd.factorize(df['A'].values, sort=True)[0]\n",
    "print pd.factorize(df['A'].values)[0]\n",
    "print pd.factorize(df['B'].values, sort=True)[0]\n",
    "print pd.factorize(df['C'].values, sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in list(train.select_dtypes(include=['object']).columns):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "\n",
    "        remove = (remove_train|remove_test)\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "\n",
    "        joined[column] = joined[column].apply(lambda x: np.nan if x in remove else x, 1)\n",
    "\n",
    "    joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n",
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "\n",
    "shift = 700\n",
    "y = np.log(train['loss'] + shift)\n",
    "ids = test['id']\n",
    "X = train.drop(['loss', 'id'], 1)\n",
    "X_test = test.drop(['loss', 'id'], 1)\n",
    "lgtrain = lgb.Dataset(X, label=y,categorical_feature=cat_features,free_raw_data=False)\n",
    "lgtest = lgb.Dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phe002/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:1002: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n"
     ]
    }
   ],
   "source": [
    "#1148.9639894 {'reg_alpha': 0.13614510960026047, 'colsample_bytree': 0.48613283826428166, 'scale_pos_weight': 1, 'learning_rate': 0.018415349849039475, 'nthread': 10, 'min_child_weight': 3, 'subsample': 0.8045758877474857, 'max_depth': 16, 'gamma': 3.738381824865164}\n",
    "RANDOM_STATE = 0\n",
    "'''params = {\n",
    "    'min_child_weight': 1,\n",
    "    'eta': 0.01,\n",
    "    'colsample_bytree': 0.5,\n",
    "    'max_depth': 12,\n",
    "    'subsample': 0.8,\n",
    "    'alpha': 1,\n",
    "    'gamma': 1,\n",
    "    'silent': 1,\n",
    "    'verbose_eval': True,\n",
    "    'seed': RANDOM_STATE,\n",
    "    'nthread':10\n",
    "}'''\n",
    "params={'colsample_bytree': 0.6, 'learning_rate': 0.01, \n",
    "             'min_child_weight': 6, #'n_estimators': 100, 'nthread': 1,\n",
    "            'subsample': 0.9, 'objective': 'regression', \n",
    "             'max_depth': 9, 'min_split_gain': 0.4,'reg_alpha': 0.0001,'seed':0}\n",
    "model = lgb.train(params, lgtrain, 100, feval=evalerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta_list  = [0.1]*100 + [0.05]*200 + [0.02]*400\n",
    "#model = xgb.train(params, xgtrain, 700, feval=evalerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Xt = np.zeros((1000,100))\n",
    "x0=np.arange(10000).reshape(10000,1)\n",
    "x1=np.arange(10000,20000).reshape(10000,1)\n",
    "x =np.concatenate((x0,x1),axis=1)\n",
    "y=np.arange(10000)\n",
    "yt=np.concatenate((np.zeros(500),np.ones(500)))\n",
    "lgtrain = lgb.Dataset(x, label=y)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_params={'colsample_bytree': 0.6, 'learning_rate': 0.01, \n",
    "            'min_child_weight': 6,# 'n_estimators': 100, 'n_jobs': 10, \n",
    "            'subsample': 0.9, 'objective':'regression',\n",
    "             'max_depth': 9, 'min_split_gain': 0.4,'reg_alpha': 0.0001,'seed':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phe002/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-aea41abf49e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#%%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m bst_cv = lgb.cv(lgb_params, lgtrain, num_boost_round=10, nfold=3, seed=0, stratified=False,\n\u001b[0;32m----> 3\u001b[0;31m                     feval=evalerror, early_stopping_rounds=None)\n\u001b[0m",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/lightgbm/engine.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    442\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    443\u001b[0m         \u001b[0mcvfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_agg_cv_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/lightgbm/engine.pyc\u001b[0m in \u001b[0;36mhandlerFunction\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboosters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandlerFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/lightgbm/basic.pyc\u001b[0m in \u001b[0;36meval_valid\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   1555\u001b[0m         \"\"\"\n\u001b[1;32m   1556\u001b[0m         return [item for i in range_(1, self.__num_dataset)\n\u001b[0;32m-> 1557\u001b[0;31m                 for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n\u001b[0m\u001b[1;32m   1558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/lightgbm/basic.pyc\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   1817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1818\u001b[0m                 \u001b[0mcur_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0mfeval_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_higher_better\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-90b41e0d2a76>\u001b[0m in \u001b[0;36mevalerror\u001b[0;34m(preds, dtrain)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mxg_eval_mae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/sklearn/metrics/regression.pyc\u001b[0m in \u001b[0;36mmean_absolute_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \"\"\"\n\u001b[1;32m    169\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 170\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    171\u001b[0m     output_errors = np.average(np.abs(y_pred - y_true),\n\u001b[1;32m    172\u001b[0m                                weights=sample_weight, axis=0)\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/sklearn/metrics/regression.pyc\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \"\"\"\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/phe002/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 43\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "bst_cv = lgb.cv(lgb_params, lgtrain, num_boost_round=10, nfold=3, seed=0, stratified=False,\n",
    "                    feval=evalerror, early_stopping_rounds=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "\n",
    "ax1.set_title('100 rounds of training')\n",
    "ax1.set_xlabel('Rounds')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "ax1.plot(bst_cv1[['train-mae-mean', 'test-mae-mean']])\n",
    "ax1.legend(['Training Loss', 'Test Loss'])\n",
    "\n",
    "ax2.set_title('60 last rounds of training')\n",
    "ax2.set_xlabel('Rounds')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.grid(True)\n",
    "ax2.plot(bst_cv1.iloc[1750:][['train-mae-mean', 'test-mae-mean']])\n",
    "ax2.legend(['Training Loss', 'Test Loss'])\n",
    "fig.set_size_inches(16,4)\n",
    "#fig.subplots_adjust(hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "Xt = np.zeros((1000,100))\n",
    "t = np.ones((1000,100))\n",
    "yt=np.concatenate((np.zeros(500),np.ones(500)))\n",
    "skf = StratifiedKFold(n_splits=2)\n",
    "for train_index, test_index in skf.split(Xt, yt):\n",
    "    t[test_index] = 0\n",
    "np.where(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To follow conventional function names in sklearn, we implement fit and predict functions\n",
    "class XGBoostRegressor(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.params = kwargs\n",
    "        self.num_boost_round = self.params.get('num_boost_round',120)\n",
    "        self.early_stopping_rounds = self.params.get('early_stopping_rounds',10)\n",
    "        self.params.update({'silent': 1, 'objective': 'reg:linear', 'seed': 0})\n",
    "        \n",
    "    def fit(self, x_train, y_train):\n",
    "        dtrain = xgb.DMatrix(x_train, y_train)\n",
    "        self.bst = xgb.train(params=self.params, dtrain=dtrain, \n",
    "                             num_boost_round=self.num_boost_round,\n",
    "                             feval=evalerror, maximize=False,early_stopping_rounds=None)\n",
    "        return self.bst\n",
    "        \n",
    "    def predict(self, x_pred):\n",
    "        dpred = xgb.DMatrix(x_pred)\n",
    "        return self.bst.predict(dpred)\n",
    "    \n",
    "    def kfold(self, x_train, y_train, nfold=5):\n",
    "        dtrain = xgb.DMatrix(x_train, y_train)\n",
    "        cv_rounds = xgb.cv(params=self.params, dtrain=dtrain, num_boost_round=self.num_boost_round,\n",
    "                           nfold=nfold, feval=evalerror, maximize=False, early_stopping_rounds=10)\n",
    "        return cv_rounds.iloc[-1,:]\n",
    "    \n",
    "    def plot_feature_importances(self):\n",
    "        feat_imp = pd.Series(self.bst.get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        \n",
    "    def get_params(self, deep=True):\n",
    "        return self.params\n",
    " \n",
    "    def set_params(self, **params):\n",
    "        self.params.update(params)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1157.47329818\n",
      "{'colsample_bytree': 0.5, 'learning_rate': 0.1, 'nthread': 10, 'min_child_weight': 6, 'n_estimators': 100, 'subsample': 0.5, 'objective': 'reg:linear', 'num_boost_round': 50, 'max_depth': 9, 'gamma': 0}\n",
      "CPU times: user 12min, sys: 6.41 s, total: 12min 7s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators':[100],\n",
    "     'gamma':[0],\n",
    "    'colsample_bytree': [0.5],\n",
    "    'subsample': [0.5],\n",
    "    'objective': ['reg:linear'],\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [6],\n",
    "    'nthread':[10],\n",
    "    'num_boost_round':[50]\n",
    "}\n",
    "clf=XGBoostRegressor()\n",
    "grid = GridSearchCV(clf,param_grid=xgb_params, cv=3, scoring=mae_scorer)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1156.26686109\n",
      "{'colsample_bytree': 0.5, 'learning_rate': 0.1, 'nthread': 10, 'min_child_weight': 6, 'n_estimators': 100, 'subsample': 0.5, 'objective': 'reg:linear', 'num_boost_round': 150, 'max_depth': 9, 'gamma': 0.4}\n",
      "CPU times: user 43min 4s, sys: 23.3 s, total: 43min 27s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators':[100],\n",
    "     'gamma':[ 0.1 * i for i in range(0,5)],\n",
    "    'colsample_bytree': [0.5],\n",
    "    'subsample': [0.5],\n",
    "    'objective': ['reg:linear'],\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [6],\n",
    "    'nthread':[10],\n",
    "    'num_boost_round':[150]\n",
    "}\n",
    "clf=XGBoostRegressor()\n",
    "grid = GridSearchCV(clf,param_grid=xgb_params, cv=3, scoring=mae_scorer)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1150.47117641\n",
      "{'colsample_bytree': 0.6, 'learning_rate': 0.1, 'nthread': 10, 'min_child_weight': 6, 'n_estimators': 100, 'subsample': 0.9, 'objective': 'reg:linear', 'num_boost_round': 150, 'max_depth': 9, 'gamma': 0.4}\n",
      "CPU times: user 9h 6min 51s, sys: 4min 47s, total: 9h 11min 39s\n",
      "Wall time: 57min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators':[100],\n",
    "     'gamma':[0.4],\n",
    "    'subsample':[i/10.0 for i in range(1,10)],\n",
    "     'colsample_bytree':[i/10.0 for i in range(1,10)],\n",
    "    'objective': ['reg:linear'],\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [6],\n",
    "    'nthread':[10],\n",
    "    'num_boost_round':[150]\n",
    "}\n",
    "clf=XGBoostRegressor()\n",
    "grid = GridSearchCV(clf,param_grid=xgb_params, cv=3, scoring=mae_scorer)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1150.47117641\n",
      "{'colsample_bytree': 0.6, 'learning_rate': 0.1, 'nthread': 10, 'min_child_weight': 6, 'n_estimators': 500, 'subsample': 0.9, 'objective': 'reg:linear', 'num_boost_round': 150, 'max_depth': 9, 'gamma': 0.4}\n",
      "CPU times: user 1h 11min 42s, sys: 37 s, total: 1h 12min 19s\n",
      "Wall time: 7min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.5,0.4,0.3,0.2,0.1,0.075,0.05,0.04,0.03],\n",
    "    'n_estimators':[500],\n",
    "     'gamma':[0.4],\n",
    "    'subsample':[0.9],\n",
    "     'colsample_bytree':[0.6],\n",
    "    'objective': ['reg:linear'],\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [6],\n",
    "    'nthread':[10],\n",
    "    'num_boost_round':[150]\n",
    "}\n",
    "clf=XGBoostRegressor()\n",
    "grid = GridSearchCV(clf,param_grid=xgb_params, cv=3, scoring=mae_scorer)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1149.84640685\n",
      "{'reg_alpha': 0.0001, 'colsample_bytree': 0.6, 'learning_rate': 0.1, 'nthread': 10, 'min_child_weight': 6, 'n_estimators': 100, 'subsample': 0.9, 'objective': 'reg:linear', 'num_boost_round': 150, 'max_depth': 9, 'gamma': 0.4}\n",
      "CPU times: user 1h 5min 45s, sys: 34.6 s, total: 1h 6min 19s\n",
      "Wall time: 6min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
    "xgb_params = {\n",
    "    'learning_rate': [0.1],\n",
    "    'reg_alpha':[1e-5,1e-4,1e-3, 1e-2, 0.1, 1,10,100],\n",
    "    'n_estimators':[100],\n",
    "     'gamma':[0.4],\n",
    "    'subsample':[0.9],\n",
    "     'colsample_bytree':[0.6],\n",
    "    'objective': ['reg:linear'],\n",
    "    'max_depth': [9],\n",
    "    'min_child_weight': [6],\n",
    "    'nthread':[10],\n",
    "    'num_boost_round':[150]\n",
    "}\n",
    "clf=XGBoostRegressor()\n",
    "grid = GridSearchCV(clf,param_grid=xgb_params, cv=3, scoring=mae_scorer)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print grid.best_score_\n",
    "print grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = np.exp(model.predict(xgtest)) - shift\n",
    "submission = pd.DataFrame()\n",
    "submission['loss'] = prediction\n",
    "submission['id'] = ids\n",
    "submission.to_csv('sub_v.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print \"a+b\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
