{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from keras.layers import Dense, Activation, concatenate, Reshape,Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import regularizers\n",
    "from keras.initializers import he_normal\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import selu\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.02889902,  0.01615068, -0.0252037 , ..., -0.01713032,\n",
      "         0.03840787,  0.04550501],\n",
      "       [-0.03738188,  0.00537691,  0.01648307, ...,  0.04850633,\n",
      "         0.04734459,  0.00600858],\n",
      "       [ 0.04861781,  0.03422617, -0.01783941, ...,  0.03981885,\n",
      "        -0.04601628,  0.02635058],\n",
      "       ..., \n",
      "       [-0.04797022,  0.00238442,  0.01963773, ..., -0.04194727,\n",
      "         0.0025719 ,  0.00808302],\n",
      "       [-0.03226335, -0.0359659 ,  0.00058913, ..., -0.02566978,\n",
      "        -0.0177977 , -0.0219571 ],\n",
      "       [-0.01194863, -0.03246237, -0.04122312, ..., -0.01994742,\n",
      "        -0.03012073,  0.02835709]], dtype=float32)]\n",
      "(1, 1000, 64)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "print (model.get_weights())\n",
    "print (np.array(model.get_weights()).shape)\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!',\n",
    "\t\t'Weak',\n",
    "\t\t'Poor effort!',\n",
    "\t\t'not good',\n",
    "\t\t'poor work',\n",
    "\t\t'Could have done better.']\n",
    "# define class labels\n",
    "labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "# integer encode the documents\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
