{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "import time\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "'''gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))'''\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size, shuffle):\n",
    "    #chenglong code for fiting from generator (https://www.kaggle.com/c/talkingdata-mobile-user-demographics/forums/t/22567/neural-network-for-sparse-matrices)\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:].toarray()\n",
    "        y_batch = y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch, y_batch\n",
    "        if (counter == number_of_batches):\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0\n",
    "\n",
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\n",
    "        X_batch = X[batch_index, :].toarray()\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if (counter == number_of_batches):\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[33898, 43407, 3458, 100139, 60086, 117290, 89806, 7751, 164059, 143297]\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "index = list(train.index)\n",
    "print (index[0:10])\n",
    "np.random.shuffle(index)\n",
    "print (index[0:10])\n",
    "train = train.iloc[index]\n",
    "'train = train.iloc[np.random.permutation(len(train))]'\n",
    "\n",
    "## set test loss to NaN\n",
    "test['loss'] = np.nan\n",
    "\n",
    "## response and IDs\n",
    "y = np.log(train['loss'].values+200)\n",
    "id_train = train['id'].values\n",
    "id_test = test['id'].values\n",
    "\n",
    "## stack train test\n",
    "ntrain = train.shape[0]\n",
    "tr_te = pd.concat((train, test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1153)\n",
      "(188318,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((188318, 1153), (188318,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Preprocessing and transforming to sparse data\n",
    "sparse_data = []\n",
    "\n",
    "f_cat = [f for f in tr_te.columns if 'cat' in f]\n",
    "for f in f_cat:\n",
    "    dummy = pd.get_dummies(tr_te[f].astype('category'))\n",
    "    tmp = csr_matrix(dummy)\n",
    "    sparse_data.append(tmp)\n",
    "\n",
    "f_num = [f for f in tr_te.columns if 'cont' in f]\n",
    "scaler = StandardScaler()\n",
    "tmp = csr_matrix(scaler.fit_transform(tr_te[f_num]))\n",
    "sparse_data.append(tmp)\n",
    "\n",
    "del(tr_te, train, test)\n",
    "\n",
    "## sparse train and test data\n",
    "xtr_te = hstack(sparse_data, format = 'csr')\n",
    "xtrain = xtr_te[:ntrain, :]\n",
    "xtest = xtr_te[ntrain:, :]\n",
    "\n",
    "print('Dim train', xtrain.shape)\n",
    "print('Dim test', xtest.shape)\n",
    "\n",
    "del(xtr_te, sparse_data, tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## neural net\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(400, input_dim = xtrain.shape[1], init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "        \n",
    "    model.add(Dense(200, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(50, init = 'he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(BatchNormalization())    \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(1, init = 'he_normal'))\n",
    "    model.compile(loss = 'mae', optimizer = 'adadelta')\n",
    "    return(model)\n",
    "\n",
    "## cv-folds\n",
    "nfolds = 10\n",
    "folds = KFold(len(y), n_folds = nfolds, shuffle = True, random_state = 111)\n",
    "\n",
    "## train models\n",
    "i = 0\n",
    "nbags = 10\n",
    "nepochs = 55\n",
    "pred_oob = np.zeros(xtrain.shape[0])\n",
    "pred_test = np.zeros(xtest.shape[0])\n",
    "\n",
    "for (inTr, inTe) in folds:\n",
    "    xtr = xtrain[inTr]\n",
    "    ytr = y[inTr]\n",
    "    xte = xtrain[inTe]\n",
    "    yte = y[inTe]\n",
    "    pred = np.zeros(xte.shape[0])\n",
    "    for j in range(nbags):\n",
    "        model = nn_model()\n",
    "        fit = model.fit_generator(generator = batch_generator(xtr, ytr, 128, True),\n",
    "                                  nb_epoch = nepochs,\n",
    "                                  samples_per_epoch = xtr.shape[0],\n",
    "                                  verbose = 0)\n",
    "        pred += np.exp(model.predict_generator(generator = batch_generatorp(xte, 800, False), val_samples = xte.shape[0])[:,0])-200\n",
    "        pred_test += np.exp(model.predict_generator(generator = batch_generatorp(xtest, 800, False), val_samples = xtest.shape[0])[:,0])-200\n",
    "    pred /= nbags\n",
    "    pred_oob[inTe] = pred\n",
    "    score = mean_absolute_error(np.exp(yte)-200, pred)\n",
    "    i += 1\n",
    "    print('Fold ', i, '- MAE:', score)\n",
    "\n",
    "print('Total - MAE:', mean_absolute_error(np.exp(y)-200, pred_oob))\n",
    "\n",
    "## train predictions\n",
    "df = pd.DataFrame({'id': id_train, 'loss': pred_oob})\n",
    "df.to_csv('preds_oob.csv', index = False)\n",
    "\n",
    "## test predictions\n",
    "pred_test /= (nfolds*nbags)\n",
    "df = pd.DataFrame({'id': id_test, 'loss': pred_test})\n",
    "df.to_csv('submission_keras_shift_perm.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
