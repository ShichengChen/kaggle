{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3, Multilayer Perceptron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csc/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/csc/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1153)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "cat_names = [c for c in train.columns if 'cat' in c]\n",
    "\n",
    "train = pd.get_dummies(data=train, columns=cat_names)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['id','loss']]\n",
    "\n",
    "train_x = np.array(train[features])\n",
    "\n",
    "ntrain = train_x.shape[0]\n",
    "\n",
    "# np.log(train['loss'] + 200) provides\n",
    "# a better score, but let's keep it simple now\n",
    "train_y = np.array(train['loss'])\n",
    "\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Hyperopt MLP hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a CV over manually chosen models, we may want to automate this process and search over many more combinations. Unfortunately, there's no fast way to tune the hyperparameters of a neural network. \n",
    "\n",
    "To search over the space of hyperparameters, I introduce [Hyperopt](https://github.com/hyperopt/hyperopt), a model-agnostic library for advanced parallel hyperparams optimization. Hyperopt represents a Bayesian approach to optimization: the library uses the prior knowledge to make more intelligent assumptions what combination of parameters to try next.\n",
    "\n",
    "I'll just briefly touch upon it, because it may and will take an enormous amount of time to train and validate multiple combinations of hyperparams via Hyperopt. Anyway, it's a reliable and popular library which is heavily (ab)used on Kaggle.\n",
    "\n",
    "My methodology is based on [an post on fastml.com blog](http://fastml.com/optimizing-hyperparams-with-hyperopt/), which brings a 3-step process:\n",
    "\n",
    "1. Describe the search space: all hyperparameters and their possible values.\n",
    "\n",
    "2. Implement a function to minimize (our Keras model which minimizes the loss).\n",
    "\n",
    "3. Analyze and pick scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 versions of Hyperopt search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Hyperopt code is for demo only. I commented out the code which actually starts Hyperopt.\n",
    "\n",
    "Running the following optimization will literally require **days** on any ordinary PC. To test that the code works, uncomment the lines with `fmin` function calls.  I'd recommend running it and seeing its output from a command-line: \n",
    "\n",
    "`$ tail -f hyperopt_v1.log`\n",
    "\n",
    "Then you should interrupt kernel and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define the search space for Hyperopt\n",
    "# Conditionals are used to maintain the structure of the network\n",
    "# Each hidden layer to the right has less units than the previous one\n",
    "\n",
    "# VERSION 1: two hidden layers, wide networks.\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1': hp.choice('hidden1', [\n",
    "            {\n",
    "                'hidden1_units': 256,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': 128\n",
    "                        }\n",
    "                    ]),\n",
    "                'hidden1_units': 512,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': hp.choice('hidden2_units', [128,256])\n",
    "                        }\n",
    "                    ]),\n",
    "                'hidden1_units': 768,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': hp.choice('hidden2_units', [128,256,512]),\n",
    "                        }\n",
    "                    ]),\n",
    "                'hidden1_units': 1024,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': hp.choice('hidden2_units', [128,256,512,768]),\n",
    "                        }\n",
    "                    ])\n",
    "            }]), \n",
    "        'hidden1_dropout': hp.uniform('hidden1_dropout', 0.1,0.6), \n",
    "        'hidden2_dropout': hp.uniform('hidden2_dropout', 0.1,0.5)}\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print 'Model Testing:', params\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1']['hidden1_units'], input_dim=train_x.shape[1]))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden1']['hidden2']['hidden2_units']))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "        \n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam', metrics = [\"mae\"])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization and see the results\n",
    "sys.stdout = open('hyperopt_v1.log', 'w')\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "# best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 50, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 2. Insights:\n",
    "# – increase the lower and upper bounds of dropout,\n",
    "# — the wider network doesn't seem to work well.\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1': hp.choice('hidden1', [\n",
    "            {\n",
    "                'hidden1_units': 128,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': 64\n",
    "                        }\n",
    "                    ]),\n",
    "                'hidden1_units': 256,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': hp.choice('hidden2_units', [32,64,128])\n",
    "                        }\n",
    "                    ]),\n",
    "\n",
    "                'hidden1_units': 512,\n",
    "                'hidden2': hp.choice('hidden2', [\n",
    "                        {\n",
    "                            'hidden2_units': hp.choice('hidden2_units', [32,64,128,256])\n",
    "                        }\n",
    "                    ])\n",
    "            }]), \n",
    "        'hidden1_dropout': hp.uniform('hidden1_dropout', 0.3,0.7), # Several rounds of hyperopt\n",
    "        'hidden2_dropout': hp.uniform('hidden2_dropout', 0.2,0.6)} # resulted in these values\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print 'Model Testing:', params\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1']['hidden1_units'], input_dim=train_x.shape[1]))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden1']['hidden2']['hidden2_units']))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "        \n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam', metrics = [\"mae\"])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization and see the results\n",
    "sys.stdout = open('hyperopt_v2.log', 'w')\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "# best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 50, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 3. Insights:\n",
    "# — models with two hidden layers and 256-128 units configuration work well\n",
    "# — let's optimize the number of units in layers with a more precise step\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int16')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int16'))\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print 'Model Testing:', params\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1]))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units']))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam', metrics = [\"mae\"])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization and see the results\n",
    "sys.stdout = open('hyperopt_v3.log', 'w')\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "# best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 50, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSION 4. Insights:\n",
    "# – why not to test 4-layer architectures?\n",
    "# — we need to introduce new optimizers\n",
    "# — adding batch normalization (https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "        'hidden3_dropout': hp.choice('hidden3_dropout', np.linspace(0.1,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int16')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int16')),\n",
    "         'hidden3_units': hp.choice('hidden3_units', np.linspace(20,80,30,dtype='int16')),\n",
    "         'optimizer': hp.choice('optimizer', ['adam','nadam','adamax','adadelta'])\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print 'Model Testing:', params\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units']))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(params['hidden3_units'])) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden3_dropout']))\n",
    "        \n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer=params['optimizer'])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "# Run the optimization and see the results\n",
    "sys.stdout = open('hyperopt_v4.log', 'w')\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "# best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 50, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: The final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took several rounds of optimization to narrow down the parameters of the model. Here are the results.\n",
    "\n",
    "First, the architecture. The final 4-layer model uses dropout as a regularization and a batch normalization prior to each hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cdn.rawgit.com/dnkirill/allstate_capstone/master/images/mlp3.svg\"></td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyper_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(351, input_dim=train_x.shape[1], init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.578947))\n",
    "    \n",
    "    model.add(Dense(293, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.26666))\n",
    "    \n",
    "    model.add(Dense(46, init='glorot_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.188888))\n",
    "    \n",
    "    model.add(Dense(1, init='glorot_normal'))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED:\n",
    "    with open('pretrained/mlp_f_score.pkl', 'rb') as f:\n",
    "        cv_score = pickle.load(f)\n",
    "else:\n",
    "    sys.stdout = open('mlp_final_out.txt', 'w')\n",
    "    cv_score = cross_validate_mlp(hyper_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = _stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for the final model: 1150.0096524\n"
     ]
    }
   ],
   "source": [
    "print \"CV score for the final model:\", cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
