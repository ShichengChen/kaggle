{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3, Multilayer Perceptron Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1153)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "cat_names = [c for c in train.columns if 'cat' in c]\n",
    "\n",
    "train = pd.get_dummies(data=train, columns=cat_names)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['id','loss']]\n",
    "\n",
    "train_x = np.array(train[features])\n",
    "\n",
    "ntrain = train_x.shape[0]\n",
    "\n",
    "# np.log(train['loss'] + 200) provides\n",
    "# a better score, but let's keep it simple now\n",
    "train_y = np.array(train['loss'])\n",
    "\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=30, verbose=0, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 455, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 272, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 69, 'optimizer': 'adadelta', 'wdecay': 0.032330000000000005}\n",
      "Fold 0, MAE: 1163.148839039124\n",
      "Fold 1, MAE: 1159.9899238609737\n",
      "Fold 2, MAE: 1140.9771315408389\n",
      "3-fold CV score: 1154.7052981469787\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 541, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 134, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 30, 'optimizer': 'adadelta', 'wdecay': 0.082830000000000001}\n",
      "Fold 0, MAE: 1181.2090776051534\n",
      "Fold 1, MAE: 1175.7242010975745\n",
      "Fold 2, MAE: 1149.1617817128606\n",
      "3-fold CV score: 1168.698353471863\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 541, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 113, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 22, 'optimizer': 'adam', 'wdecay': 0.010110000000000001}\n",
      "Fold 0, MAE: 1161.0922305512295\n",
      "Fold 1, MAE: 1171.8751691438747\n",
      "Fold 2, MAE: 1142.5130553590734\n",
      "3-fold CV score: 1158.4934850180591\n",
      "Model Testing: {'hidden1_dropout': 0.56842105263157894, 'hidden1_units': 351, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 168, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 44, 'optimizer': 'adam', 'wdecay': 0.066669999999999993}\n",
      "Fold 0, MAE: 1175.3901692187044\n",
      "Fold 1, MAE: 1176.3282762184622\n",
      "Fold 2, MAE: 1171.493253623609\n",
      "3-fold CV score: 1174.4038996869251\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 351, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 237, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.0040499999999999998}\n",
      "Fold 0, MAE: 1167.663476455973\n",
      "Fold 1, MAE: 1158.721241361386\n",
      "Fold 2, MAE: 1148.2196995976833\n",
      "3-fold CV score: 1158.2014724716807\n",
      "Model Testing: {'hidden1_dropout': 0.45263157894736844, 'hidden1_units': 437, 'hidden2_dropout': 0.5, 'hidden2_units': 120, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 30, 'optimizer': 'adam', 'wdecay': 0.055560000000000005}\n",
      "Fold 0, MAE: 1192.865180594892\n",
      "Fold 1, MAE: 1170.481112811505\n",
      "Fold 2, MAE: 1171.0399158641085\n",
      "3-fold CV score: 1178.128736423502\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 368, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 237, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 34, 'optimizer': 'adadelta', 'wdecay': 0.082830000000000001}\n",
      "Fold 0, MAE: 1163.6607907634843\n",
      "Fold 1, MAE: 1170.6899247157944\n",
      "Fold 2, MAE: 1149.6886504229296\n",
      "3-fold CV score: 1161.3464553007361\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 394, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 217, 'hidden3_dropout': 0.5, 'hidden3_units': 26, 'optimizer': 'adadelta', 'wdecay': 0.086870000000000003}\n",
      "Fold 0, MAE: 1186.010471694171\n",
      "Fold 1, MAE: 1194.6887222751257\n",
      "Fold 2, MAE: 1178.986956053622\n",
      "3-fold CV score: 1186.5620500076395\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 550, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 175, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 30, 'optimizer': 'adadelta', 'wdecay': 0.054550000000000008}\n",
      "Fold 0, MAE: 1173.4874602173836\n",
      "Fold 1, MAE: 1168.5583186899196\n",
      "Fold 2, MAE: 1157.0938108456683\n",
      "3-fold CV score: 1166.3798632509906\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 420, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 272, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 20, 'optimizer': 'adam', 'wdecay': 0.051520000000000003}\n",
      "Fold 0, MAE: 1187.3547460026052\n",
      "Fold 1, MAE: 1165.0640505257604\n",
      "Fold 2, MAE: 1152.0578003723883\n",
      "3-fold CV score: 1168.1588656335846\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 506, 'hidden2_dropout': 0.5, 'hidden2_units': 155, 'hidden3_dropout': 0.5, 'hidden3_units': 38, 'optimizer': 'adam', 'wdecay': 0.0030400000000000002}\n",
      "Fold 0, MAE: 1165.5917111922117\n",
      "Fold 1, MAE: 1157.0224820727203\n",
      "Fold 2, MAE: 1145.9990145378135\n",
      "3-fold CV score: 1156.2044026009153\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 550, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 217, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 46, 'optimizer': 'adadelta', 'wdecay': 0.053540000000000004}\n",
      "Fold 0, MAE: 1178.285182548254\n",
      "Fold 1, MAE: 1161.227306544953\n",
      "Fold 2, MAE: 1146.3369486137735\n",
      "3-fold CV score: 1161.9498125689936\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 308, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 100, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 48, 'optimizer': 'adadelta', 'wdecay': 0.083839999999999998}\n",
      "Fold 0, MAE: 1175.8002146167726\n",
      "Fold 1, MAE: 1172.0227174765078\n",
      "Fold 2, MAE: 1154.2503788092345\n",
      "3-fold CV score: 1167.3577703008384\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 446, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 217, 'hidden3_dropout': 0.5, 'hidden3_units': 22, 'optimizer': 'adadelta', 'wdecay': 0.089899999999999994}\n",
      "Fold 0, MAE: 1198.0498968117809\n",
      "Fold 1, MAE: 1192.423294968826\n",
      "Fold 2, MAE: 1191.1537727940815\n",
      "3-fold CV score: 1193.8756548582294\n",
      "Model Testing: {'hidden1_dropout': 0.40000000000000002, 'hidden1_units': 541, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 258, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 75, 'optimizer': 'adadelta', 'wdecay': 0.02324}\n",
      "Fold 0, MAE: 1162.2846737751456\n",
      "Fold 1, MAE: 1157.0434973514984\n",
      "Fold 2, MAE: 1149.8406610896955\n",
      "3-fold CV score: 1156.3896107387798\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 412, 'hidden2_dropout': 0.5, 'hidden2_units': 203, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 24, 'optimizer': 'adadelta', 'wdecay': 0.065659999999999996}\n",
      "Fold 0, MAE: 1162.6868081304262\n",
      "Fold 1, MAE: 1207.019499323167\n",
      "Fold 2, MAE: 1164.6480082339472\n",
      "3-fold CV score: 1178.1181052291802\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 386, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 113, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 44, 'optimizer': 'adam', 'wdecay': 0.01213}\n",
      "Fold 0, MAE: 1162.20426789193\n",
      "Fold 1, MAE: 1165.871551874401\n",
      "Fold 2, MAE: 1144.635762708466\n",
      "3-fold CV score: 1157.570527491599\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 489, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 272, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 42, 'optimizer': 'adadelta', 'wdecay': 0.047480000000000008}\n",
      "Fold 0, MAE: 1159.1230678129568\n",
      "Fold 1, MAE: 1170.1456261211185\n",
      "Fold 2, MAE: 1141.1387421620404\n",
      "3-fold CV score: 1156.8024786987053\n",
      "Model Testing: {'hidden1_dropout': 0.47368421052631582, 'hidden1_units': 368, 'hidden2_dropout': 0.5, 'hidden2_units': 196, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 61, 'optimizer': 'adam', 'wdecay': 0.076770000000000005}\n",
      "Fold 0, MAE: 1167.3144952524326\n",
      "Fold 1, MAE: 1158.442539164428\n",
      "Fold 2, MAE: 1170.1439974149687\n",
      "3-fold CV score: 1165.3003439439433\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 325, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.070709999999999995}\n",
      "Fold 0, MAE: 1161.177618771013\n",
      "Fold 1, MAE: 1157.4865109404855\n",
      "Fold 2, MAE: 1143.5037899814306\n",
      "3-fold CV score: 1154.0559732309764\n",
      "Model Testing: {'hidden1_dropout': 0.58947368421052637, 'hidden1_units': 325, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 148, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 69, 'optimizer': 'adadelta', 'wdecay': 0.070709999999999995}\n",
      "Fold 0, MAE: 1169.0057906375498\n",
      "Fold 1, MAE: 1163.4061472146307\n",
      "Fold 2, MAE: 1150.80821222944\n",
      "3-fold CV score: 1161.0733833605402\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 455, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.032330000000000005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, MAE: 1160.1148852933434\n",
      "Fold 1, MAE: 1153.5388360438872\n",
      "Fold 2, MAE: 1146.284142179796\n",
      "3-fold CV score: 1153.312621172342\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 325, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.058590000000000003}\n",
      "Fold 0, MAE: 1159.533359263806\n",
      "Fold 1, MAE: 1154.5401700530679\n",
      "Fold 2, MAE: 1152.34695090863\n",
      "3-fold CV score: 1155.473493408501\n",
      "Model Testing: {'hidden1_dropout': 0.43157894736842106, 'hidden1_units': 377, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 189, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.081820000000000004}\n",
      "Fold 0, MAE: 1162.9307342647628\n",
      "Fold 1, MAE: 1177.8290937843656\n",
      "Fold 2, MAE: 1148.0584482485735\n",
      "3-fold CV score: 1162.9394254325673\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 455, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 162, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.091920000000000002}\n",
      "Fold 0, MAE: 1161.4301880915023\n",
      "Fold 1, MAE: 1169.6419220691284\n",
      "Fold 2, MAE: 1144.48012878803\n",
      "3-fold CV score: 1158.5174129828868\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 472, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 279, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 77, 'optimizer': 'adadelta', 'wdecay': 0.069699999999999998}\n",
      "Fold 0, MAE: 1158.2613808574185\n",
      "Fold 1, MAE: 1156.7999195958928\n",
      "Fold 2, MAE: 1153.1107720208292\n",
      "3-fold CV score: 1156.0573574913803\n",
      "Model Testing: {'hidden1_dropout': 0.4631578947368421, 'hidden1_units': 524, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 265, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 40, 'optimizer': 'adadelta', 'wdecay': 0.094950000000000007}\n",
      "Fold 0, MAE: 1195.4596557820985\n",
      "Fold 1, MAE: 1166.5828679966726\n",
      "Fold 2, MAE: 1151.5350493940605\n",
      "3-fold CV score: 1171.1925243909438\n",
      "Model Testing: {'hidden1_dropout': 0.51578947368421058, 'hidden1_units': 498, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 231, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.08788}\n",
      "Fold 0, MAE: 1166.894567545835\n",
      "Fold 1, MAE: 1158.6302274524644\n",
      "Fold 2, MAE: 1146.4025209941412\n",
      "3-fold CV score: 1157.3091053308135\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 334, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 106, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.035360000000000003}\n",
      "Fold 0, MAE: 1156.7171541562705\n",
      "Fold 1, MAE: 1158.617184146834\n",
      "Fold 2, MAE: 1140.9087310499317\n",
      "3-fold CV score: 1152.0810231176786\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 515, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 286, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 71, 'optimizer': 'adadelta', 'wdecay': 0.035360000000000003}\n",
      "Fold 0, MAE: 1166.3465921313127\n",
      "Fold 1, MAE: 1155.4647161447238\n",
      "Fold 2, MAE: 1145.8189915304222\n",
      "3-fold CV score: 1155.8767666021529\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 334, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 106, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 65, 'optimizer': 'adadelta', 'wdecay': 0.052530000000000007}\n",
      "Fold 0, MAE: 1165.2972734181083\n",
      "Fold 1, MAE: 1162.3415273140827\n",
      "Fold 2, MAE: 1143.2022154553692\n",
      "3-fold CV score: 1156.9470053958535\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.00809}\n",
      "Fold 0, MAE: 1159.2449543552357\n",
      "Fold 1, MAE: 1154.8565114282767\n",
      "Fold 2, MAE: 1140.933696843291\n",
      "3-fold CV score: 1151.6783875422677\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 334, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.014149999999999999}\n",
      "Fold 0, MAE: 1165.0503812670863\n",
      "Fold 1, MAE: 1157.1801844593206\n",
      "Fold 2, MAE: 1141.5077789837922\n",
      "3-fold CV score: 1154.579448236733\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.00809}\n",
      "Fold 0, MAE: 1162.7622893053533\n",
      "Fold 1, MAE: 1156.484460623302\n",
      "Fold 2, MAE: 1141.2796353609326\n",
      "3-fold CV score: 1153.5087950965292\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 141, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 53, 'optimizer': 'adam', 'wdecay': 0.025260000000000001}\n",
      "Fold 0, MAE: 1173.398777121047\n",
      "Fold 1, MAE: 1163.4376312509926\n",
      "Fold 2, MAE: 1149.4355696684008\n",
      "3-fold CV score: 1162.0906593468135\n",
      "Model Testing: {'hidden1_dropout': 0.56842105263157894, 'hidden1_units': 343, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 224, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 57, 'optimizer': 'adadelta', 'wdecay': 0.01617}\n",
      "Fold 0, MAE: 1157.8129235479112\n",
      "Fold 1, MAE: 1162.9604955281113\n",
      "Fold 2, MAE: 1137.8848549871118\n",
      "3-fold CV score: 1152.886091354378\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 463, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 210, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 67, 'optimizer': 'adam', 'wdecay': 0.028289999999999999}\n",
      "Fold 0, MAE: 1167.1458696599193\n",
      "Fold 1, MAE: 1160.2728405505222\n",
      "Fold 2, MAE: 1154.656251596023\n",
      "3-fold CV score: 1160.6916539354881\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 360, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 300, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 32, 'optimizer': 'adadelta', 'wdecay': 0.071720000000000006}\n",
      "Fold 0, MAE: 1157.0820389802964\n",
      "Fold 1, MAE: 1173.8628986425185\n",
      "Fold 2, MAE: 1147.7328339035314\n",
      "3-fold CV score: 1159.5592571754487\n",
      "Model Testing: {'hidden1_dropout': 0.45263157894736844, 'hidden1_units': 317, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 293, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 36, 'optimizer': 'adadelta', 'wdecay': 0.027280000000000002}\n",
      "Fold 0, MAE: 1162.285383334579\n",
      "Fold 1, MAE: 1160.938020819047\n",
      "Fold 2, MAE: 1145.0485755819968\n",
      "3-fold CV score: 1156.0906599118741\n",
      "Model Testing: {'hidden1_dropout': 0.51578947368421058, 'hidden1_units': 429, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 244, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 80, 'optimizer': 'adam', 'wdecay': 0.097979999999999998}\n",
      "Fold 0, MAE: 1187.3501504499832\n",
      "Fold 1, MAE: 1177.8064867953847\n",
      "Fold 2, MAE: 1153.1573311127709\n",
      "3-fold CV score: 1172.7713227860463\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1157.9619118652226\n",
      "Fold 1, MAE: 1155.7216872165664\n",
      "Fold 2, MAE: 1136.927755550967\n",
      "3-fold CV score: 1150.2037848775853\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1156.0632860191683\n",
      "Fold 1, MAE: 1151.4671627447715\n",
      "Fold 2, MAE: 1140.1617350157348\n",
      "3-fold CV score: 1149.230727926558\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adam', 'wdecay': 0.059600000000000007}\n",
      "Fold 0, MAE: 1168.5160545404665\n",
      "Fold 1, MAE: 1175.4194884359572\n",
      "Fold 2, MAE: 1152.5397784139907\n",
      "3-fold CV score: 1165.4917737968046\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.062630000000000005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, MAE: 1163.881212321844\n",
      "Fold 1, MAE: 1164.1301011722203\n",
      "Fold 2, MAE: 1150.2623383121809\n",
      "3-fold CV score: 1159.4245506020818\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 532, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 251, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adadelta', 'wdecay': 0.074749999999999997}\n",
      "Fold 0, MAE: 1161.926003949864\n",
      "Fold 1, MAE: 1171.028653969578\n",
      "Fold 2, MAE: 1147.243582406554\n",
      "3-fold CV score: 1160.0660801086653\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 127, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adam', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1157.4905136832626\n",
      "Fold 1, MAE: 1157.7959576055189\n",
      "Fold 2, MAE: 1145.165175664648\n",
      "3-fold CV score: 1153.4838823178097\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1154.1358029742491\n",
      "Fold 1, MAE: 1150.8801407907292\n",
      "Fold 2, MAE: 1136.143128436801\n",
      "3-fold CV score: 1147.0530240672597\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 134, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.088889999999999997}\n",
      "Fold 0, MAE: 1174.4271644173484\n",
      "Fold 1, MAE: 1160.2759364815581\n",
      "Fold 2, MAE: 1143.464337864193\n",
      "3-fold CV score: 1159.3891462543666\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 403, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 120, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 26, 'optimizer': 'adadelta', 'wdecay': 0.079799999999999996}\n",
      "Fold 0, MAE: 1177.3892872402876\n",
      "Fold 1, MAE: 1162.1597504698693\n",
      "Fold 2, MAE: 1156.8215401050213\n",
      "3-fold CV score: 1165.456859271726\n",
      "Model Testing: {'hidden1_dropout': 0.40000000000000002, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 168, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 73, 'optimizer': 'adam', 'wdecay': 0.072730000000000003}\n",
      "Fold 0, MAE: 1210.498220541357\n",
      "Fold 1, MAE: 1184.6350615099377\n",
      "Fold 2, MAE: 1151.241476172358\n",
      "3-fold CV score: 1182.1249194078841\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 351, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 182, 'hidden3_dropout': 0.5, 'hidden3_units': 20, 'optimizer': 'adadelta', 'wdecay': 0.0293}\n",
      "Fold 0, MAE: 1171.7247792783894\n",
      "Fold 1, MAE: 1176.3316548287394\n",
      "Fold 2, MAE: 1154.8692943033595\n",
      "3-fold CV score: 1167.6419094701625\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 394, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 155, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 34, 'optimizer': 'adadelta', 'wdecay': 0.01516}\n",
      "Fold 0, MAE: 1168.154443232974\n",
      "Fold 1, MAE: 1157.7412081981763\n",
      "Fold 2, MAE: 1141.040754590281\n",
      "3-fold CV score: 1155.6454686738105\n",
      "Model Testing: {'hidden1_dropout': 0.47368421052631582, 'hidden1_units': 420, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 237, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.046470000000000004}\n",
      "Fold 0, MAE: 1164.3777441806067\n",
      "Fold 1, MAE: 1177.209987984398\n",
      "Fold 2, MAE: 1155.6756505514757\n",
      "3-fold CV score: 1165.7544609054933\n",
      "Model Testing: {'hidden1_dropout': 0.58947368421052637, 'hidden1_units': 437, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 175, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 38, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1156.644832466085\n",
      "Fold 1, MAE: 1157.6123849948424\n",
      "Fold 2, MAE: 1137.0233535830582\n",
      "3-fold CV score: 1150.4268570146621\n",
      "Model Testing: {'hidden1_dropout': 0.43157894736842106, 'hidden1_units': 506, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 100, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 46, 'optimizer': 'adadelta', 'wdecay': 0.018190000000000001}\n",
      "Fold 0, MAE: 1156.7325890863394\n",
      "Fold 1, MAE: 1163.3614326980687\n",
      "Fold 2, MAE: 1140.6892231279783\n",
      "3-fold CV score: 1153.5944149707955\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 308, 'hidden2_dropout': 0.5, 'hidden2_units': 203, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 48, 'optimizer': 'adadelta', 'wdecay': 0.067680000000000004}\n",
      "Fold 0, MAE: 1187.777674460418\n",
      "Fold 1, MAE: 1164.7113053835535\n",
      "Fold 2, MAE: 1148.0043875643821\n",
      "3-fold CV score: 1166.8311224694514\n",
      "Model Testing: {'hidden1_dropout': 0.4631578947368421, 'hidden1_units': 386, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.080810000000000007}\n",
      "Fold 0, MAE: 1165.0194917910676\n",
      "Fold 1, MAE: 1158.2729473606164\n",
      "Fold 2, MAE: 1152.9800377245338\n",
      "3-fold CV score: 1158.7574922920726\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 412, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 196, 'hidden3_dropout': 0.5, 'hidden3_units': 75, 'optimizer': 'adam', 'wdecay': 0.038390000000000007}\n",
      "Fold 0, MAE: 1161.5217347156527\n",
      "Fold 1, MAE: 1162.386833252451\n",
      "Fold 2, MAE: 1145.6365952047393\n",
      "3-fold CV score: 1156.5150543909476\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 446, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 148, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.096970000000000001}\n",
      "Fold 0, MAE: 1160.2965376990726\n",
      "Fold 1, MAE: 1168.4996048508942\n",
      "Fold 2, MAE: 1149.110823150734\n",
      "3-fold CV score: 1159.3023219002337\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 489, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 113, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 42, 'optimizer': 'adadelta', 'wdecay': 0.060610000000000004}\n",
      "Fold 0, MAE: 1183.286138317614\n",
      "Fold 1, MAE: 1177.9553902909124\n",
      "Fold 2, MAE: 1155.3975773011248\n",
      "3-fold CV score: 1172.2130353032169\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 550, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 162, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 24, 'optimizer': 'adadelta', 'wdecay': 0.056570000000000002}\n",
      "Fold 0, MAE: 1161.7203151536628\n",
      "Fold 1, MAE: 1179.3700442916124\n"
     ]
    }
   ],
   "source": [
    "# VERSION 4. Insights:\n",
    "# – why not to test 4-layer architectures?\n",
    "# — we need to introduce new optimizers\n",
    "# — adding batch normalization (https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "        'hidden3_dropout': hp.choice('hidden3_dropout', np.linspace(0.1,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int32')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int32')),\n",
    "         'hidden3_units': hp.choice('hidden3_units', np.linspace(20,80,30,dtype='int32')),\n",
    "         'optimizer': hp.choice('optimizer', ['adam','adadelta']),\n",
    "         'wdecay':hp.choice('wdecay', np.linspace(0.00001,0.1,1000)),\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print ('Model Testing:', params)\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1], init='he_normal',\n",
    "                        W_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units'], init='he_normal',W_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(params['hidden3_units'], init='he_normal',W_regularizer=l2(params['wdecay']))) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden3_dropout']))\n",
    "        \n",
    "        model.add(Dense(1, init='he_normal',W_regularizer=l2(params['wdecay'])))\n",
    "        model.compile(loss='mae', optimizer=params['optimizer'])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 100, trials=trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Step 6: The final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took several rounds of optimization to narrow down the parameters of the model. Here are the results.\n",
    "\n",
    "First, the architecture. The final 4-layer model uses dropout as a regularization and a batch normalization prior to each hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cdn.rawgit.com/dnkirill/allstate_capstone/master/images/mlp3.svg\"></td>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=5):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=30, verbose=1, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "def hyper_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(437, input_dim=train_x.shape[1], init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.536))\n",
    "    \n",
    "    model.add(Dense(182, init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(73, init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.233))\n",
    "    \n",
    "    model.add(Dense(1, init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(437, input_dim=1153, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  \"\"\"\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(182, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(73, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120523 samples, validate on 30131 samples\n",
      "Epoch 1/30\n",
      "120523/120523 [==============================] - 10s 87us/step - loss: 2110.2391 - val_loss: 1197.0431\n",
      "Epoch 2/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1324.4037 - val_loss: 1175.1480\n",
      "Epoch 3/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1300.8877 - val_loss: 1170.7676\n",
      "Epoch 4/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1293.9524 - val_loss: 1162.6197\n",
      "Epoch 5/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1278.0474 - val_loss: 1159.6682\n",
      "Epoch 6/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1279.1510 - val_loss: 1154.3661\n",
      "Epoch 7/30\n",
      "120523/120523 [==============================] - 9s 79us/step - loss: 1275.6025 - val_loss: 1154.0299\n",
      "Epoch 8/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1264.7699 - val_loss: 1151.0177\n",
      "Epoch 9/30\n",
      "120523/120523 [==============================] - 10s 81us/step - loss: 1261.7134 - val_loss: 1149.9987\n",
      "Epoch 10/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1257.2884 - val_loss: 1147.9958\n",
      "Epoch 11/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1253.8893 - val_loss: 1148.3996\n",
      "Epoch 12/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1245.8143 - val_loss: 1147.9791\n",
      "Epoch 13/30\n",
      "120523/120523 [==============================] - 10s 81us/step - loss: 1246.5476 - val_loss: 1144.0683\n",
      "Epoch 14/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1239.0972 - val_loss: 1143.3708\n",
      "Epoch 15/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1234.8523 - val_loss: 1147.5229\n",
      "Epoch 16/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1230.2166 - val_loss: 1141.6999\n",
      "Epoch 17/30\n",
      "120523/120523 [==============================] - 9s 79us/step - loss: 1232.0711 - val_loss: 1144.6186\n",
      "Epoch 18/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1224.3803 - val_loss: 1142.3976\n",
      "Epoch 19/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1219.1504 - val_loss: 1142.3538\n",
      "Epoch 20/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1215.5281 - val_loss: 1143.7729\n",
      "Epoch 21/30\n",
      "120523/120523 [==============================] - 9s 77us/step - loss: 1212.9943 - val_loss: 1145.3209\n",
      "Fold 0, MAE: 1157.8927767760244\n",
      "Train on 120523 samples, validate on 30131 samples\n",
      "Epoch 1/30\n",
      "120523/120523 [==============================] - 10s 83us/step - loss: 2139.8835 - val_loss: 1224.5449\n",
      "Epoch 2/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1352.4641 - val_loss: 1169.3085\n",
      "Epoch 3/30\n",
      "120523/120523 [==============================] - 10s 81us/step - loss: 1333.3364 - val_loss: 1164.8906\n",
      "Epoch 4/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1318.7354 - val_loss: 1157.0082\n",
      "Epoch 5/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1315.1019 - val_loss: 1155.8698\n",
      "Epoch 6/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1305.4001 - val_loss: 1153.0055\n",
      "Epoch 7/30\n",
      "120523/120523 [==============================] - 10s 81us/step - loss: 1302.4857 - val_loss: 1151.2674\n",
      "Epoch 8/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1294.7493 - val_loss: 1149.9034\n",
      "Epoch 9/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1286.9761 - val_loss: 1146.8180\n",
      "Epoch 10/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1278.7119 - val_loss: 1147.9614\n",
      "Epoch 11/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1275.4224 - val_loss: 1142.6669\n",
      "Epoch 12/30\n",
      "120523/120523 [==============================] - 9s 79us/step - loss: 1270.4690 - val_loss: 1144.9337\n",
      "Epoch 13/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1265.3817 - val_loss: 1140.2971\n",
      "Epoch 14/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1260.6658 - val_loss: 1142.1946\n",
      "Epoch 15/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1260.9419 - val_loss: 1138.9245\n",
      "Epoch 16/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1254.2588 - val_loss: 1142.2211\n",
      "Epoch 17/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1247.7875 - val_loss: 1138.0360\n",
      "Epoch 18/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1242.9680 - val_loss: 1136.9435\n",
      "Epoch 19/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1240.6452 - val_loss: 1138.9480\n",
      "Epoch 20/30\n",
      "120523/120523 [==============================] - 9s 77us/step - loss: 1238.2638 - val_loss: 1140.5329\n",
      "Epoch 21/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1228.3514 - val_loss: 1138.0111\n",
      "Epoch 22/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1225.9706 - val_loss: 1137.0124\n",
      "Epoch 23/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1221.2098 - val_loss: 1139.5125\n",
      "Fold 1, MAE: 1147.8425960229633\n",
      "Train on 120523 samples, validate on 30131 samples\n",
      "Epoch 1/30\n",
      "120523/120523 [==============================] - 11s 88us/step - loss: 2169.5086 - val_loss: 1204.3094\n",
      "Epoch 2/30\n",
      "120523/120523 [==============================] - 10s 82us/step - loss: 1370.3818 - val_loss: 1181.3530\n",
      "Epoch 3/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1349.0176 - val_loss: 1165.8722\n",
      "Epoch 4/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1337.8187 - val_loss: 1168.2315\n",
      "Epoch 5/30\n",
      "120523/120523 [==============================] - 9s 79us/step - loss: 1331.6595 - val_loss: 1159.0115\n",
      "Epoch 6/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1320.3929 - val_loss: 1154.2599\n",
      "Epoch 7/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1314.6393 - val_loss: 1156.6191\n",
      "Epoch 8/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1308.9008 - val_loss: 1151.2942\n",
      "Epoch 9/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1302.5426 - val_loss: 1149.2379\n",
      "Epoch 10/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1297.6606 - val_loss: 1148.9235\n",
      "Epoch 11/30\n",
      "120523/120523 [==============================] - 10s 81us/step - loss: 1291.7460 - val_loss: 1147.0063\n",
      "Epoch 12/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1284.1899 - val_loss: 1146.0605\n",
      "Epoch 13/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1279.2934 - val_loss: 1144.1217\n",
      "Epoch 14/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1274.7437 - val_loss: 1146.2598\n",
      "Epoch 15/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1268.5180 - val_loss: 1146.9622\n",
      "Epoch 16/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1262.0862 - val_loss: 1147.0632\n",
      "Epoch 17/30\n",
      "120523/120523 [==============================] - 9s 79us/step - loss: 1259.2545 - val_loss: 1142.8214\n",
      "Epoch 18/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1251.8894 - val_loss: 1142.8127\n",
      "Epoch 19/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1247.7466 - val_loss: 1140.7313\n",
      "Epoch 20/30\n",
      "120523/120523 [==============================] - 10s 79us/step - loss: 1245.6316 - val_loss: 1139.8069\n",
      "Epoch 21/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1239.8448 - val_loss: 1144.7733\n",
      "Epoch 22/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1231.4997 - val_loss: 1140.2639\n",
      "Epoch 23/30\n",
      "120523/120523 [==============================] - 9s 78us/step - loss: 1231.7294 - val_loss: 1141.2281\n",
      "Epoch 24/30\n",
      "120523/120523 [==============================] - 10s 80us/step - loss: 1228.0471 - val_loss: 1140.2578\n",
      "Epoch 25/30\n",
      "120523/120523 [==============================] - 9s 79us/step - loss: 1221.4062 - val_loss: 1140.2420\n",
      "Fold 2, MAE: 1145.8194055484328\n",
      "Train on 120524 samples, validate on 30131 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120524/120524 [==============================] - 11s 88us/step - loss: 2309.3518 - val_loss: 1204.4172\n",
      "Epoch 2/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1328.8722 - val_loss: 1172.1232\n",
      "Epoch 3/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1301.3722 - val_loss: 1165.6668\n",
      "Epoch 4/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1288.8669 - val_loss: 1155.5804\n",
      "Epoch 5/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1287.3586 - val_loss: 1153.7166\n",
      "Epoch 6/30\n",
      "120524/120524 [==============================] - 9s 78us/step - loss: 1277.0266 - val_loss: 1151.5029\n",
      "Epoch 7/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1276.8890 - val_loss: 1147.0473\n",
      "Epoch 8/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1265.8273 - val_loss: 1148.0210\n",
      "Epoch 9/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1260.6379 - val_loss: 1145.8397\n",
      "Epoch 10/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1256.5958 - val_loss: 1144.0210\n",
      "Epoch 11/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1251.1111 - val_loss: 1142.9174\n",
      "Epoch 12/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1247.1106 - val_loss: 1141.5048\n",
      "Epoch 13/30\n",
      "120524/120524 [==============================] - 9s 79us/step - loss: 1242.0846 - val_loss: 1140.2577\n",
      "Epoch 14/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1239.3181 - val_loss: 1139.8825\n",
      "Epoch 15/30\n",
      "120524/120524 [==============================] - 9s 78us/step - loss: 1233.3052 - val_loss: 1139.7550\n",
      "Epoch 16/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1230.5145 - val_loss: 1137.3097\n",
      "Epoch 17/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1227.8559 - val_loss: 1141.3984\n",
      "Epoch 18/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1224.0645 - val_loss: 1138.8894\n",
      "Epoch 19/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1220.7952 - val_loss: 1137.2148\n",
      "Epoch 20/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1215.3049 - val_loss: 1138.7385\n",
      "Epoch 21/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1209.9910 - val_loss: 1137.2053\n",
      "Epoch 22/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1211.2480 - val_loss: 1137.0756\n",
      "Epoch 23/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1207.9591 - val_loss: 1137.6922\n",
      "Epoch 24/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1200.9708 - val_loss: 1137.7421\n",
      "Epoch 25/30\n",
      "120524/120524 [==============================] - 9s 79us/step - loss: 1196.1285 - val_loss: 1139.0863\n",
      "Epoch 26/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1190.7972 - val_loss: 1135.7609\n",
      "Epoch 27/30\n",
      "120524/120524 [==============================] - 9s 78us/step - loss: 1192.6278 - val_loss: 1137.2014\n",
      "Epoch 28/30\n",
      "120524/120524 [==============================] - 9s 78us/step - loss: 1187.1783 - val_loss: 1135.9216\n",
      "Epoch 29/30\n",
      "120524/120524 [==============================] - 9s 79us/step - loss: 1187.7776 - val_loss: 1139.0797\n",
      "Epoch 30/30\n",
      "120524/120524 [==============================] - 9s 79us/step - loss: 1183.9472 - val_loss: 1138.1771\n",
      "Fold 3, MAE: 1144.8897001632513\n",
      "Train on 120524 samples, validate on 30131 samples\n",
      "Epoch 1/30\n",
      "120524/120524 [==============================] - 10s 86us/step - loss: 2217.6084 - val_loss: 1262.4328\n",
      "Epoch 2/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1475.5226 - val_loss: 1213.5848\n",
      "Epoch 3/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1442.4749 - val_loss: 1196.0806\n",
      "Epoch 4/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1418.6502 - val_loss: 1188.9372\n",
      "Epoch 5/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1405.1624 - val_loss: 1177.9283\n",
      "Epoch 6/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1394.1913 - val_loss: 1171.9193\n",
      "Epoch 7/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1386.0842 - val_loss: 1169.8189\n",
      "Epoch 8/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1375.6719 - val_loss: 1167.3027\n",
      "Epoch 9/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1362.6023 - val_loss: 1162.5180\n",
      "Epoch 10/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1356.0220 - val_loss: 1158.1158\n",
      "Epoch 11/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1346.3676 - val_loss: 1158.7382\n",
      "Epoch 12/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1337.5261 - val_loss: 1155.6417\n",
      "Epoch 13/30\n",
      "120524/120524 [==============================] - 10s 81us/step - loss: 1328.3404 - val_loss: 1155.2246\n",
      "Epoch 14/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1317.7809 - val_loss: 1155.0485\n",
      "Epoch 15/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1309.6728 - val_loss: 1153.1410\n",
      "Epoch 16/30\n",
      "120524/120524 [==============================] - 10s 81us/step - loss: 1302.0033 - val_loss: 1151.5219\n",
      "Epoch 17/30\n",
      "120524/120524 [==============================] - 10s 81us/step - loss: 1289.5754 - val_loss: 1155.1745\n",
      "Epoch 18/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1283.4072 - val_loss: 1152.8210\n",
      "Epoch 19/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1278.1898 - val_loss: 1152.4890\n",
      "Epoch 20/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1269.7803 - val_loss: 1153.5462\n",
      "Epoch 21/30\n",
      "120524/120524 [==============================] - 9s 78us/step - loss: 1264.0980 - val_loss: 1150.8331\n",
      "Epoch 22/30\n",
      "120524/120524 [==============================] - 9s 79us/step - loss: 1253.2913 - val_loss: 1151.0083\n",
      "Epoch 23/30\n",
      "120524/120524 [==============================] - 9s 79us/step - loss: 1248.6619 - val_loss: 1149.0686\n",
      "Epoch 24/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1239.3750 - val_loss: 1151.2878\n",
      "Epoch 25/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1239.6325 - val_loss: 1149.4843\n",
      "Epoch 26/30\n",
      "120524/120524 [==============================] - 10s 81us/step - loss: 1227.0756 - val_loss: 1150.8638\n",
      "Epoch 27/30\n",
      "120524/120524 [==============================] - 10s 80us/step - loss: 1225.3372 - val_loss: 1150.2304\n",
      "Epoch 28/30\n",
      "120524/120524 [==============================] - 10s 79us/step - loss: 1220.0235 - val_loss: 1150.6759\n",
      "Fold 4, MAE: 1136.2435341393161\n",
      "5-fold CV score: 1146.5376025299975\n",
      "CV score for the final model: 1146.53760253\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_validate_mlp(hyper_model)\n",
    "print (\"CV score for the final model:\", cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, \n",
    " 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta',\n",
    " 'wdecay': 0.0020300000000000001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
