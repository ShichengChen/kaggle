{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = test['id']\n",
    "test['loss'] = np.nan\n",
    "joined = pd.concat([train, test])\n",
    "f_num = [f for f in train.columns if 'cont' in f]\n",
    "f_cat = [f for f in train.columns if 'cat' in f]\n",
    "for column in f_cat:\n",
    "    if (train[column].nunique() != test[column].nunique()\n",
    "        or (train[column].unique() != test[column].unique()).any()):\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        \n",
    "        remove = ((set_train - set_test)|(set_test - set_train))\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "        myfilter = lambda x: np.nan if x in remove else x\n",
    "        joined[column] = joined[column].apply(filter_cat, 1)\n",
    "\n",
    "    #joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n",
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "\n",
    "shift = 200\n",
    "#train_y = np.log(train['loss'] + shift)\n",
    "train_y=train['loss']\n",
    "train_x = train.drop(['loss', 'id'], 1)\n",
    "test = test.drop(['loss', 'id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1065) (188318, 14)\n",
      "(188318, 1079)\n",
      "(125546, 1079)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "contx = scaler.fit_transform(train_x[f_num])\n",
    "catx = pd.get_dummies(data=train_x[f_cat],columns=f_cat,dummy_na=False).values\n",
    "\n",
    "print (catx.shape,contx.shape)\n",
    "\n",
    "train_x = np.concatenate((contx,catx),axis=1)\n",
    "\n",
    "\n",
    "test = pd.get_dummies(data=test,columns=f_cat).values\n",
    "print (train_x.shape)\n",
    "print (test.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def mae_score(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.exp(y_true)-K.exp(y_pred)), axis=-1)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', \n",
    "                             verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "class ExponentialMovingAverage(Callback):\n",
    "    \"\"\"create a copy of trainable weights which gets updated at every\n",
    "       batch using exponential weight decay. The moving average weights along\n",
    "       with the other states of original model(except original model trainable\n",
    "       weights) will be saved at every epoch if save_mv_ave_model is True.\n",
    "       If both save_mv_ave_model and save_best_only are True, the latest\n",
    "       best moving average model according to the quantity monitored\n",
    "       will not be overwritten. Of course, save_best_only can be True\n",
    "       only if there is a validation set.\n",
    "       This is equivalent to save_best_only mode of ModelCheckpoint\n",
    "       callback with similar code. custom_objects is a dictionary\n",
    "       holding name and Class implementation for custom layers.\n",
    "       At end of every batch, the update is as follows:\n",
    "       mv_weight -= (1 - decay) * (mv_weight - weight)\n",
    "       where weight and mv_weight is the ordinal model weight and the moving\n",
    "       averaged weight respectively. At the end of the training, the moving\n",
    "       averaged weights are transferred to the original model.\n",
    "       \"\"\"\n",
    "    def __init__(self, decay=0.999, filepath='model3/model{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                 save_mv_ave_model=False, verbose=0,\n",
    "                 save_best_only=False, monitor='val_loss', mode='auto',\n",
    "                 save_weights_only=False, custom_objects={}):\n",
    "        self.decay = decay\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_mv_ave_model = save_mv_ave_model\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.save_best_only = save_best_only\n",
    "        self.monitor = monitor\n",
    "        self.custom_objects = custom_objects  # dictionary of custom layers\n",
    "        self.sym_trainable_weights = None  # trainable weights of model\n",
    "        self.mv_trainable_weights_vals = None  # moving averaged values\n",
    "        self.epochs = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.sym_trainable_weights = self.model.trainable_weights\n",
    "        # Initialize moving averaged weights using original model values\n",
    "        self.mv_trainable_weights_vals = {x.name: K.get_value(x).copy() for x in\n",
    "                                          self.sym_trainable_weights}\n",
    "        if self.verbose:\n",
    "            print('Created a copy of model weights to initialize moving averaged weights.')\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''old = K.get_value(self.model.trainable_weights[0])\n",
    "        name = self.model.trainable_weights[0].name\n",
    "        assert((old != self.mv_trainable_weights_vals[name]).any())'''\n",
    "        \n",
    "        for weight in self.sym_trainable_weights:\n",
    "            K.set_value(weight, self.mv_trainable_weights_vals[weight.name])\n",
    "\n",
    "        '''old = K.get_value(self.model.trainable_weights[0])\n",
    "        name = self.model.trainable_weights[0].name\n",
    "        assert((old == self.mv_trainable_weights_vals[name]).all())'''\n",
    "            \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if(self.epochs + 5 > self.params['epochs']):\n",
    "            for weight in self.sym_trainable_weights:\n",
    "                old_val = self.mv_trainable_weights_vals[weight.name].copy()\n",
    "                self.mv_trainable_weights_vals[weight.name] = \\\n",
    "                    old_val * self.decay + (1.0 - self.decay) * K.get_value(weight).copy()\n",
    "            #assert((old_val == self.mv_trainable_weights_vals[weight.name]).all())\n",
    "            #assert((old_val == K.get_value(weight)).all())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epochs += 1\n",
    "        if(self.epochs + 5 > self.params['epochs']):\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            self.model.save(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.initializers import he_normal\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import selu\n",
    "from keras.optimizers import Adam\n",
    "def hyper_model(seed = None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(437, input_dim=train_x.shape[1], kernel_initializer=he_normal(seed = seed)\n",
    "                    ,kernel_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.536))\n",
    "    \n",
    "    model.add(Dense(182, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(73, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.233))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    #model.compile(optimizer='adadelta',loss = 'mae',metrics = [mae_score])\n",
    "    model.compile(optimizer='adadelta',loss = 'mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 14s 100us/step - loss: 1925.6660 - val_loss: 1233.0826\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1472.5587 - val_loss: 1212.4064\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1459.4909 - val_loss: 1210.9034\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1448.2593 - val_loss: 1196.8310\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1437.9991 - val_loss: 1185.3718\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1426.9940 - val_loss: 1181.3583\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1418.1283 - val_loss: 1181.7783\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1405.6848 - val_loss: 1179.7277\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1397.5975 - val_loss: 1174.5976\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1391.4682 - val_loss: 1169.4164\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1388.4550 - val_loss: 1175.5008\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1376.2547 - val_loss: 1169.2576\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1373.4598 - val_loss: 1170.8662\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1369.6285 - val_loss: 1166.7997\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1361.2573 - val_loss: 1162.5318\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1351.8344 - val_loss: 1163.1845\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1350.5609 - val_loss: 1159.9826\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1339.3588 - val_loss: 1161.1529\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1337.1922 - val_loss: 1158.6945\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1330.2409 - val_loss: 1162.0982\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1325.6658 - val_loss: 1157.3565\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1320.6505 - val_loss: 1160.4007\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1312.2344 - val_loss: 1157.6795\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1311.2554 - val_loss: 1157.7003\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1305.9127 - val_loss: 1159.3781\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 92us/step - loss: 1299.8483 - val_loss: 1156.3883\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1297.1335 - val_loss: 1155.2751\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1296.8355 - val_loss: 1154.6135\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1290.9031 - val_loss: 1159.8768\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1286.8886 - val_loss: 1154.2971\n",
      "nfold:0,bag:0 1159.89499772\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1914.6036 - val_loss: 1239.9235\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1440.9676 - val_loss: 1198.6292\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1415.3380 - val_loss: 1200.0106\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1409.4200 - val_loss: 1197.4440\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1396.9325 - val_loss: 1185.2494\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1391.7899 - val_loss: 1178.4671\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1380.5155 - val_loss: 1178.7613\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1380.8524 - val_loss: 1174.0269\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1368.0764 - val_loss: 1169.7217\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1365.9724 - val_loss: 1167.3145\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1359.7876 - val_loss: 1170.5808\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1354.7284 - val_loss: 1166.5564\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1343.1734 - val_loss: 1165.3961\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1340.6129 - val_loss: 1167.9837\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1338.8523 - val_loss: 1162.9410\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1333.4401 - val_loss: 1164.8587\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1328.2488 - val_loss: 1162.4843\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1324.2284 - val_loss: 1159.3578\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1316.9345 - val_loss: 1160.8043\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1313.7743 - val_loss: 1162.1155\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1313.3934 - val_loss: 1160.1478\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1306.1682 - val_loss: 1161.8024\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1302.3309 - val_loss: 1156.3332\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1300.4137 - val_loss: 1158.1360\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1295.8052 - val_loss: 1155.0422\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1292.7057 - val_loss: 1156.6093\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1287.8900 - val_loss: 1157.6641\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1286.7411 - val_loss: 1153.2722\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1283.4889 - val_loss: 1153.1432\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1282.0926 - val_loss: 1152.1433\n",
      "nfold:0,bag:1 1157.39816181\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 2137.1126 - val_loss: 1247.5340\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1340.5091 - val_loss: 1190.7900\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1325.6939 - val_loss: 1182.3506\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1318.2792 - val_loss: 1177.3370\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1309.1592 - val_loss: 1172.3529\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1306.5364 - val_loss: 1170.1792\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1299.7261 - val_loss: 1170.3285\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 13s 92us/step - loss: 1297.3329 - val_loss: 1171.7947\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1290.2383 - val_loss: 1165.3378\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 13s 96us/step - loss: 1289.3202 - val_loss: 1162.8053\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1285.5039 - val_loss: 1165.0546\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1285.1570 - val_loss: 1161.5478\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1281.0012 - val_loss: 1160.1952\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1278.9509 - val_loss: 1164.2880\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1278.6519 - val_loss: 1161.2380\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1276.1171 - val_loss: 1158.7853\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1274.5203 - val_loss: 1157.8897\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1268.1803 - val_loss: 1156.9451\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1268.9113 - val_loss: 1157.3402\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1264.7797 - val_loss: 1157.9313\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1261.7797 - val_loss: 1156.0039\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1262.0499 - val_loss: 1154.9609\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1261.2938 - val_loss: 1153.5050\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1258.4414 - val_loss: 1152.8201\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1258.5697 - val_loss: 1153.3222\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1252.7618 - val_loss: 1155.1006\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1250.2970 - val_loss: 1153.7712\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1249.0527 - val_loss: 1151.5341\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1248.0779 - val_loss: 1151.6176\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1249.6214 - val_loss: 1151.0095\n",
      "nfold:0,bag:2 1157.06216559\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 2084.1532 - val_loss: 1248.0513\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1392.4989 - val_loss: 1199.3338\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1368.8497 - val_loss: 1184.0079\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1364.7082 - val_loss: 1178.1637\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1351.7830 - val_loss: 1177.8579\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1345.4685 - val_loss: 1172.7273\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1340.8815 - val_loss: 1171.0588\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1332.5933 - val_loss: 1168.4923\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1327.6703 - val_loss: 1167.5699\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1326.1415 - val_loss: 1167.5354\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1318.9192 - val_loss: 1163.5201\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1320.9568 - val_loss: 1163.9063\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1316.5334 - val_loss: 1163.1820\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1309.5626 - val_loss: 1159.6757\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1311.3060 - val_loss: 1163.2873\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1304.2517 - val_loss: 1160.1981\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1301.6419 - val_loss: 1159.9278\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1295.8066 - val_loss: 1159.3547\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1293.9932 - val_loss: 1159.4781\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1295.6671 - val_loss: 1159.2817\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1290.9182 - val_loss: 1158.5983\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1284.7022 - val_loss: 1158.3703\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 1283.2321 - val_loss: 1158.8191\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1281.5053 - val_loss: 1157.6051\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1278.3426 - val_loss: 1156.9081\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1278.3031 - val_loss: 1156.6956\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1272.0677 - val_loss: 1157.5599\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1268.5358 - val_loss: 1157.7700\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1268.3062 - val_loss: 1153.4399\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1266.1138 - val_loss: 1161.2482\n",
      "nfold:0,bag:3 1161.27394747\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1921.3949 - val_loss: 1213.5548\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1410.7675 - val_loss: 1197.4125\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1390.0374 - val_loss: 1188.0745\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1376.1815 - val_loss: 1184.2248\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1370.0463 - val_loss: 1177.3525\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1363.8236 - val_loss: 1171.8019\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1356.6271 - val_loss: 1175.4310\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1348.9460 - val_loss: 1172.2272\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1347.6106 - val_loss: 1173.8657\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1337.5279 - val_loss: 1166.5479\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1332.5858 - val_loss: 1163.5307\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1328.1250 - val_loss: 1161.7496\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1325.7656 - val_loss: 1160.8921\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1319.1808 - val_loss: 1159.3078\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1314.3123 - val_loss: 1157.8880\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1309.0322 - val_loss: 1158.9871\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1303.0231 - val_loss: 1158.8866\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1303.7238 - val_loss: 1156.1321\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1298.4497 - val_loss: 1156.3868\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 84us/step - loss: 1293.4744 - val_loss: 1154.2033\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1291.3564 - val_loss: 1156.6765\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1286.6473 - val_loss: 1155.3640\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1283.2246 - val_loss: 1154.5849\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1282.0924 - val_loss: 1154.9337\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1277.2260 - val_loss: 1153.3779\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1275.7087 - val_loss: 1152.5905\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1272.2212 - val_loss: 1153.6475\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1270.6769 - val_loss: 1153.7410\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1265.7893 - val_loss: 1150.7933\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1267.7692 - val_loss: 1149.5620\n",
      "nfold:0,bag:4 1156.53290832\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1925.8935 - val_loss: 1239.8181\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1427.5343 - val_loss: 1204.6749\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1407.3583 - val_loss: 1196.0390\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1392.2352 - val_loss: 1190.8078\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1385.1772 - val_loss: 1183.1624\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1379.0768 - val_loss: 1182.3714\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1370.2197 - val_loss: 1178.7061\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1362.9510 - val_loss: 1177.0441\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1353.1990 - val_loss: 1171.9409\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1355.3050 - val_loss: 1172.1954\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1346.4672 - val_loss: 1172.7271\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1341.4953 - val_loss: 1168.4052\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1330.6077 - val_loss: 1171.7414\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1325.4258 - val_loss: 1165.9101\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1326.0549 - val_loss: 1164.7488\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1319.6138 - val_loss: 1167.5490\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1315.5034 - val_loss: 1164.9935\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1310.4455 - val_loss: 1162.2558\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1308.2308 - val_loss: 1163.1977\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1305.4048 - val_loss: 1160.8747\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1296.1344 - val_loss: 1159.1634\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1290.2645 - val_loss: 1160.1301\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1290.3919 - val_loss: 1163.1844\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1287.3854 - val_loss: 1157.9784\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1282.0414 - val_loss: 1158.9953\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1280.0271 - val_loss: 1159.0195\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1276.4988 - val_loss: 1158.3775\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1268.0762 - val_loss: 1155.4615\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1267.7054 - val_loss: 1153.8491\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1261.4214 - val_loss: 1153.7655\n",
      "nfold:1,bag:0 1177.22664112\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1923.4504 - val_loss: 1241.3250\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1426.5085 - val_loss: 1214.5071\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1407.9688 - val_loss: 1202.5168\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1399.3469 - val_loss: 1194.3985\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1390.0929 - val_loss: 1188.5620\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1384.1900 - val_loss: 1188.8355\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1377.6260 - val_loss: 1178.0914\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1365.8969 - val_loss: 1176.9172\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1360.5668 - val_loss: 1175.2852\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1352.6602 - val_loss: 1174.1536\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1342.5643 - val_loss: 1168.3443\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1345.8002 - val_loss: 1166.7001\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1330.7468 - val_loss: 1166.2334\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1330.3236 - val_loss: 1165.1196\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1326.4467 - val_loss: 1163.5760\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1320.7631 - val_loss: 1165.4356\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1315.1793 - val_loss: 1163.7348\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1313.0362 - val_loss: 1163.3403\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1311.6827 - val_loss: 1162.7783\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1306.6262 - val_loss: 1160.6090\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1300.0513 - val_loss: 1161.8867\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1299.0479 - val_loss: 1162.2027\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1295.4419 - val_loss: 1158.1439\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1289.5135 - val_loss: 1161.0860\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1291.7780 - val_loss: 1157.2510\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1281.1025 - val_loss: 1160.0449\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1280.4381 - val_loss: 1158.4078\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1272.7346 - val_loss: 1159.0783\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1275.1236 - val_loss: 1158.0374\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 21s 154us/step - loss: 1272.4808 - val_loss: 1154.9816\n",
      "nfold:1,bag:1 1177.51965016\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 2022.6294 - val_loss: 1248.2605\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1430.1546 - val_loss: 1208.0880\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1401.8442 - val_loss: 1201.0941\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1387.5786 - val_loss: 1191.3741\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1383.0411 - val_loss: 1193.1999\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1369.9199 - val_loss: 1181.8132\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1362.9070 - val_loss: 1176.3032\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1357.1503 - val_loss: 1175.0084\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1349.8449 - val_loss: 1174.8791\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1344.3069 - val_loss: 1173.0127\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1338.5792 - val_loss: 1169.2376\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1334.0217 - val_loss: 1167.6691\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1329.3071 - val_loss: 1164.3506\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1325.9725 - val_loss: 1166.1629\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1315.8215 - val_loss: 1172.4837\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1317.7245 - val_loss: 1165.7734\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1315.8557 - val_loss: 1163.3741\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1305.7172 - val_loss: 1162.3219\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1302.7356 - val_loss: 1163.9960\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1297.8789 - val_loss: 1160.0744\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1291.8500 - val_loss: 1160.5236\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1290.1385 - val_loss: 1159.2871\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1285.9512 - val_loss: 1159.1452\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1278.8327 - val_loss: 1159.8360\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1278.9164 - val_loss: 1157.7713\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1273.9283 - val_loss: 1157.1519\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1268.0569 - val_loss: 1159.1511\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1264.1932 - val_loss: 1156.7460\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1263.0405 - val_loss: 1153.5964\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1263.0286 - val_loss: 1156.5771\n",
      "nfold:1,bag:2 1183.99209582\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2059.8312 - val_loss: 1236.0349\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1363.4052 - val_loss: 1197.4976\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1340.7169 - val_loss: 1189.0918\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1330.3112 - val_loss: 1184.8344\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1323.3739 - val_loss: 1177.3673\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1314.4923 - val_loss: 1174.1321\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1315.5150 - val_loss: 1178.1844\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1310.0404 - val_loss: 1170.8558\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1306.2169 - val_loss: 1168.8745\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1304.6939 - val_loss: 1169.0275\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1299.2725 - val_loss: 1167.8856\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1294.3764 - val_loss: 1169.5720\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1285.1152 - val_loss: 1167.3016\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1286.9433 - val_loss: 1166.0058\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1284.0213 - val_loss: 1163.5643\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1281.2209 - val_loss: 1162.5039\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1279.9529 - val_loss: 1163.6418\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1277.0260 - val_loss: 1162.7558\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1274.3180 - val_loss: 1159.8287\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1268.3519 - val_loss: 1160.0200\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1269.7988 - val_loss: 1159.1265\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1263.9511 - val_loss: 1159.9703\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1263.7864 - val_loss: 1159.3225\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1259.7108 - val_loss: 1156.3532\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1260.0263 - val_loss: 1156.8455\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1255.8105 - val_loss: 1155.6304\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1253.3259 - val_loss: 1158.1297\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1254.3866 - val_loss: 1156.4060\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1251.0599 - val_loss: 1156.6145\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1246.0516 - val_loss: 1155.1023\n",
      "nfold:1,bag:3 1174.5250323\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 2018.7497 - val_loss: 1291.8408\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1507.3383 - val_loss: 1229.9850\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1456.9137 - val_loss: 1224.0959\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1448.0959 - val_loss: 1207.7736\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1428.0048 - val_loss: 1195.8736\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1417.9086 - val_loss: 1195.0085\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1407.6477 - val_loss: 1192.6882\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1396.6736 - val_loss: 1179.1295\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 81us/step - loss: 1391.3166 - val_loss: 1179.7400\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1383.3361 - val_loss: 1180.7782\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1374.1898 - val_loss: 1173.6962\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1369.6746 - val_loss: 1175.0710\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1365.5039 - val_loss: 1174.9235\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1357.8096 - val_loss: 1170.6497\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1349.5745 - val_loss: 1168.3464\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1347.9950 - val_loss: 1168.9486\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1343.0645 - val_loss: 1167.7369\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1334.2131 - val_loss: 1166.8345\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1327.6013 - val_loss: 1164.0027\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1325.2212 - val_loss: 1163.0537\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1318.2605 - val_loss: 1164.3053\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1313.1588 - val_loss: 1160.1906\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1308.8116 - val_loss: 1163.4290\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1304.0677 - val_loss: 1158.1142\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1301.0126 - val_loss: 1159.1689\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1297.1839 - val_loss: 1157.3637\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1295.5668 - val_loss: 1160.3223\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1292.3991 - val_loss: 1158.1497\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1284.9018 - val_loss: 1158.7370\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1278.7807 - val_loss: 1156.2528\n",
      "nfold:1,bag:4 1182.04761224\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1852.0841 - val_loss: 1222.6126\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1390.2721 - val_loss: 1190.7312\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1377.6167 - val_loss: 1188.3065\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1370.5255 - val_loss: 1184.6590\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1360.3825 - val_loss: 1178.2941\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1355.4805 - val_loss: 1178.5667\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1349.0261 - val_loss: 1176.1142\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1350.6476 - val_loss: 1174.4580\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1343.9400 - val_loss: 1171.1378\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1339.7778 - val_loss: 1170.9122\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1332.7568 - val_loss: 1168.2588\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1326.8654 - val_loss: 1166.9644\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1324.8098 - val_loss: 1165.2944\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1322.6749 - val_loss: 1165.7732\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1315.0654 - val_loss: 1161.9321\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1311.2922 - val_loss: 1165.4215\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1309.8755 - val_loss: 1161.3778\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1303.5945 - val_loss: 1162.9678\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1303.7320 - val_loss: 1161.8768\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1297.9601 - val_loss: 1158.9703\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1294.6082 - val_loss: 1160.4245\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1288.5609 - val_loss: 1159.7781\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1289.2894 - val_loss: 1158.8407\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1289.3984 - val_loss: 1158.1915\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1278.5626 - val_loss: 1156.3392\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1280.4598 - val_loss: 1159.4338\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1277.9864 - val_loss: 1158.7687\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1273.9852 - val_loss: 1155.1424\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1270.5650 - val_loss: 1155.7703\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1266.9081 - val_loss: 1155.4065\n",
      "nfold:2,bag:0 1160.13582395\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1929.1522 - val_loss: 1246.3942\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1484.9327 - val_loss: 1223.9414\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1456.9641 - val_loss: 1216.9783\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1442.9558 - val_loss: 1207.7750\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1433.1408 - val_loss: 1199.7182\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1418.9814 - val_loss: 1200.5455\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1411.6126 - val_loss: 1190.7119\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1404.2405 - val_loss: 1180.9819\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1394.6410 - val_loss: 1181.3751\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1386.8912 - val_loss: 1174.8028\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1382.6811 - val_loss: 1173.7423\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1373.8005 - val_loss: 1172.1800\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1365.4016 - val_loss: 1169.3426\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1356.7702 - val_loss: 1172.7740\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1353.4707 - val_loss: 1168.7270\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1349.0996 - val_loss: 1167.5045\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1344.9822 - val_loss: 1167.1035\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1340.9689 - val_loss: 1164.6291\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 82us/step - loss: 1334.4726 - val_loss: 1163.9931\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1330.4385 - val_loss: 1161.3130\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1319.5986 - val_loss: 1160.9661\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1315.2884 - val_loss: 1160.8184\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1313.5260 - val_loss: 1161.8224\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1308.8379 - val_loss: 1161.7290\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1299.3193 - val_loss: 1161.1116\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1302.1511 - val_loss: 1158.0471\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1298.1136 - val_loss: 1165.3058\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1291.5116 - val_loss: 1163.0692\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1287.5802 - val_loss: 1159.7413\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1287.9892 - val_loss: 1158.0628\n",
      "nfold:2,bag:1 1161.95383042\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1950.6351 - val_loss: 1217.7480\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1355.0142 - val_loss: 1198.0718\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1335.9517 - val_loss: 1185.6233\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1332.2798 - val_loss: 1181.3317\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1322.1982 - val_loss: 1177.4648\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1317.9830 - val_loss: 1175.9961\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1308.9052 - val_loss: 1175.8079\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1311.7058 - val_loss: 1178.4548\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1308.8695 - val_loss: 1169.2520\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1305.2392 - val_loss: 1167.3925\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1296.5592 - val_loss: 1164.7552\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1296.8672 - val_loss: 1165.0628\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1293.0952 - val_loss: 1164.3030\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1290.6520 - val_loss: 1162.7950\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1285.4901 - val_loss: 1161.7700\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1281.4983 - val_loss: 1166.8880\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1283.6422 - val_loss: 1160.9298\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1273.5595 - val_loss: 1161.9332\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1273.5791 - val_loss: 1159.9009\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1272.7353 - val_loss: 1160.1283\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1268.4861 - val_loss: 1156.0669\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1269.6170 - val_loss: 1157.6575\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1266.9811 - val_loss: 1156.4183\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1262.0416 - val_loss: 1155.7649\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1261.3577 - val_loss: 1154.9089\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1259.7592 - val_loss: 1158.4378\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1256.0346 - val_loss: 1156.7211\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1257.2773 - val_loss: 1154.5476\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1252.0215 - val_loss: 1157.5991\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1251.8166 - val_loss: 1158.6952\n",
      "nfold:2,bag:2 1157.62204347\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 2058.5084 - val_loss: 1293.8725\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1418.0300 - val_loss: 1205.7550\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1388.6028 - val_loss: 1200.7150\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1374.0945 - val_loss: 1193.6616\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1366.7683 - val_loss: 1190.7008\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1358.9772 - val_loss: 1179.9777\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1351.8049 - val_loss: 1175.6827\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1344.8292 - val_loss: 1175.6146\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1339.6327 - val_loss: 1173.5144\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1336.8536 - val_loss: 1172.5451\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1333.8127 - val_loss: 1168.6488\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1326.1734 - val_loss: 1167.9831\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1318.0697 - val_loss: 1167.8222\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1316.8243 - val_loss: 1165.6543\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1314.6815 - val_loss: 1165.1406\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1306.7686 - val_loss: 1162.9386\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1303.0939 - val_loss: 1160.1060\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1305.4877 - val_loss: 1164.3377\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1298.4883 - val_loss: 1160.5369\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1296.4788 - val_loss: 1157.4021\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1290.8865 - val_loss: 1162.8189\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1284.6970 - val_loss: 1160.0365\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1281.5928 - val_loss: 1159.7234\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1279.9242 - val_loss: 1159.5201\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1276.5620 - val_loss: 1156.9251\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1272.3347 - val_loss: 1157.0611\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1269.8311 - val_loss: 1153.1842\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1264.4386 - val_loss: 1157.9526\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 22s 164us/step - loss: 1266.4160 - val_loss: 1155.5409\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1263.5507 - val_loss: 1154.5785\n",
      "nfold:2,bag:3 1155.03016245\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 2203.6316 - val_loss: 1318.7148\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1401.9900 - val_loss: 1215.7792\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1377.9560 - val_loss: 1195.9750\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1364.0579 - val_loss: 1182.2292\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1357.5506 - val_loss: 1184.4596\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1347.9222 - val_loss: 1177.9713\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1340.0439 - val_loss: 1178.1051\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1334.8989 - val_loss: 1170.8789\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1328.1598 - val_loss: 1170.1502\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1331.1076 - val_loss: 1167.9976\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1323.3988 - val_loss: 1169.3215\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1317.8734 - val_loss: 1167.8643\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1312.6032 - val_loss: 1170.0931\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1307.4217 - val_loss: 1163.5990\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1310.3650 - val_loss: 1161.7840\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1306.1655 - val_loss: 1162.5128\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1303.4169 - val_loss: 1163.6116\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1298.0485 - val_loss: 1162.7719\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1291.1667 - val_loss: 1160.3147\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1286.1434 - val_loss: 1162.3557\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1285.9031 - val_loss: 1160.2276\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1284.5623 - val_loss: 1158.3820\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1279.6110 - val_loss: 1157.9781\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1277.0373 - val_loss: 1161.3976\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1272.8575 - val_loss: 1160.1253\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1272.0857 - val_loss: 1157.3765\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1270.2007 - val_loss: 1155.9173\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1266.0482 - val_loss: 1156.5017\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1266.3818 - val_loss: 1156.0397\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1261.2902 - val_loss: 1158.9802\n",
      "nfold:2,bag:4 1159.28357117\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1992.6599 - val_loss: 1213.2190\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1405.4107 - val_loss: 1190.6288\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1380.8695 - val_loss: 1188.8334\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1374.1737 - val_loss: 1179.8031\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1361.4947 - val_loss: 1178.2849\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1358.7859 - val_loss: 1174.3392\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1352.6316 - val_loss: 1170.4325\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1347.4919 - val_loss: 1168.5189\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1338.7539 - val_loss: 1168.0642\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1333.6425 - val_loss: 1165.0564\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1333.6249 - val_loss: 1161.5408\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1330.9461 - val_loss: 1162.7670\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1318.9844 - val_loss: 1162.2319\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1323.5227 - val_loss: 1162.0778\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1313.3607 - val_loss: 1160.1756\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1308.4511 - val_loss: 1160.4752\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1306.4090 - val_loss: 1160.3017\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1304.5725 - val_loss: 1158.3617\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1300.9254 - val_loss: 1156.6592\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1299.5396 - val_loss: 1158.6861\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1296.7059 - val_loss: 1157.2383\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1289.2345 - val_loss: 1154.6733\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1285.0292 - val_loss: 1155.2346\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1284.5385 - val_loss: 1155.7419\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1280.6021 - val_loss: 1154.2403\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1281.3935 - val_loss: 1151.0242\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1278.5386 - val_loss: 1151.8306\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1274.6038 - val_loss: 1153.2278\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1268.8453 - val_loss: 1151.3346\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1267.3955 - val_loss: 1149.8463\n",
      "nfold:3,bag:0 1171.62960437\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 2072.0097 - val_loss: 1234.2491\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1389.2671 - val_loss: 1194.4412\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1367.4809 - val_loss: 1187.4743\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1357.2350 - val_loss: 1176.9209\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1348.2979 - val_loss: 1174.1369\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1343.0482 - val_loss: 1171.7669\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1339.2715 - val_loss: 1167.6070\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 83us/step - loss: 1332.8436 - val_loss: 1173.9189\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1328.0425 - val_loss: 1166.2699\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1325.4547 - val_loss: 1163.6882\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1314.9094 - val_loss: 1161.2544\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1318.8780 - val_loss: 1163.0699\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1311.4237 - val_loss: 1158.4997\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1307.2518 - val_loss: 1160.4741\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1300.9325 - val_loss: 1158.1758\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1298.3298 - val_loss: 1156.3442\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1297.4570 - val_loss: 1154.9173\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1295.3661 - val_loss: 1155.2941\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1289.2757 - val_loss: 1153.1314\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1286.5881 - val_loss: 1156.0249\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1283.6798 - val_loss: 1151.7931\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1279.6856 - val_loss: 1153.8726\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1282.2322 - val_loss: 1152.2238\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1276.4914 - val_loss: 1152.1467\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1271.7430 - val_loss: 1150.0880\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1271.7284 - val_loss: 1150.9969\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1268.9584 - val_loss: 1149.2558\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1266.4663 - val_loss: 1151.2081\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1263.2072 - val_loss: 1147.7294\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1262.3434 - val_loss: 1148.8226\n",
      "nfold:3,bag:1 1170.02585239\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 2019.8291 - val_loss: 1299.3029\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1506.1997 - val_loss: 1218.4057\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1457.6755 - val_loss: 1205.6493\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1435.9761 - val_loss: 1191.5461\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1420.2687 - val_loss: 1182.3878\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1409.3357 - val_loss: 1179.7131\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1400.5813 - val_loss: 1179.7329\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1390.4396 - val_loss: 1169.3532\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1385.9226 - val_loss: 1167.2184\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1378.1415 - val_loss: 1168.5615\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1369.1198 - val_loss: 1164.7282\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1359.5861 - val_loss: 1167.7964\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1352.6072 - val_loss: 1167.8437\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1344.1130 - val_loss: 1165.0220\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1341.3228 - val_loss: 1158.9896\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1338.8230 - val_loss: 1159.6859\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1330.8281 - val_loss: 1158.2408\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1324.3731 - val_loss: 1157.6849\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1320.2253 - val_loss: 1154.1601\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1312.3926 - val_loss: 1153.5663\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1309.4943 - val_loss: 1153.6422\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1305.2557 - val_loss: 1151.7945\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1301.0785 - val_loss: 1150.8779\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1293.3461 - val_loss: 1154.1338\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1288.5883 - val_loss: 1150.3366\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1285.1214 - val_loss: 1152.3540\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1282.7416 - val_loss: 1150.1797\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1279.9644 - val_loss: 1150.3354\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1275.3929 - val_loss: 1154.4332\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1271.9943 - val_loss: 1148.8773\n",
      "nfold:3,bag:2 1168.41032354\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 2057.4421 - val_loss: 1297.4937\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1563.0292 - val_loss: 1252.8275\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1503.2539 - val_loss: 1231.1130\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1483.8143 - val_loss: 1219.8730\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1460.1665 - val_loss: 1198.4714\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1449.0795 - val_loss: 1192.5221\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1434.3897 - val_loss: 1186.1210\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1423.7214 - val_loss: 1181.6465\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1414.6048 - val_loss: 1177.0588\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1408.0576 - val_loss: 1172.3008\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1394.3642 - val_loss: 1173.3816\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1386.1753 - val_loss: 1167.7010\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1377.5013 - val_loss: 1161.5212\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1368.8632 - val_loss: 1163.1126\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1363.0474 - val_loss: 1164.9136\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1353.1044 - val_loss: 1165.1978\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1342.0449 - val_loss: 1159.4050\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 83us/step - loss: 1339.1281 - val_loss: 1158.6885\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1332.4877 - val_loss: 1157.7200\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1326.7470 - val_loss: 1158.8130\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1321.9041 - val_loss: 1156.9072\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1312.9987 - val_loss: 1154.2983\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1312.3105 - val_loss: 1155.0082\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1303.8594 - val_loss: 1151.2969\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1298.1981 - val_loss: 1153.0240\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1290.0371 - val_loss: 1150.5975\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1288.5467 - val_loss: 1151.3206\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1281.5556 - val_loss: 1152.2750\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1284.7241 - val_loss: 1150.5686\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1274.7453 - val_loss: 1149.5104\n",
      "nfold:3,bag:3 1173.05676251\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 2139.3134 - val_loss: 1230.9025\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1354.8912 - val_loss: 1193.8617\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1335.4938 - val_loss: 1182.5492\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1328.9301 - val_loss: 1176.4244\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1319.9632 - val_loss: 1173.0360\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1315.2146 - val_loss: 1167.6542\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1310.6220 - val_loss: 1166.6967\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1305.0956 - val_loss: 1165.0898\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1299.6865 - val_loss: 1164.7904\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1296.2039 - val_loss: 1160.1657\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1292.3524 - val_loss: 1158.0203\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1291.2367 - val_loss: 1156.4737\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1284.8651 - val_loss: 1161.0284\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1281.7999 - val_loss: 1156.1085\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1279.2248 - val_loss: 1156.4405\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1276.1145 - val_loss: 1155.5184\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1274.1769 - val_loss: 1155.5018\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1270.0768 - val_loss: 1153.3642\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1268.5347 - val_loss: 1154.9973\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1264.2420 - val_loss: 1151.8301\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1264.8943 - val_loss: 1150.4314\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1262.5706 - val_loss: 1153.6126\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1259.8067 - val_loss: 1149.7602\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1260.1966 - val_loss: 1149.6955\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1255.7929 - val_loss: 1149.9927\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 1254.3508 - val_loss: 1154.0593\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1252.5776 - val_loss: 1152.2642\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1248.5327 - val_loss: 1147.0269\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1254.6225 - val_loss: 1148.0257\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1245.2863 - val_loss: 1147.8018\n",
      "nfold:3,bag:4 1166.89137029\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 2036.4235 - val_loss: 1313.0799\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1588.5544 - val_loss: 1276.7095\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1546.8298 - val_loss: 1259.9452\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1528.3797 - val_loss: 1236.2866\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1511.9574 - val_loss: 1219.0036\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1497.5573 - val_loss: 1199.8949\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1478.0948 - val_loss: 1201.1121\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1469.7682 - val_loss: 1188.6267\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1455.1964 - val_loss: 1190.2908\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1444.5296 - val_loss: 1179.3674\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1435.8445 - val_loss: 1177.0440\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1426.5515 - val_loss: 1174.6949\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1411.2662 - val_loss: 1170.5820\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1408.7718 - val_loss: 1169.7354\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1392.0120 - val_loss: 1182.9137\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1386.2968 - val_loss: 1166.4913\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1378.5898 - val_loss: 1167.2851\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1368.0027 - val_loss: 1165.4145\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1360.2484 - val_loss: 1162.6409\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1348.0253 - val_loss: 1163.8473\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1346.3434 - val_loss: 1162.3807\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1337.3509 - val_loss: 1161.0784\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1330.2762 - val_loss: 1156.7457\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1325.2807 - val_loss: 1158.9480\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1319.6712 - val_loss: 1159.3159\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 1307.2397 - val_loss: 1156.6206\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1302.0680 - val_loss: 1157.8473\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 25s 182us/step - loss: 1301.9206 - val_loss: 1153.0977\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1290.0705 - val_loss: 1154.3892\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1288.6252 - val_loss: 1154.1797\n",
      "nfold:4,bag:0 1164.42808246\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 2045.1130 - val_loss: 1264.4325\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1413.7464 - val_loss: 1201.9177\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1390.5541 - val_loss: 1193.0179\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1382.3017 - val_loss: 1184.7780\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1374.5505 - val_loss: 1177.8309\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1367.2556 - val_loss: 1172.4609\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1361.3520 - val_loss: 1175.2871\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1351.7167 - val_loss: 1165.7894\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1346.5941 - val_loss: 1166.2249\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1343.3712 - val_loss: 1164.6255\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1334.6426 - val_loss: 1166.9033\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1333.4161 - val_loss: 1162.9517\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1324.0265 - val_loss: 1160.0629\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1323.3441 - val_loss: 1161.2712\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1320.8948 - val_loss: 1159.3862\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1312.1512 - val_loss: 1156.5697\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1313.9570 - val_loss: 1159.0528\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1308.4089 - val_loss: 1157.6658\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1305.3213 - val_loss: 1156.4498\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1297.8971 - val_loss: 1157.6013\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1294.6932 - val_loss: 1156.2442\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1293.1524 - val_loss: 1153.7121\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1288.3177 - val_loss: 1158.5669\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1287.1421 - val_loss: 1157.3807\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1285.4450 - val_loss: 1155.6888\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1285.4748 - val_loss: 1152.7235\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1278.4297 - val_loss: 1152.6196\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1278.5159 - val_loss: 1151.0304\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1275.8485 - val_loss: 1153.3399\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1271.2597 - val_loss: 1153.5265\n",
      "nfold:4,bag:1 1165.26198109\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 2074.7114 - val_loss: 1320.7749\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1535.0971 - val_loss: 1237.5836\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1496.5951 - val_loss: 1231.5143\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1484.0078 - val_loss: 1220.4225\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1465.8958 - val_loss: 1204.3249\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1457.4043 - val_loss: 1190.1657\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1439.1252 - val_loss: 1183.9262\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1430.7836 - val_loss: 1175.2104\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1421.3659 - val_loss: 1175.5715\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1409.3690 - val_loss: 1168.8224\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1402.9899 - val_loss: 1166.1291\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1391.6535 - val_loss: 1166.6553\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1383.7138 - val_loss: 1164.4174\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1374.0746 - val_loss: 1162.3981\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1371.2767 - val_loss: 1161.1420\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1364.1781 - val_loss: 1162.0138\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1358.5617 - val_loss: 1163.6391\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1345.8449 - val_loss: 1160.2492\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1341.0353 - val_loss: 1162.3748\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1339.5665 - val_loss: 1159.7623\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1330.3970 - val_loss: 1157.3359\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1324.3766 - val_loss: 1157.1837\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1324.2965 - val_loss: 1156.8491\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1317.1999 - val_loss: 1154.5212\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1315.9686 - val_loss: 1155.3912\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 103us/step - loss: 1308.2396 - val_loss: 1155.8184\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1305.6298 - val_loss: 1154.4564\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1295.8206 - val_loss: 1154.0447\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1297.2749 - val_loss: 1152.3854\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1288.8921 - val_loss: 1151.5415\n",
      "nfold:4,bag:2 1165.72410689\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 2067.8619 - val_loss: 1317.8990\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1548.5019 - val_loss: 1252.2735\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1501.3808 - val_loss: 1225.0606\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1485.1495 - val_loss: 1219.9787\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1465.9217 - val_loss: 1206.4887\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1449.1226 - val_loss: 1192.4017\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 84us/step - loss: 1439.9517 - val_loss: 1181.4681\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1426.0935 - val_loss: 1185.6069\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1413.5997 - val_loss: 1173.4137\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1406.4954 - val_loss: 1167.1067\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1397.2800 - val_loss: 1171.7824\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1389.2607 - val_loss: 1166.8660\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1377.4120 - val_loss: 1167.1331\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1369.1319 - val_loss: 1165.4026\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1365.7873 - val_loss: 1159.9332\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1357.4167 - val_loss: 1164.3440\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1352.8761 - val_loss: 1157.9376\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1341.4895 - val_loss: 1158.6907\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1334.8813 - val_loss: 1160.8444\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1330.2507 - val_loss: 1159.6543\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1324.3845 - val_loss: 1157.3129\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1319.8344 - val_loss: 1156.8068\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1309.4806 - val_loss: 1156.2712\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1310.6108 - val_loss: 1154.9556\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1302.6927 - val_loss: 1156.9350\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 107us/step - loss: 1296.1416 - val_loss: 1151.9286\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1293.4625 - val_loss: 1157.4911\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1291.2885 - val_loss: 1150.3813\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1283.1333 - val_loss: 1153.0706\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1278.9245 - val_loss: 1153.5509\n",
      "nfold:4,bag:3 1166.98533869\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 2044.9596 - val_loss: 1276.5232\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1489.6465 - val_loss: 1227.0079\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1459.6679 - val_loss: 1214.4679\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1439.1344 - val_loss: 1198.1062\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1421.6166 - val_loss: 1186.4652\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1412.2070 - val_loss: 1183.3559\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1399.7926 - val_loss: 1177.9070\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1386.5376 - val_loss: 1171.5468\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1387.1021 - val_loss: 1173.0403\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1377.1057 - val_loss: 1168.6752\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1372.0396 - val_loss: 1169.1085\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1360.8756 - val_loss: 1166.9685\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1357.6924 - val_loss: 1161.9140\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1349.7799 - val_loss: 1166.8566\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1342.4302 - val_loss: 1163.7921\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1341.9093 - val_loss: 1161.4977\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1330.3712 - val_loss: 1160.9296\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1326.1019 - val_loss: 1161.9835\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1329.9700 - val_loss: 1158.1409\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1319.9669 - val_loss: 1158.1855\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1316.0053 - val_loss: 1157.6953\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1306.0233 - val_loss: 1161.4007\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1305.1210 - val_loss: 1160.0096\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1303.1136 - val_loss: 1155.1667\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1292.6425 - val_loss: 1155.3453\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 102us/step - loss: 1292.8706 - val_loss: 1153.5633\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1293.0667 - val_loss: 1152.9728\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1285.4129 - val_loss: 1153.1936\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1278.6372 - val_loss: 1151.2769\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1276.9667 - val_loss: 1152.2197\n",
      "nfold:4,bag:4 1162.745276\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 2300.1034 - val_loss: 1308.7791\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1359.7974 - val_loss: 1200.8462\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1336.0300 - val_loss: 1190.4915\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1325.2663 - val_loss: 1188.1544\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1316.9310 - val_loss: 1185.0853\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1309.0487 - val_loss: 1182.8117\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1308.5010 - val_loss: 1177.3573\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1302.4180 - val_loss: 1176.5222\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1297.6299 - val_loss: 1175.0365\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1293.7347 - val_loss: 1172.4119\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1286.4918 - val_loss: 1170.5913\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1284.6518 - val_loss: 1167.6407\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1278.2150 - val_loss: 1168.6689\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1276.6067 - val_loss: 1167.6799\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1272.9733 - val_loss: 1166.2519\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1269.4135 - val_loss: 1166.8593\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 84us/step - loss: 1269.5994 - val_loss: 1163.0294\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1267.2839 - val_loss: 1165.1023\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1262.1074 - val_loss: 1163.6350\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1260.8580 - val_loss: 1162.5348\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1259.2214 - val_loss: 1161.4808\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1257.0539 - val_loss: 1161.8008\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1254.5952 - val_loss: 1159.6667\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1250.3313 - val_loss: 1158.6567\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1253.5984 - val_loss: 1159.2054\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 104us/step - loss: 1247.4518 - val_loss: 1159.9018\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1245.5428 - val_loss: 1159.9540\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1245.1137 - val_loss: 1156.0700\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1242.6168 - val_loss: 1157.4817\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1240.2487 - val_loss: 1158.0051\n",
      "nfold:5,bag:0 1159.07199416\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2044.9309 - val_loss: 1272.9106\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1453.1957 - val_loss: 1214.7422\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1418.2789 - val_loss: 1205.7607\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1402.9234 - val_loss: 1197.6728\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1389.7052 - val_loss: 1186.2432\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1382.6777 - val_loss: 1182.1062\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1374.5072 - val_loss: 1175.0736\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1369.4613 - val_loss: 1173.6039\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1359.9864 - val_loss: 1172.7199\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1356.6514 - val_loss: 1176.0665\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1354.3931 - val_loss: 1173.1082\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1347.3716 - val_loss: 1166.1220\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1338.3561 - val_loss: 1166.2025\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1335.0946 - val_loss: 1166.8682\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1325.7000 - val_loss: 1166.5337\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1323.9430 - val_loss: 1164.4448\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1315.8131 - val_loss: 1161.8786\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1317.3272 - val_loss: 1163.5442\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1309.2096 - val_loss: 1162.4079\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1306.2943 - val_loss: 1161.6662\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1303.1452 - val_loss: 1158.7938\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1296.6713 - val_loss: 1158.8272\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1294.4840 - val_loss: 1159.5802\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1291.7749 - val_loss: 1157.7329\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1285.4216 - val_loss: 1157.9728\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 102us/step - loss: 1283.0431 - val_loss: 1157.2137\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1280.7572 - val_loss: 1158.1032\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1274.4154 - val_loss: 1156.2905\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1274.0116 - val_loss: 1157.6514\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1271.1178 - val_loss: 1156.7086\n",
      "nfold:5,bag:1 1170.56917142\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 2089.8957 - val_loss: 1257.4197\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1396.0891 - val_loss: 1201.3702\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1370.4081 - val_loss: 1196.4136\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1361.5361 - val_loss: 1184.8345\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1352.9103 - val_loss: 1183.5499\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1347.2807 - val_loss: 1186.3922\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1345.5838 - val_loss: 1177.2355\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1336.0918 - val_loss: 1174.1466\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1333.6750 - val_loss: 1173.2383\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1329.9683 - val_loss: 1170.8534\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1324.0597 - val_loss: 1174.9791\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1325.0140 - val_loss: 1168.0747\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1318.5703 - val_loss: 1167.6262\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1312.7057 - val_loss: 1165.8917\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1307.1473 - val_loss: 1164.0649\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1305.8418 - val_loss: 1162.5179\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1306.4125 - val_loss: 1161.8698\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1301.8277 - val_loss: 1162.6661\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1298.9037 - val_loss: 1160.5230\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1291.9959 - val_loss: 1160.0554\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1292.3296 - val_loss: 1158.7296\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1287.9348 - val_loss: 1163.9917\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1287.6107 - val_loss: 1159.3038\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1285.1194 - val_loss: 1157.8453\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1281.1569 - val_loss: 1157.4603\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 106us/step - loss: 1273.9831 - val_loss: 1156.6860\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 26s 190us/step - loss: 1275.1088 - val_loss: 1155.9379\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1271.5176 - val_loss: 1156.1459\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1272.7786 - val_loss: 1155.4902\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1265.4771 - val_loss: 1155.0411\n",
      "nfold:5,bag:2 1161.13758416\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1957.4974 - val_loss: 1236.9955\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1432.6195 - val_loss: 1201.4884\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1404.2464 - val_loss: 1195.9649\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1393.5925 - val_loss: 1182.6016\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1385.5021 - val_loss: 1183.2951\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1377.1751 - val_loss: 1182.8354\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1369.0686 - val_loss: 1177.7212\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1360.1122 - val_loss: 1173.2696\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1355.7740 - val_loss: 1171.0306\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1345.0749 - val_loss: 1171.2208\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1341.0636 - val_loss: 1166.3471\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1333.0375 - val_loss: 1167.1268\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1333.7453 - val_loss: 1162.9888\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1321.1857 - val_loss: 1162.9347\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1315.5422 - val_loss: 1160.0784\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1315.8795 - val_loss: 1162.6253\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1307.7844 - val_loss: 1160.4251\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1301.2100 - val_loss: 1160.8452\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1300.5053 - val_loss: 1158.3586\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1293.3119 - val_loss: 1159.3789\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1291.6309 - val_loss: 1156.2344\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1288.7145 - val_loss: 1156.3685\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1282.7961 - val_loss: 1157.7983\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1277.3210 - val_loss: 1154.3035\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1274.2709 - val_loss: 1156.7451\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 114us/step - loss: 1270.5295 - val_loss: 1158.8175\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1266.7670 - val_loss: 1156.9271\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1264.9201 - val_loss: 1159.8204\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1262.7913 - val_loss: 1153.6816\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1258.1422 - val_loss: 1156.5514\n",
      "nfold:5,bag:3 1160.94860715\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 2199.1067 - val_loss: 1295.5028\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1338.9775 - val_loss: 1194.6355\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1314.6806 - val_loss: 1186.4995\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1308.4648 - val_loss: 1179.4025\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1300.8167 - val_loss: 1176.3941\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1291.9410 - val_loss: 1177.2727\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1289.5670 - val_loss: 1172.3500\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1282.8424 - val_loss: 1171.6404\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1281.3090 - val_loss: 1169.1406\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1281.5044 - val_loss: 1166.9065\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1273.4346 - val_loss: 1166.7525\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1269.7955 - val_loss: 1167.2790\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1268.3524 - val_loss: 1167.5467\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1266.4330 - val_loss: 1164.9316\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1262.2400 - val_loss: 1164.4791\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1262.6779 - val_loss: 1164.4687\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1255.9543 - val_loss: 1160.8456\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1255.0735 - val_loss: 1162.2560\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1253.9235 - val_loss: 1160.8213\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1251.9068 - val_loss: 1158.9893\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1250.5600 - val_loss: 1160.3601\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1247.0669 - val_loss: 1159.1169\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1248.0285 - val_loss: 1162.8354\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1245.2895 - val_loss: 1159.0963\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1244.0646 - val_loss: 1157.6930\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 109us/step - loss: 1239.6668 - val_loss: 1157.0499\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1238.1634 - val_loss: 1156.7412\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1237.0762 - val_loss: 1155.9684\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1236.5783 - val_loss: 1158.3940\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1236.1672 - val_loss: 1157.9396\n",
      "nfold:5,bag:4 1159.26744858\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 14s 100us/step - loss: 2102.3131 - val_loss: 1258.4866\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1382.0086 - val_loss: 1192.4882\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1354.8761 - val_loss: 1191.5485\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1345.4745 - val_loss: 1183.7126\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1339.2359 - val_loss: 1180.1609\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 12s 87us/step - loss: 1331.3818 - val_loss: 1172.8239\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1327.2923 - val_loss: 1171.8619\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1320.2975 - val_loss: 1168.6833\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1316.6727 - val_loss: 1168.4210\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1313.1046 - val_loss: 1163.2811\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1305.2027 - val_loss: 1164.4002\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1301.5019 - val_loss: 1162.4884\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1300.1417 - val_loss: 1161.6445\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1296.4363 - val_loss: 1159.9654\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1294.5246 - val_loss: 1160.4894\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1289.1812 - val_loss: 1159.4732\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1286.3707 - val_loss: 1156.7266\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1283.4868 - val_loss: 1158.2200\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1282.0148 - val_loss: 1157.4829\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1278.4789 - val_loss: 1156.1497\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1276.5831 - val_loss: 1156.6455\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1278.3334 - val_loss: 1155.1537\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1269.8617 - val_loss: 1156.9565\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1269.3789 - val_loss: 1155.5742\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1265.9565 - val_loss: 1158.8763\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 16s 114us/step - loss: 1260.4607 - val_loss: 1153.7564\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1264.9074 - val_loss: 1154.8910\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1257.2462 - val_loss: 1154.0090\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1257.0305 - val_loss: 1150.6554\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1254.3338 - val_loss: 1154.5971\n",
      "nfold:6,bag:0 1159.28491022\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1989.0387 - val_loss: 1209.4004\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1360.6790 - val_loss: 1194.3759\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1341.6785 - val_loss: 1181.3275\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1332.7460 - val_loss: 1177.9843\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1328.7788 - val_loss: 1173.9699\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1321.8689 - val_loss: 1174.7133\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1319.4908 - val_loss: 1167.6399\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1313.9771 - val_loss: 1169.9775\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1311.0821 - val_loss: 1164.8333\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1306.0475 - val_loss: 1167.4160\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1304.4069 - val_loss: 1163.4237\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1300.4514 - val_loss: 1164.1260\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1293.7633 - val_loss: 1160.8311\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1294.7476 - val_loss: 1163.0543\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1294.7990 - val_loss: 1161.2705\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1293.3792 - val_loss: 1158.7348\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1284.6537 - val_loss: 1159.8503\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1283.5017 - val_loss: 1159.7432\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1278.4538 - val_loss: 1159.1344\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1277.3876 - val_loss: 1154.7717\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1276.4318 - val_loss: 1154.9838\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1273.2624 - val_loss: 1157.0397\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1272.7181 - val_loss: 1156.0087\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1269.5179 - val_loss: 1155.9174\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1266.9918 - val_loss: 1158.0073\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 109us/step - loss: 1265.2191 - val_loss: 1153.7262\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1263.4018 - val_loss: 1153.3828\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1258.5077 - val_loss: 1153.9392\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1260.0532 - val_loss: 1151.2405\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1255.3986 - val_loss: 1151.1746\n",
      "nfold:6,bag:1 1157.83744733\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 2113.1777 - val_loss: 1249.2940\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1380.4361 - val_loss: 1201.0992\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1355.9594 - val_loss: 1185.1025\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1345.8509 - val_loss: 1182.6098\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1336.2988 - val_loss: 1175.4976\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1332.5081 - val_loss: 1178.8255\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1324.7351 - val_loss: 1170.0786\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1320.2852 - val_loss: 1169.2272\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1315.3656 - val_loss: 1166.9278\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1308.2752 - val_loss: 1164.6953\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1306.9598 - val_loss: 1164.7529\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1299.7435 - val_loss: 1162.4217\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1296.9709 - val_loss: 1164.1243\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1293.5433 - val_loss: 1164.8134\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1291.3347 - val_loss: 1163.2156\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 12s 87us/step - loss: 1283.0305 - val_loss: 1160.2323\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1285.2989 - val_loss: 1156.2960\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1282.8980 - val_loss: 1158.2929\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1278.7307 - val_loss: 1157.0136\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1278.1498 - val_loss: 1159.3363\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1273.9035 - val_loss: 1156.7966\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 13s 92us/step - loss: 1271.8235 - val_loss: 1155.2335\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1272.8105 - val_loss: 1156.2097\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1266.5472 - val_loss: 1154.8536\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 13s 95us/step - loss: 1262.1951 - val_loss: 1152.7147\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 16s 115us/step - loss: 1261.6176 - val_loss: 1153.2547\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1259.8003 - val_loss: 1160.1466\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1258.9724 - val_loss: 1155.0138\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 203us/step - loss: 1255.4697 - val_loss: 1151.8589\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1254.6139 - val_loss: 1151.5999\n",
      "nfold:6,bag:2 1158.15212854\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 2020.7822 - val_loss: 1307.4282\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1503.8529 - val_loss: 1239.2128\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1474.3761 - val_loss: 1223.7681\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1452.7265 - val_loss: 1213.1966\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1439.7967 - val_loss: 1203.0149\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 14s 100us/step - loss: 1425.0387 - val_loss: 1189.7782\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1415.3804 - val_loss: 1183.0461\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1406.3972 - val_loss: 1175.5141\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1396.9383 - val_loss: 1174.2538\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1383.1540 - val_loss: 1174.2530\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1379.5349 - val_loss: 1170.7847\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1369.6111 - val_loss: 1172.7555\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1360.5304 - val_loss: 1165.9147\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1357.7400 - val_loss: 1165.6355\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1347.3440 - val_loss: 1163.1908\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 13s 92us/step - loss: 1344.9834 - val_loss: 1165.3003\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1338.2212 - val_loss: 1162.3238\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 13s 94us/step - loss: 1336.5768 - val_loss: 1164.7208\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1331.2346 - val_loss: 1160.0511\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 91us/step - loss: 1322.1921 - val_loss: 1162.6973\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1318.6611 - val_loss: 1158.8281\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1310.5918 - val_loss: 1159.6999\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1305.6714 - val_loss: 1158.3589\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1306.3382 - val_loss: 1159.1194\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1296.6445 - val_loss: 1157.7159\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 109us/step - loss: 1291.5536 - val_loss: 1157.9570\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1289.7564 - val_loss: 1155.9906\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1292.9958 - val_loss: 1154.5446\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1280.2365 - val_loss: 1155.4671\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1278.1761 - val_loss: 1152.7805\n",
      "nfold:6,bag:3 1160.55850358\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1977.3183 - val_loss: 1246.0161\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1406.8148 - val_loss: 1212.9384\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1386.0730 - val_loss: 1191.0250\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1377.5029 - val_loss: 1185.4539\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1369.6804 - val_loss: 1178.8700\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1363.8008 - val_loss: 1184.1222\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1355.4837 - val_loss: 1177.6054\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1348.8178 - val_loss: 1172.8988\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1344.9606 - val_loss: 1171.5160\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1340.3413 - val_loss: 1169.8713\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1336.4737 - val_loss: 1168.7019\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1326.8832 - val_loss: 1165.4144\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1328.5751 - val_loss: 1166.8302\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1327.4145 - val_loss: 1163.6398\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1320.7822 - val_loss: 1160.8486\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1316.3448 - val_loss: 1159.6257\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1306.5944 - val_loss: 1162.0718\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1305.0759 - val_loss: 1159.8624\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1301.9630 - val_loss: 1158.0100\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1295.6367 - val_loss: 1158.4496\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1298.3522 - val_loss: 1157.4326\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1291.7011 - val_loss: 1158.2988\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1282.7511 - val_loss: 1156.0675\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1288.0872 - val_loss: 1154.1121\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1279.6910 - val_loss: 1154.5607\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 16s 118us/step - loss: 1276.5290 - val_loss: 1152.5214\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1278.0584 - val_loss: 1158.8177\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1271.8754 - val_loss: 1154.3675\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1270.4001 - val_loss: 1152.6825\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1261.3224 - val_loss: 1154.4973\n",
      "nfold:6,bag:4 1159.60891786\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 14s 105us/step - loss: 2004.6371 - val_loss: 1207.2381\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1391.5774 - val_loss: 1198.5894\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1376.8205 - val_loss: 1184.3151\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1366.8512 - val_loss: 1178.9794\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1363.0776 - val_loss: 1175.8762\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1350.5984 - val_loss: 1177.6256\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1349.7204 - val_loss: 1179.5288\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1344.3690 - val_loss: 1169.7819\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1339.4823 - val_loss: 1166.4757\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1337.2234 - val_loss: 1166.3489\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1328.9839 - val_loss: 1168.9159\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1324.7385 - val_loss: 1164.9325\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1321.4761 - val_loss: 1163.8194\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1317.1194 - val_loss: 1161.4001\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1314.3192 - val_loss: 1161.9231\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1310.2400 - val_loss: 1160.9239\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1308.3953 - val_loss: 1158.5571\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1301.9487 - val_loss: 1160.8186\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1300.6850 - val_loss: 1155.2319\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1297.2453 - val_loss: 1156.6159\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1295.1806 - val_loss: 1155.4476\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1293.1416 - val_loss: 1155.9139\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1285.3046 - val_loss: 1154.8401\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1283.5304 - val_loss: 1154.0782\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1280.5248 - val_loss: 1153.4687\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 16s 115us/step - loss: 1279.0895 - val_loss: 1151.7377\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1276.3634 - val_loss: 1152.0365\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1270.9851 - val_loss: 1150.0713\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1270.4575 - val_loss: 1154.7022\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1270.6451 - val_loss: 1149.9915\n",
      "nfold:7,bag:0 1157.25298407\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1970.0115 - val_loss: 1227.8392\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1400.6342 - val_loss: 1201.3927\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 1367.2420 - val_loss: 1180.6095\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1357.0353 - val_loss: 1182.8958\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1347.4438 - val_loss: 1170.3804\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1336.2613 - val_loss: 1168.5470\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1329.4355 - val_loss: 1166.7090\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1324.8919 - val_loss: 1165.4879\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1317.5221 - val_loss: 1162.5508\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1316.1857 - val_loss: 1161.1602\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1311.6487 - val_loss: 1162.2303\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1306.8237 - val_loss: 1162.8117\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1305.5127 - val_loss: 1162.2236\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1302.9626 - val_loss: 1156.3716\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1293.0237 - val_loss: 1158.6473\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1293.0068 - val_loss: 1155.8440\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1287.2037 - val_loss: 1154.7549\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1282.0742 - val_loss: 1158.3206\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1281.3247 - val_loss: 1158.2561\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1276.0005 - val_loss: 1151.9786\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1275.0088 - val_loss: 1152.4773\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1271.1544 - val_loss: 1151.9037\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1273.3615 - val_loss: 1151.9502\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1264.6147 - val_loss: 1156.9974\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1264.5699 - val_loss: 1151.0132\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 113us/step - loss: 1261.1779 - val_loss: 1149.7650\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1259.1593 - val_loss: 1149.9249\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1255.8739 - val_loss: 1149.4933\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1252.4152 - val_loss: 1149.1142\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1250.1434 - val_loss: 1149.0452\n",
      "nfold:7,bag:1 1150.5020168\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1888.4042 - val_loss: 1247.6432\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1459.1542 - val_loss: 1205.4619\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1430.8208 - val_loss: 1200.3767\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1417.9892 - val_loss: 1187.0505\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 12s 87us/step - loss: 1410.2847 - val_loss: 1177.0643\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1400.9766 - val_loss: 1178.4884\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1393.0272 - val_loss: 1173.8809\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1380.1265 - val_loss: 1173.0272\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1372.2639 - val_loss: 1168.8161\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1369.5104 - val_loss: 1169.7550\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1361.0649 - val_loss: 1170.2875\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1351.7501 - val_loss: 1165.1223\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1342.7406 - val_loss: 1163.5030\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1344.2398 - val_loss: 1160.2521\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1334.1249 - val_loss: 1164.1240\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1330.6578 - val_loss: 1161.6904\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1321.5450 - val_loss: 1160.2408\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1318.2903 - val_loss: 1158.9298\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1315.1071 - val_loss: 1157.6846\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1309.5841 - val_loss: 1157.6047\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1302.0433 - val_loss: 1154.0422\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1300.5128 - val_loss: 1157.9254\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1292.8670 - val_loss: 1154.3841\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1287.8004 - val_loss: 1153.7785\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1286.8825 - val_loss: 1154.4344\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 114us/step - loss: 1281.3726 - val_loss: 1151.7534\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1275.3605 - val_loss: 1149.6544\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1274.2107 - val_loss: 1154.2803\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1271.6289 - val_loss: 1151.5095\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1267.9547 - val_loss: 1151.0382\n",
      "nfold:7,bag:2 1159.13185554\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 2014.4631 - val_loss: 1272.5646\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1462.7806 - val_loss: 1208.3046\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1437.4129 - val_loss: 1200.0751\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1422.9615 - val_loss: 1201.8511\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1408.2445 - val_loss: 1181.7564\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1402.8935 - val_loss: 1184.5862\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1395.1643 - val_loss: 1174.5637\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1384.0885 - val_loss: 1168.8534\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1376.6715 - val_loss: 1169.8919\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1378.5859 - val_loss: 1167.0885\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1368.0289 - val_loss: 1170.7467\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1363.9910 - val_loss: 1166.0096\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1353.8180 - val_loss: 1161.7556\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1347.5905 - val_loss: 1163.8856\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1347.8828 - val_loss: 1161.2943\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1333.7880 - val_loss: 1160.4693\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1338.7333 - val_loss: 1165.3958\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1331.2002 - val_loss: 1163.5797\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1322.9888 - val_loss: 1159.4479\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1325.0025 - val_loss: 1157.4229\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1312.4426 - val_loss: 1157.4659\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1308.0176 - val_loss: 1154.2155\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1304.8784 - val_loss: 1153.3305\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1306.8725 - val_loss: 1153.9205\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1295.9465 - val_loss: 1151.1790\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 16s 116us/step - loss: 1292.2811 - val_loss: 1150.3661\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1287.8752 - val_loss: 1153.6879\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1282.1094 - val_loss: 1152.4003\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1281.1463 - val_loss: 1153.3727\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1280.3962 - val_loss: 1151.5207\n",
      "nfold:7,bag:3 1158.9181354\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 14s 100us/step - loss: 2021.3671 - val_loss: 1228.3885\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1379.0354 - val_loss: 1193.6236\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1365.0178 - val_loss: 1179.8282\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1352.3671 - val_loss: 1177.0055\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1349.0881 - val_loss: 1172.8085\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1340.0615 - val_loss: 1168.7707\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1335.6207 - val_loss: 1164.8112\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1332.9284 - val_loss: 1165.1560\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1326.4155 - val_loss: 1166.4831\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1317.3799 - val_loss: 1162.8781\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1323.6849 - val_loss: 1163.6316\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1314.9050 - val_loss: 1163.9945\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1310.4484 - val_loss: 1160.5798\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1304.7577 - val_loss: 1159.8733\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 12s 85us/step - loss: 1301.4309 - val_loss: 1158.1881\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1301.0248 - val_loss: 1159.5597\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1296.5498 - val_loss: 1160.6282\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1287.8516 - val_loss: 1156.9233\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1288.4059 - val_loss: 1154.9089\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1288.3737 - val_loss: 1155.4500\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1284.8074 - val_loss: 1152.9778\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 1281.1596 - val_loss: 1154.9701\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1277.7923 - val_loss: 1155.3360\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1274.1656 - val_loss: 1153.7877\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1274.7408 - val_loss: 1151.0386\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 16s 120us/step - loss: 1272.4425 - val_loss: 1152.3227\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1269.4292 - val_loss: 1150.8766\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1265.7106 - val_loss: 1151.3809\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1259.2009 - val_loss: 1148.4761\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1261.2731 - val_loss: 1148.9146\n",
      "nfold:7,bag:4 1157.37211693\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 15s 107us/step - loss: 1995.0305 - val_loss: 1269.9605\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1479.7049 - val_loss: 1220.6781\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1459.0377 - val_loss: 1211.8944\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1447.8002 - val_loss: 1207.1788\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1428.8464 - val_loss: 1194.5893\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1430.2936 - val_loss: 1189.1552\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1415.4753 - val_loss: 1182.5034\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1409.0580 - val_loss: 1183.7665\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1403.7991 - val_loss: 1181.7641\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1391.2177 - val_loss: 1176.7418\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1383.9522 - val_loss: 1172.7229\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1375.9317 - val_loss: 1175.4963\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1372.3655 - val_loss: 1171.7656\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1359.1377 - val_loss: 1170.8400\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1355.9598 - val_loss: 1168.2007\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1350.2544 - val_loss: 1169.1753\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1342.9562 - val_loss: 1168.2005\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1337.0210 - val_loss: 1165.1279\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1330.5396 - val_loss: 1165.7810\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1329.3896 - val_loss: 1167.2880\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1321.7901 - val_loss: 1163.6254\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1313.6813 - val_loss: 1164.3021\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1315.3870 - val_loss: 1161.6650\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1306.8555 - val_loss: 1161.7974\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1302.3160 - val_loss: 1162.1296\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 118us/step - loss: 1294.8393 - val_loss: 1162.5182\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 28s 208us/step - loss: 1294.8349 - val_loss: 1159.9702\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 28s 207us/step - loss: 1289.9907 - val_loss: 1160.0136\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 28s 210us/step - loss: 1287.4016 - val_loss: 1159.2812\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 29s 211us/step - loss: 1284.2054 - val_loss: 1159.9671\n",
      "nfold:8,bag:0 1151.83501149\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 13s 100us/step - loss: 2098.9063 - val_loss: 1351.0775\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1542.5273 - val_loss: 1259.4404\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1502.0732 - val_loss: 1239.3959\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1485.6962 - val_loss: 1224.7136\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1470.6794 - val_loss: 1213.5983\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1459.8529 - val_loss: 1213.9542\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1447.1863 - val_loss: 1201.2943\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1442.0432 - val_loss: 1198.1488\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1426.6030 - val_loss: 1189.4760\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1422.3896 - val_loss: 1187.0863\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1413.5776 - val_loss: 1182.8717\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1401.7431 - val_loss: 1184.1588\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1397.2668 - val_loss: 1177.1762\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1381.5796 - val_loss: 1176.1596\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1375.2722 - val_loss: 1176.9313\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1368.2634 - val_loss: 1170.6866\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1361.8188 - val_loss: 1170.5321\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1357.2843 - val_loss: 1179.3677\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1349.7611 - val_loss: 1166.8896\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1344.9752 - val_loss: 1170.4407\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1338.0166 - val_loss: 1173.3896\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1336.9392 - val_loss: 1167.0960\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1327.9501 - val_loss: 1166.6049\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1323.5030 - val_loss: 1163.7626\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 12s 87us/step - loss: 1318.8721 - val_loss: 1166.4336\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 122us/step - loss: 1312.2733 - val_loss: 1163.4217\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 29s 213us/step - loss: 1309.7701 - val_loss: 1162.8173\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 30s 218us/step - loss: 1307.5007 - val_loss: 1163.5249\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 30s 223us/step - loss: 1296.8710 - val_loss: 1162.0324\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 30s 219us/step - loss: 1294.8233 - val_loss: 1165.5866\n",
      "nfold:8,bag:1 1156.6310628\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 14s 102us/step - loss: 2249.0383 - val_loss: 1297.9463\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1445.5962 - val_loss: 1220.0156\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1406.1350 - val_loss: 1203.4092\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1383.4631 - val_loss: 1192.9150\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1376.2769 - val_loss: 1191.1032\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1363.4885 - val_loss: 1185.5536\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1359.2465 - val_loss: 1181.1139\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1354.4195 - val_loss: 1179.1789\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1352.0601 - val_loss: 1176.3523\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1339.4013 - val_loss: 1175.1164\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1336.2531 - val_loss: 1171.4441\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1332.7961 - val_loss: 1175.6712\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1328.0603 - val_loss: 1168.1530\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1325.3107 - val_loss: 1170.8078\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1322.9449 - val_loss: 1166.7270\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1313.1637 - val_loss: 1171.0451\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1312.6849 - val_loss: 1166.9436\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1306.7446 - val_loss: 1171.1406\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1302.9094 - val_loss: 1167.8337\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1299.3937 - val_loss: 1163.9272\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1297.1222 - val_loss: 1163.7437\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1289.9582 - val_loss: 1163.6991\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1288.7273 - val_loss: 1162.7071\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1286.7265 - val_loss: 1163.3868\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1281.6723 - val_loss: 1161.8162\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 17s 125us/step - loss: 1280.4305 - val_loss: 1159.6845\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 31s 225us/step - loss: 1280.7426 - val_loss: 1162.4125\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 31s 228us/step - loss: 1275.4617 - val_loss: 1162.7915\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 31s 228us/step - loss: 1275.0968 - val_loss: 1161.0004\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 30s 223us/step - loss: 1270.9628 - val_loss: 1158.1629\n",
      "nfold:8,bag:2 1143.22328377\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 14s 100us/step - loss: 2087.1304 - val_loss: 1312.3027\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1522.9371 - val_loss: 1243.9402\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1476.3200 - val_loss: 1223.1150\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1455.8020 - val_loss: 1214.0893\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1440.6386 - val_loss: 1200.7789\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1430.4535 - val_loss: 1198.1484\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1421.5847 - val_loss: 1188.1881\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1411.2174 - val_loss: 1185.6438\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1399.5771 - val_loss: 1180.5691\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1395.0852 - val_loss: 1178.7913\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1382.4248 - val_loss: 1177.2350\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1375.0805 - val_loss: 1174.4788\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 86us/step - loss: 1367.5984 - val_loss: 1170.4559\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1362.9294 - val_loss: 1170.4080\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1355.4998 - val_loss: 1170.5011\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1347.1241 - val_loss: 1167.6128\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1343.8161 - val_loss: 1165.5275\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1334.0717 - val_loss: 1167.9852\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1321.4527 - val_loss: 1166.9534\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1324.8022 - val_loss: 1165.0894\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1318.3627 - val_loss: 1161.9213\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1315.0439 - val_loss: 1162.1020\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1306.0069 - val_loss: 1161.1630\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1303.2403 - val_loss: 1162.0872\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1301.2464 - val_loss: 1160.2948\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 17s 127us/step - loss: 1295.9595 - val_loss: 1161.3472\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 31s 229us/step - loss: 1288.8966 - val_loss: 1161.9996\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 32s 237us/step - loss: 1284.4778 - val_loss: 1161.0827\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 31s 230us/step - loss: 1282.6179 - val_loss: 1158.4493\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 32s 234us/step - loss: 1282.7338 - val_loss: 1159.3954\n",
      "nfold:8,bag:3 1150.68965516\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 14s 103us/step - loss: 2291.4621 - val_loss: 1315.8764\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1338.3291 - val_loss: 1199.4902\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1315.0434 - val_loss: 1193.7192\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 12s 92us/step - loss: 1308.7301 - val_loss: 1186.5224\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1302.5206 - val_loss: 1182.0422\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1295.3467 - val_loss: 1179.4417\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1292.0860 - val_loss: 1181.0846\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1288.5089 - val_loss: 1175.6624\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1284.1568 - val_loss: 1175.4851\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1281.2952 - val_loss: 1173.1553\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1275.6021 - val_loss: 1170.6424\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1273.4828 - val_loss: 1168.7170\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1269.1661 - val_loss: 1167.2722\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1267.1666 - val_loss: 1167.3648\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1267.5039 - val_loss: 1166.8678\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1264.2786 - val_loss: 1165.9864\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1261.2048 - val_loss: 1164.1948\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1258.6198 - val_loss: 1166.3908\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1255.6081 - val_loss: 1162.4775\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 13s 99us/step - loss: 1256.4815 - val_loss: 1161.0897\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1253.5551 - val_loss: 1160.9473\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1248.7922 - val_loss: 1162.5376\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1250.5690 - val_loss: 1158.7073\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 13s 92us/step - loss: 1246.5440 - val_loss: 1160.0097\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1246.2718 - val_loss: 1158.3198\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 121us/step - loss: 1244.4971 - val_loss: 1159.4576\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 31s 230us/step - loss: 1243.4090 - val_loss: 1158.9207\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 31s 232us/step - loss: 1244.3621 - val_loss: 1157.2711\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 32s 232us/step - loss: 1245.4207 - val_loss: 1157.6364\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 30s 219us/step - loss: 1241.1436 - val_loss: 1160.5687\n",
      "nfold:8,bag:4 1144.75279596\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 14s 106us/step - loss: 2176.1661 - val_loss: 1267.6737\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1345.5434 - val_loss: 1199.2004\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1326.2695 - val_loss: 1190.7277\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1313.7105 - val_loss: 1185.0739\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1306.2015 - val_loss: 1185.8876\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1301.6920 - val_loss: 1178.4904\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1296.1448 - val_loss: 1177.3644\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1293.7924 - val_loss: 1174.6013\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1287.7955 - val_loss: 1172.4610\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1288.0807 - val_loss: 1170.9974\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1281.0759 - val_loss: 1168.3981\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1278.8393 - val_loss: 1170.4243\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1275.7802 - val_loss: 1167.0220\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1277.0139 - val_loss: 1166.6656\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1274.5539 - val_loss: 1165.0196\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 87us/step - loss: 1268.7250 - val_loss: 1166.9632\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1267.5030 - val_loss: 1166.5229\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1262.6674 - val_loss: 1163.1016\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1260.9127 - val_loss: 1166.2919\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1260.7135 - val_loss: 1162.0290\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1256.3712 - val_loss: 1163.1967\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1256.1266 - val_loss: 1164.9912\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1255.0988 - val_loss: 1161.4544\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1251.1437 - val_loss: 1159.9656\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 14s 100us/step - loss: 1251.1311 - val_loss: 1161.2318\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 17s 125us/step - loss: 1248.1299 - val_loss: 1161.3842\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 32s 239us/step - loss: 1246.4242 - val_loss: 1159.7246\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 33s 241us/step - loss: 1242.9706 - val_loss: 1160.0273\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 32s 238us/step - loss: 1243.7536 - val_loss: 1160.8088\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 31s 232us/step - loss: 1240.4378 - val_loss: 1160.1879\n",
      "nfold:9,bag:0 1149.9410364\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 15s 107us/step - loss: 2041.8775 - val_loss: 1294.3885\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1526.3303 - val_loss: 1243.1022\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1470.9367 - val_loss: 1218.8688\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1453.1358 - val_loss: 1202.1554\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1437.7829 - val_loss: 1202.0401\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1426.3947 - val_loss: 1193.4229\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1414.9901 - val_loss: 1188.6143\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 13s 99us/step - loss: 1401.7618 - val_loss: 1184.4683\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1395.8879 - val_loss: 1178.0204\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1392.0069 - val_loss: 1179.0279\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1381.1317 - val_loss: 1171.9205\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1375.3058 - val_loss: 1174.6021\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1368.0172 - val_loss: 1174.8065\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 13s 97us/step - loss: 1358.8739 - val_loss: 1170.7539\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1354.2854 - val_loss: 1170.8578\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1351.9035 - val_loss: 1171.0982\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1342.6258 - val_loss: 1166.3252\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1339.2100 - val_loss: 1163.8139\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1330.4638 - val_loss: 1166.2842\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1328.1965 - val_loss: 1166.2774\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1325.1886 - val_loss: 1166.2058\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1320.0434 - val_loss: 1163.5806\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1315.6765 - val_loss: 1161.7680\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1308.4380 - val_loss: 1162.2428\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1304.8787 - val_loss: 1161.4137\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 18s 135us/step - loss: 1302.1792 - val_loss: 1162.2432\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 33s 244us/step - loss: 1298.1465 - val_loss: 1160.0629\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 32s 236us/step - loss: 1293.9708 - val_loss: 1162.0797\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 33s 241us/step - loss: 1286.4990 - val_loss: 1157.8137\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 33s 243us/step - loss: 1287.3012 - val_loss: 1160.0700\n",
      "nfold:9,bag:1 1156.42812443\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 15s 110us/step - loss: 1986.2763 - val_loss: 1234.3037\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1414.6718 - val_loss: 1208.2512\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1394.4509 - val_loss: 1195.9826\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1380.1510 - val_loss: 1188.6731\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1373.5819 - val_loss: 1190.0606\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1359.2842 - val_loss: 1182.0392\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1359.2784 - val_loss: 1178.7034\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1351.4896 - val_loss: 1177.3949\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1343.9613 - val_loss: 1175.0521\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1340.0500 - val_loss: 1173.2551\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1327.9690 - val_loss: 1167.4512\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1327.9357 - val_loss: 1167.4367\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1324.8613 - val_loss: 1167.3166\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1322.5204 - val_loss: 1163.9941\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1316.6160 - val_loss: 1162.0649\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1306.7445 - val_loss: 1165.6579\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 88us/step - loss: 1303.9123 - val_loss: 1164.1646\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1299.7373 - val_loss: 1164.2509\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1300.0009 - val_loss: 1163.6147\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 14s 103us/step - loss: 1293.9142 - val_loss: 1164.3594\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1292.2442 - val_loss: 1160.0114\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1285.2506 - val_loss: 1160.1034\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1283.6941 - val_loss: 1159.7283\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1278.9994 - val_loss: 1160.8537\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1277.4599 - val_loss: 1161.1766\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 18s 131us/step - loss: 1273.9687 - val_loss: 1162.1494\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 33s 245us/step - loss: 1274.2520 - val_loss: 1158.3174\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 32s 237us/step - loss: 1270.6648 - val_loss: 1158.5390\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 33s 242us/step - loss: 1267.9190 - val_loss: 1156.5519\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 31s 227us/step - loss: 1263.8951 - val_loss: 1159.9094\n",
      "nfold:9,bag:2 1157.63626831\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 15s 107us/step - loss: 2208.1162 - val_loss: 1299.2860\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1437.6421 - val_loss: 1211.7655\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1396.3394 - val_loss: 1202.1483\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1373.5198 - val_loss: 1190.8342\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1366.6810 - val_loss: 1185.6953\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1355.2451 - val_loss: 1189.3218\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1349.9826 - val_loss: 1180.8442\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1346.4206 - val_loss: 1178.2079\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1339.6359 - val_loss: 1172.8653\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1332.6502 - val_loss: 1174.9688\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1329.8063 - val_loss: 1171.5354\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1322.4702 - val_loss: 1172.1963\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1317.8085 - val_loss: 1173.2666\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 1313.7293 - val_loss: 1169.8100\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1308.5989 - val_loss: 1168.1641\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1304.7560 - val_loss: 1167.9871\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1301.7088 - val_loss: 1166.9954\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1298.0517 - val_loss: 1166.2727\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1290.1623 - val_loss: 1164.4763\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1292.2967 - val_loss: 1160.9512\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1283.1380 - val_loss: 1162.6262\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1283.6895 - val_loss: 1165.9527\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 14s 100us/step - loss: 1277.9612 - val_loss: 1161.5547\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 13s 96us/step - loss: 1273.0456 - val_loss: 1161.2088\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 1271.0892 - val_loss: 1160.2710\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 19s 143us/step - loss: 1266.3669 - val_loss: 1164.1971\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 33s 242us/step - loss: 1265.9344 - val_loss: 1158.6002\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 34s 252us/step - loss: 1263.6676 - val_loss: 1160.4005\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 33s 243us/step - loss: 1257.8966 - val_loss: 1159.0719\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 33s 241us/step - loss: 1256.2180 - val_loss: 1157.9200\n",
      "nfold:9,bag:3 1151.07024515\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 15s 109us/step - loss: 2103.4908 - val_loss: 1302.1991\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1455.0058 - val_loss: 1217.3484\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 1418.3963 - val_loss: 1211.8713\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1403.3806 - val_loss: 1196.9150\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 13s 98us/step - loss: 1389.2765 - val_loss: 1185.5505\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1377.3969 - val_loss: 1184.9848\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1377.1511 - val_loss: 1178.9845\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 13s 92us/step - loss: 1364.4009 - val_loss: 1174.8950\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1353.8183 - val_loss: 1176.7666\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1352.0727 - val_loss: 1172.1029\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1342.3110 - val_loss: 1169.5152\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 1335.7435 - val_loss: 1171.9921\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1328.8857 - val_loss: 1166.7092\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 13s 95us/step - loss: 1324.5999 - val_loss: 1168.5742\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 14s 100us/step - loss: 1318.4141 - val_loss: 1171.5121\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 14s 103us/step - loss: 1313.6580 - val_loss: 1163.8243\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1308.8006 - val_loss: 1167.4840\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 13s 96us/step - loss: 1299.7331 - val_loss: 1162.5608\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 13s 99us/step - loss: 1301.3223 - val_loss: 1160.8178\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1293.4697 - val_loss: 1161.7901\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1287.4730 - val_loss: 1160.6481\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1280.5328 - val_loss: 1160.0821\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 13s 97us/step - loss: 1280.0349 - val_loss: 1160.0645\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 13s 93us/step - loss: 1273.5980 - val_loss: 1159.1896\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 1269.5610 - val_loss: 1158.4641\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 19s 139us/step - loss: 1269.1996 - val_loss: 1159.1195\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 34s 251us/step - loss: 1263.9807 - val_loss: 1159.1663\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 33s 246us/step - loss: 1256.2416 - val_loss: 1158.7770\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 34s 251us/step - loss: 1259.4473 - val_loss: 1160.0093\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 33s 245us/step - loss: 1254.9195 - val_loss: 1155.7209\n",
      "nfold:9,bag:4 1152.67854849\n",
      "CV score for the final model: [ 1161.0701086   1162.41272987  1161.20918556  1161.30563898  1160.11805658]\n"
     ]
    }
   ],
   "source": [
    "nnmodel={}\n",
    "nnmodel.clear()\n",
    "def cross_validate_mlp(mlp_func, nfolds=10,nbags=5):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nbags,))\n",
    "    stack_train = np.zeros((nbags,len(train_y)))\n",
    "    stack_test = np.zeros((nbags,len(test)))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        for bag in range(nbags):\n",
    "            nnmodel['nn%d',k*10+bag*1] = mlp_func(seed = k*10+bag*1)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            fit = nnmodel['nn%d',k*10+bag*1].fit(xtr, ytr, validation_split=0.2, batch_size=128,\n",
    "                          epochs=30, verbose=1, callbacks=[ExponentialMovingAverage()])\n",
    "            pred = nnmodel['nn%d',k*10+bag*1].predict(xte, batch_size=1024)\n",
    "            #score = mean_absolute_error(np.exp(yte), np.exp(pred))\n",
    "            score = mean_absolute_error(yte, pred)\n",
    "            val_scores[bag] += score\n",
    "            #stack_train[bag][test_index] = pred[:,0]\n",
    "            print (\"nfold:{},bag:{}\".format(k,bag),score)\n",
    "    for bag in range(nbags):\n",
    "        val_scores[bag] = val_scores[bag] / float(nfolds)\n",
    "    \n",
    "    \n",
    "    return val_scores\n",
    "\n",
    "cv_score = cross_validate_mlp(hyper_model)\n",
    "print (\"CV score for the final model:\", cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'l3-nn': {\n",
    "        'predictions': l2_predictions,\n",
    "        'n_bags': 4,\n",
    "        'model': Keras(nn_lr, lambda: {'l1': 1e-5, 'l2': 1e-5, 'n_epoch': 30, 'batch_size': 128, 'optimizer': SGD(3e-2, momentum=0.8, nesterov=True, decay=3e-5), 'callbacks': [ExponentialMovingAverage(save_mv_ave_model=False)]}),\n",
    "    },\n",
    "     'optimizer': SGD(1e-4, momentum=0.9, nesterov=True, decay=5e-5)\n",
    "            'optimizer': SGD(1e-5, momentum=0.9, nesterov=True, decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, \n",
    " 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta',\n",
    " 'wdecay': 0.0020300000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.1, batch_size=128, \n",
    "                      epochs=10, verbose=0, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=512)\n",
    "        score = mean_absolute_error(np.exp(yte), np.exp(pred))\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 463, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 155, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 20, 'optimizer': 'adam', 'wdecay': 0.0012891891891891893}\n",
      "Fold 0, MAE: 1664.3154518123333\n",
      "Fold 1, MAE: 1789.6478768193072\n",
      "Fold 2, MAE: 2230.6679431902985\n",
      "3-fold CV score: 1894.877090607313\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 308, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 244, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 28, 'optimizer': 'adadelta', 'wdecay': 0.0075522522522522527}\n",
      "Fold 0, MAE: 2663.589775993532\n",
      "Fold 1, MAE: 2550.3793884253055\n",
      "Fold 2, MAE: 2698.309167392798\n",
      "3-fold CV score: 2637.426110603878\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 394, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 120, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 65, 'optimizer': 'adam', 'wdecay': 0.0076711711711711711}\n",
      "Fold 0, MAE: 1349.8792955749364\n",
      "Fold 1, MAE: 1348.0942437736849\n",
      "Fold 2, MAE: 1365.2535296867036\n",
      "3-fold CV score: 1354.409023011775\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 541, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 258, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.0024189189189189188}\n",
      "Fold 0, MAE: 2598.4352573980386\n",
      "Fold 1, MAE: 2447.588075147961\n",
      "Fold 2, MAE: 2721.9736304441863\n",
      "3-fold CV score: 2589.332320996729\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 472, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 217, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 71, 'optimizer': 'adadelta', 'wdecay': 0.0055603603603603608}\n",
      "Fold 0, MAE: 1613.5498840747402\n",
      "Fold 1, MAE: 1411.0097020300386\n",
      "Fold 2, MAE: 1711.7215931226579\n",
      "3-fold CV score: 1578.7603930758123\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 429, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 224, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 36, 'optimizer': 'adam', 'wdecay': 0.0053720720720720726}\n",
      "Fold 0, MAE: 1803.0232709901081\n",
      "Fold 1, MAE: 2716.749151946116\n",
      "Fold 2, MAE: 2137.6247824488005\n",
      "3-fold CV score: 2219.1324017950083\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 308, 'hidden2_dropout': 0.5, 'hidden2_units': 148, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 69, 'optimizer': 'adam', 'wdecay': 0.0020225225225225223}\n",
      "Fold 0, MAE: 2290.4482287011933\n",
      "Fold 1, MAE: 1908.4728142537497\n",
      "Fold 2, MAE: 1895.8826351188964\n",
      "3-fold CV score: 2031.6012260246132\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 481, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 127, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.0085432432432432422}\n",
      "Fold 0, MAE: 1813.761108549234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-f3e899cfd5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperopt_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-f3e899cfd5b2>\u001b[0m in \u001b[0;36mhyperopt_search\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcv_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcv_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-50776d6fa3b5>\u001b[0m in \u001b[0;36mcross_validate_mlp\u001b[0;34m(mlp_func, nfolds)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         fit = mlp.fit(xtr, ytr, validation_split=0.1, batch_size=128, \n\u001b[0;32m---> 12\u001b[0;31m                       epochs=10, verbose=0, callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VERSION 4. Insights:\n",
    "# – why not to test 4-layer architectures?\n",
    "# — we need to introduce new optimizers\n",
    "# — adding batch normalization (https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "        'hidden3_dropout': hp.choice('hidden3_dropout', np.linspace(0.1,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int32')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int32')),\n",
    "         'hidden3_units': hp.choice('hidden3_units', np.linspace(20,80,30,dtype='int32')),\n",
    "         'optimizer': hp.choice('optimizer', ['adadelta','adam']),\n",
    "         'wdecay':hp.choice('wdecay', np.linspace(0.0001,0.01,1000)),\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print ('Model Testing:', params)\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1], kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units'], kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(params['hidden3_units'], kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(params['wdecay']))) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden3_dropout']))\n",
    "        \n",
    "        model.add(Dense(1, kernel_initializer='he_normal',kernel_regularizer=l2(params['wdecay'])))\n",
    "        model.compile(loss=mae_score,metrics=[mae_score], optimizer=params['optimizer'])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 100, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
