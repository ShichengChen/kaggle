{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a callback function to be used with training of Keras models.\n",
    "# It create an exponential moving average of a model (trainable) weights.\n",
    "# This functionlity is already available in TensorFlow:\n",
    "# https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#ExponentialMovingAverage\n",
    "# and can often be used to get better validation/test performance. For an\n",
    "# intuitive explantion on why to use this, see 'Model Ensembles\" section here:\n",
    "# http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(Callback):\n",
    "    \"\"\"create a copy of trainable weights which gets updated at every\n",
    "       batch using exponential weight decay. The moving average weights along\n",
    "       with the other states of original model(except original model trainable\n",
    "       weights) will be saved at every epoch if save_mv_ave_model is True.\n",
    "       If both save_mv_ave_model and save_best_only are True, the latest\n",
    "       best moving average model according to the quantity monitored\n",
    "       will not be overwritten. Of course, save_best_only can be True\n",
    "       only if there is a validation set.\n",
    "       This is equivalent to save_best_only mode of ModelCheckpoint\n",
    "       callback with similar code. custom_objects is a dictionary\n",
    "       holding name and Class implementation for custom layers.\n",
    "       At end of every batch, the update is as follows:\n",
    "       mv_weight -= (1 - decay) * (mv_weight - weight)\n",
    "       where weight and mv_weight is the ordinal model weight and the moving\n",
    "       averaged weight respectively. At the end of the training, the moving\n",
    "       averaged weights are transferred to the original model.\n",
    "       \"\"\"\n",
    "    def __init__(self, decay=0.999, filepath='temp_weight.hdf5',\n",
    "                 save_mv_ave_model=True, verbose=0,\n",
    "                 save_best_only=False, monitor='val_loss', mode='auto',\n",
    "                 save_weights_only=False, custom_objects={}):\n",
    "        self.decay = decay\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_mv_ave_model = save_mv_ave_model\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.save_best_only = save_best_only\n",
    "        self.monitor = monitor\n",
    "        self.custom_objects = custom_objects  # dictionary of custom layers\n",
    "        self.sym_trainable_weights = None  # trainable weights of model\n",
    "        self.mv_trainable_weights_vals = None  # moving averaged values\n",
    "        super(ExponentialMovingAverage, self).__init__()\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.sym_trainable_weights = self.model.trainable_weights\n",
    "        # Initialize moving averaged weights using original model values\n",
    "        self.mv_trainable_weights_vals = {x.name: K.get_value(x) for x in\n",
    "                                          self.sym_trainable_weights}\n",
    "        if self.verbose:\n",
    "            print('Created a copy of model weights to initialize moving'\n",
    "                  ' averaged weights.')\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        for weight in self.sym_trainable_weights:\n",
    "            old_val = self.mv_trainable_weights_vals[weight.name]\n",
    "            self.mv_trainable_weights_vals[weight.name] -= \\\n",
    "                (1.0 - self.decay) * (old_val - K.get_value(weight))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"After each epoch, we can optionally save the moving averaged model,\n",
    "        but the weights will NOT be transferred to the original model. This\n",
    "        happens only at the end of training. We also need to transfer state of\n",
    "        original model to model2 as model2 only gets updated trainable weight\n",
    "        at end of each batch and non-trainable weights are not transferred\n",
    "        (for example mean and var for batch normalization layers).\"\"\"\n",
    "        if self.save_mv_ave_model:\n",
    "            filepath = self.filepath.format(epoch=epoch, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best moving averaged model only '\n",
    "                                  'with %s available, skipping.'\n",
    "                                  % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('saving moving average model to %s'\n",
    "                                  % (filepath))\n",
    "                        self.best = current\n",
    "                        model2 = self._make_mv_model(filepath)\n",
    "                        if self.save_weights_only:\n",
    "                            model2.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            model2.save(filepath, overwrite=True)\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving moving average model to %s' % (epoch, filepath))\n",
    "                model2 = self._make_mv_model(filepath)\n",
    "                if self.save_weights_only:\n",
    "                    model2.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    model2.save(filepath, overwrite=True)\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        for weight in self.sym_trainable_weights:\n",
    "            K.set_value(weight, self.mv_trainable_weights_vals[weight.name])\n",
    "\n",
    "    def _make_mv_model(self, filepath):\n",
    "        \"\"\" Create a model with moving averaged weights. Other variables are\n",
    "        the same as original mode. We first save original model to save its\n",
    "        state. Then copy moving averaged weights over.\"\"\"\n",
    "        self.model.save(filepath, overwrite=True)\n",
    "        model2 = load_model(filepath, custom_objects=self.custom_objects)\n",
    "\n",
    "        for w2, w in zip(model2.trainable_weights, self.model.trainable_weights):\n",
    "            K.set_value(w2, self.mv_trainable_weights_vals[w.name])\n",
    "\n",
    "        return model2\n",
    "\n",
    "\n",
    "def batch_generator(X, y=None, batch_size=128, shuffle=False):\n",
    "    index = np.arange(X.shape[0])\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index)\n",
    "\n",
    "        batch_start = 0\n",
    "        while batch_start < X.shape[0]:\n",
    "            batch_index = index[batch_start:batch_start + batch_size]\n",
    "            batch_start += batch_size\n",
    "\n",
    "            X_batch = X[batch_index, :]\n",
    "\n",
    "            if sp.issparse(X_batch):\n",
    "                X_batch = X_batch.toarray()\n",
    "\n",
    "            if y is None:\n",
    "                yield X_batch\n",
    "            else:\n",
    "                yield X_batch, y[batch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1153)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "cat_names = [c for c in train.columns if 'cat' in c]\n",
    "\n",
    "train = pd.get_dummies(data=train, columns=cat_names)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['id','loss']]\n",
    "\n",
    "train_x = np.array(train[features])\n",
    "\n",
    "ntrain = train_x.shape[0]\n",
    "\n",
    "# np.log(train['loss'] + 200) provides\n",
    "# a better score, but let's keep it simple now\n",
    "train_y = np.array(train['loss'])\n",
    "\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cdn.rawgit.com/dnkirill/allstate_capstone/master/images/mlp3.svg\"></td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=10):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=30, verbose=1, callbacks=[ExponentialMovingAverage(save_mv_ave_model=False)])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "def hyper_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(437, input_dim=train_x.shape[1], init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.536))\n",
    "    \n",
    "    model.add(Dense(182, init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(73, init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.233))\n",
    "    \n",
    "    model.add(Dense(1, init='he_normal',W_regularizer=l2(0.002)))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(437, input_dim=1153, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  \"\"\"\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(182, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(73, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"he_normal\", kernel_regularizer=<keras.reg...)`\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 2167.0100 - val_loss: 1212.0037\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1360.8910 - val_loss: 1183.5801\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1333.6907 - val_loss: 1164.5651\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1322.8107 - val_loss: 1156.8181\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1309.4708 - val_loss: 1157.0989\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1301.3325 - val_loss: 1150.9495\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1296.9288 - val_loss: 1152.1262\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1291.9648 - val_loss: 1149.0049\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1280.1525 - val_loss: 1149.9305\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1278.2210 - val_loss: 1147.2588\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1271.8310 - val_loss: 1144.6121\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1262.5508 - val_loss: 1143.0170\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1258.6420 - val_loss: 1141.7921\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1253.8949 - val_loss: 1142.8058\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1247.6313 - val_loss: 1141.6390\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1245.9400 - val_loss: 1143.9284\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1239.7167 - val_loss: 1143.0127\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1234.0026 - val_loss: 1138.8894\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 129us/step - loss: 1229.9493 - val_loss: 1139.4701\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1224.3423 - val_loss: 1138.9610\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1220.2235 - val_loss: 1139.0151\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1215.1935 - val_loss: 1139.4911\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1213.9630 - val_loss: 1137.5117\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1207.2895 - val_loss: 1137.6611\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 18s 129us/step - loss: 1205.6138 - val_loss: 1137.7827\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1199.7973 - val_loss: 1137.0102\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 129us/step - loss: 1194.7466 - val_loss: 1138.9971\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1193.9729 - val_loss: 1140.5606\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1188.4866 - val_loss: 1139.9172\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1183.5390 - val_loss: 1137.3822\n",
      "Fold 0, MAE: 1138.4274601724862\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 2014.0466 - val_loss: 1197.9543\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1330.2535 - val_loss: 1176.0142\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1314.0003 - val_loss: 1168.9073\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1302.9117 - val_loss: 1166.8811\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1299.6341 - val_loss: 1160.4019\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1292.9641 - val_loss: 1156.1191\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1285.7682 - val_loss: 1155.6768\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1281.3461 - val_loss: 1154.1451\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1272.8027 - val_loss: 1153.3748\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1271.7417 - val_loss: 1152.1768\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1263.4062 - val_loss: 1150.2138\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1258.4268 - val_loss: 1148.6555\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1253.6728 - val_loss: 1147.5438\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1252.7736 - val_loss: 1146.2580\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1243.8572 - val_loss: 1145.2604\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1242.2751 - val_loss: 1144.8543\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1237.0821 - val_loss: 1142.9158\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1231.7844 - val_loss: 1144.5207\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1229.2321 - val_loss: 1142.9169\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1224.9435 - val_loss: 1141.2335\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1222.2604 - val_loss: 1145.7249\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1218.1572 - val_loss: 1142.4500\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1214.1846 - val_loss: 1142.4351\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1205.1486 - val_loss: 1142.2297\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1205.8896 - val_loss: 1140.3327\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1198.6364 - val_loss: 1142.9886\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1197.8703 - val_loss: 1140.5772\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1190.6730 - val_loss: 1144.2675\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1189.4736 - val_loss: 1140.5032\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1189.5293 - val_loss: 1143.4036\n",
      "Fold 1, MAE: 1161.068376084087\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 2289.1612 - val_loss: 1240.2243\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1348.5332 - val_loss: 1175.1741\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1326.1976 - val_loss: 1165.6695\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1313.7651 - val_loss: 1169.0622\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1305.6440 - val_loss: 1158.7413\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1298.6105 - val_loss: 1157.3183\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1290.8597 - val_loss: 1153.6828\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1284.7015 - val_loss: 1150.4986\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1283.8839 - val_loss: 1147.3532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1271.2397 - val_loss: 1148.9495\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1268.0408 - val_loss: 1146.9558\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 136us/step - loss: 1263.0347 - val_loss: 1147.2024\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1255.2314 - val_loss: 1144.7743\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1250.4589 - val_loss: 1144.1285\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1249.6973 - val_loss: 1143.0931\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1240.0072 - val_loss: 1142.0111\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1235.7677 - val_loss: 1141.8242\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1234.8782 - val_loss: 1141.2443\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1226.6400 - val_loss: 1140.8926\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1223.5565 - val_loss: 1141.0393\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1216.8925 - val_loss: 1140.8163\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1215.8931 - val_loss: 1140.8035\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1215.1614 - val_loss: 1140.5563\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1211.9412 - val_loss: 1140.1254\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1203.6717 - val_loss: 1142.7072\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1199.5353 - val_loss: 1139.0441\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1200.1102 - val_loss: 1141.9845\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1192.7067 - val_loss: 1139.9827\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1187.7176 - val_loss: 1140.2402\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1187.2181 - val_loss: 1141.1464\n",
      "Fold 2, MAE: 1140.3706276719086\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 2144.2611 - val_loss: 1192.2345\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1400.0017 - val_loss: 1179.0850\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1367.2249 - val_loss: 1170.3284\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1352.4193 - val_loss: 1156.6614\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1346.2932 - val_loss: 1161.1607\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1333.0410 - val_loss: 1150.8521\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1329.0416 - val_loss: 1149.4683\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1313.4493 - val_loss: 1144.8523\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1302.6887 - val_loss: 1145.2037\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1301.3788 - val_loss: 1144.8801\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1292.9715 - val_loss: 1147.4730\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1286.0759 - val_loss: 1141.1973\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1277.4361 - val_loss: 1138.2450\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1272.0589 - val_loss: 1139.0522\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1267.0788 - val_loss: 1139.2886\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1256.6019 - val_loss: 1137.7023\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1254.6115 - val_loss: 1138.4588\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1244.5292 - val_loss: 1135.1024\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1240.9914 - val_loss: 1135.0995\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1235.6625 - val_loss: 1136.1200\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1229.4193 - val_loss: 1134.5950\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1226.0053 - val_loss: 1135.8462\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1219.1323 - val_loss: 1136.4517\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1215.4190 - val_loss: 1134.0215\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1213.5242 - val_loss: 1135.5425\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1209.3883 - val_loss: 1133.2958\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1200.3775 - val_loss: 1135.1051\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1196.3233 - val_loss: 1133.8332\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1193.1043 - val_loss: 1133.0191\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1187.1562 - val_loss: 1134.0104\n",
      "Fold 3, MAE: 1147.4858157405674\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1973.4163 - val_loss: 1182.2336\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1296.1351 - val_loss: 1164.2440\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1282.5997 - val_loss: 1160.2976\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1276.5806 - val_loss: 1157.6038\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1266.8405 - val_loss: 1152.6267\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1265.6350 - val_loss: 1151.7481\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1261.1854 - val_loss: 1150.6228\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1255.6338 - val_loss: 1148.4209\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1252.5442 - val_loss: 1145.2298\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1248.8103 - val_loss: 1144.4613\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1243.1014 - val_loss: 1142.6844\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1241.6290 - val_loss: 1142.9264\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1235.3432 - val_loss: 1143.0119\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1229.5066 - val_loss: 1143.0537\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1226.7427 - val_loss: 1144.2760\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1229.3703 - val_loss: 1141.5266\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1220.5435 - val_loss: 1139.1240\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1221.2418 - val_loss: 1138.8950\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 19s 141us/step - loss: 1217.1953 - val_loss: 1137.9409\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1213.8227 - val_loss: 1138.5104\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1209.3225 - val_loss: 1138.5747\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1203.1665 - val_loss: 1141.1783\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1206.2553 - val_loss: 1139.0098\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1201.3716 - val_loss: 1138.2706\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1198.7591 - val_loss: 1139.2440\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1191.3677 - val_loss: 1139.1006\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1189.0438 - val_loss: 1139.6527\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1188.3124 - val_loss: 1137.8226\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1183.4833 - val_loss: 1137.1620\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1182.9349 - val_loss: 1137.4927\n",
      "Fold 4, MAE: 1138.385635362105\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 2146.1925 - val_loss: 1208.2313\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1383.0523 - val_loss: 1176.1931\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1353.7167 - val_loss: 1168.6311\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1341.9618 - val_loss: 1163.2262\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1334.1859 - val_loss: 1157.6628\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1322.7169 - val_loss: 1155.8194\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1317.0486 - val_loss: 1154.0400\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1312.2938 - val_loss: 1151.1541\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1298.9313 - val_loss: 1151.2943\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1292.9488 - val_loss: 1147.1517\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1288.4639 - val_loss: 1148.1221\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1279.6468 - val_loss: 1144.3419\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1276.0559 - val_loss: 1145.9306\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1267.1822 - val_loss: 1145.0548\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1264.7437 - val_loss: 1145.6965\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1258.2122 - val_loss: 1144.6449\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1247.7081 - val_loss: 1141.1719\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1244.2054 - val_loss: 1141.2541\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1241.7903 - val_loss: 1141.9055\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1235.7225 - val_loss: 1141.9880\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1232.7522 - val_loss: 1140.4740\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1223.9206 - val_loss: 1140.7734\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1217.7669 - val_loss: 1145.4831\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1212.9081 - val_loss: 1140.4926\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1212.0763 - val_loss: 1141.9736\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1206.9395 - val_loss: 1140.2561\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1202.8094 - val_loss: 1142.0128\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1201.0557 - val_loss: 1142.0280\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1194.6833 - val_loss: 1141.4093\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1186.6682 - val_loss: 1140.3333\n",
      "Fold 5, MAE: 1147.6812917612444\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 2224.9476 - val_loss: 1228.5881\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1464.4378 - val_loss: 1202.2232\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1385.3156 - val_loss: 1181.1141\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1371.4360 - val_loss: 1168.2648\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1357.7245 - val_loss: 1163.0364\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1342.2410 - val_loss: 1154.7615\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1325.3850 - val_loss: 1156.1663\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1318.0467 - val_loss: 1153.8613\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1308.5392 - val_loss: 1149.9744\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1299.9526 - val_loss: 1149.7810\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1289.7794 - val_loss: 1146.9095\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1286.4959 - val_loss: 1148.5083\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1276.4766 - val_loss: 1145.3068\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1269.3414 - val_loss: 1144.9921\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1257.5564 - val_loss: 1146.6413\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1255.1385 - val_loss: 1141.7447\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1244.8428 - val_loss: 1142.9728\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1240.0259 - val_loss: 1140.2051\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1231.8727 - val_loss: 1140.8432\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1230.5945 - val_loss: 1139.9403\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1224.0839 - val_loss: 1140.0073\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1216.3151 - val_loss: 1138.6751\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1216.0512 - val_loss: 1140.7886\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1208.0295 - val_loss: 1138.5203\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1203.9200 - val_loss: 1140.0231\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1197.4256 - val_loss: 1140.1516\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1193.2738 - val_loss: 1138.5939\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 19s 142us/step - loss: 1190.5755 - val_loss: 1139.0932\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1186.1293 - val_loss: 1138.0854\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1182.4782 - val_loss: 1138.7171\n",
      "Fold 6, MAE: 1138.8995213783282\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 2195.4001 - val_loss: 1222.9675\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1348.9641 - val_loss: 1180.9750\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1330.3625 - val_loss: 1172.3513\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1316.8404 - val_loss: 1157.2486\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1309.1679 - val_loss: 1157.2928\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1301.9643 - val_loss: 1151.7404\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1292.1042 - val_loss: 1151.1422\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1287.2833 - val_loss: 1147.2109\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1278.4701 - val_loss: 1147.1643\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1276.2527 - val_loss: 1144.4961\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1269.3295 - val_loss: 1144.3173\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1264.7407 - val_loss: 1140.0244\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1257.9463 - val_loss: 1144.3176\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1256.4110 - val_loss: 1141.7673\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1247.1978 - val_loss: 1138.9148\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1248.3492 - val_loss: 1139.8635\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1241.3958 - val_loss: 1137.9412\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1238.0156 - val_loss: 1138.2774\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1233.2031 - val_loss: 1137.2177\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1227.2867 - val_loss: 1135.1028\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1223.0248 - val_loss: 1137.4901\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1221.1362 - val_loss: 1136.3933\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1216.9978 - val_loss: 1136.1563\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1211.6949 - val_loss: 1136.4052\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1208.1765 - val_loss: 1135.3656\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1202.2069 - val_loss: 1138.2343\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1200.3769 - val_loss: 1136.9709\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1197.9587 - val_loss: 1138.6044\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1192.7485 - val_loss: 1136.7667\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1191.8173 - val_loss: 1135.7671\n",
      "Fold 7, MAE: 1140.1412696450752\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 2352.0180 - val_loss: 1260.6895\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 20s 146us/step - loss: 1352.2867 - val_loss: 1189.0343\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1320.4867 - val_loss: 1177.0543\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 20s 149us/step - loss: 1307.5614 - val_loss: 1162.2866\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 1299.9480 - val_loss: 1162.0216\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 20s 144us/step - loss: 1294.7846 - val_loss: 1154.5788\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 1283.5902 - val_loss: 1153.9726\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 20s 144us/step - loss: 1277.0893 - val_loss: 1154.3866\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 1270.4072 - val_loss: 1150.3769\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 19s 144us/step - loss: 1265.0631 - val_loss: 1150.5322\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 19s 144us/step - loss: 1259.1403 - val_loss: 1150.4150\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 20s 144us/step - loss: 1251.4840 - val_loss: 1148.0553\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 1249.6258 - val_loss: 1144.6987\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 20s 146us/step - loss: 1241.8437 - val_loss: 1146.2684\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 1239.5443 - val_loss: 1144.8813\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 20s 146us/step - loss: 1234.1509 - val_loss: 1144.9558\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 20s 146us/step - loss: 1230.2226 - val_loss: 1143.2016\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 19s 143us/step - loss: 1230.5191 - val_loss: 1143.8773\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 20s 146us/step - loss: 1223.1794 - val_loss: 1144.9206\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 20s 145us/step - loss: 1214.0690 - val_loss: 1142.0747\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1212.7420 - val_loss: 1141.1571\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1211.5242 - val_loss: 1144.2699\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1207.9039 - val_loss: 1139.7774\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1202.8304 - val_loss: 1140.3526\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 20s 149us/step - loss: 1202.0053 - val_loss: 1139.6036\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1197.2578 - val_loss: 1143.4643\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1193.5382 - val_loss: 1143.1771\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1185.9683 - val_loss: 1143.0515\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1184.5614 - val_loss: 1143.4280\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1181.2386 - val_loss: 1143.1609\n",
      "Fold 8, MAE: 1127.3331638537663\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 21s 153us/step - loss: 2217.0324 - val_loss: 1247.3760\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 20s 151us/step - loss: 1461.9761 - val_loss: 1192.6782\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 20s 149us/step - loss: 1409.3910 - val_loss: 1181.7366\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1389.7981 - val_loss: 1169.1824\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1371.0142 - val_loss: 1167.5732\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 21s 152us/step - loss: 1356.8840 - val_loss: 1164.3111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 20s 151us/step - loss: 1351.7093 - val_loss: 1160.1289\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1338.7285 - val_loss: 1155.9709\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1330.4124 - val_loss: 1156.8836\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1320.3980 - val_loss: 1156.9649\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 21s 152us/step - loss: 1310.6417 - val_loss: 1151.5215\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1296.6134 - val_loss: 1150.1943\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1289.2152 - val_loss: 1151.0073\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1282.8837 - val_loss: 1152.2900\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 21s 153us/step - loss: 1280.9225 - val_loss: 1147.0201\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 20s 151us/step - loss: 1269.0510 - val_loss: 1148.9092\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1265.2599 - val_loss: 1149.4693\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1256.3878 - val_loss: 1144.8069\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1248.2053 - val_loss: 1148.0631\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 20s 149us/step - loss: 1242.1686 - val_loss: 1148.4180\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 20s 148us/step - loss: 1232.5336 - val_loss: 1146.6616\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 20s 147us/step - loss: 1229.4416 - val_loss: 1147.6042\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 21s 151us/step - loss: 1226.8623 - val_loss: 1146.7529\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 20s 149us/step - loss: 1220.2226 - val_loss: 1145.3044\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1211.6056 - val_loss: 1143.1802\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 20s 146us/step - loss: 1206.6782 - val_loss: 1145.2594\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 20s 150us/step - loss: 1204.0878 - val_loss: 1145.3479\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 20s 149us/step - loss: 1197.0147 - val_loss: 1144.5518\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 21s 152us/step - loss: 1195.5985 - val_loss: 1144.5594\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 21s 153us/step - loss: 1191.9685 - val_loss: 1144.4051\n",
      "Fold 9, MAE: 1138.9470892691488\n",
      "10-fold CV score: 1141.8740250938713\n",
      "CV score for the final model: 1141.87402509\n"
     ]
    }
   ],
   "source": [
    "cv_score = cross_validate_mlp(hyper_model)\n",
    "print (\"CV score for the final model:\", cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, \n",
    " 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta',\n",
    " 'wdecay': 0.0020300000000000001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'l3-nn': {\n",
    "        'predictions': l2_predictions,\n",
    "        'n_bags': 4,\n",
    "        'model': Keras(nn_lr, lambda: {'l1': 1e-5, 'l2': 1e-5, 'n_epoch': 30, 'batch_size': 128, 'optimizer': SGD(3e-2, momentum=0.8, nesterov=True, decay=3e-5), 'callbacks': [ExponentialMovingAverage(save_mv_ave_model=False)]}),\n",
    "    },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=30, verbose=0, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 455, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 272, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 69, 'optimizer': 'adadelta', 'wdecay': 0.032330000000000005}\n",
      "Fold 0, MAE: 1163.148839039124\n",
      "Fold 1, MAE: 1159.9899238609737\n",
      "Fold 2, MAE: 1140.9771315408389\n",
      "3-fold CV score: 1154.7052981469787\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 541, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 134, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 30, 'optimizer': 'adadelta', 'wdecay': 0.082830000000000001}\n",
      "Fold 0, MAE: 1181.2090776051534\n",
      "Fold 1, MAE: 1175.7242010975745\n",
      "Fold 2, MAE: 1149.1617817128606\n",
      "3-fold CV score: 1168.698353471863\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 541, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 113, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 22, 'optimizer': 'adam', 'wdecay': 0.010110000000000001}\n",
      "Fold 0, MAE: 1161.0922305512295\n",
      "Fold 1, MAE: 1171.8751691438747\n",
      "Fold 2, MAE: 1142.5130553590734\n",
      "3-fold CV score: 1158.4934850180591\n",
      "Model Testing: {'hidden1_dropout': 0.56842105263157894, 'hidden1_units': 351, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 168, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 44, 'optimizer': 'adam', 'wdecay': 0.066669999999999993}\n",
      "Fold 0, MAE: 1175.3901692187044\n",
      "Fold 1, MAE: 1176.3282762184622\n",
      "Fold 2, MAE: 1171.493253623609\n",
      "3-fold CV score: 1174.4038996869251\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 351, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 237, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.0040499999999999998}\n",
      "Fold 0, MAE: 1167.663476455973\n",
      "Fold 1, MAE: 1158.721241361386\n",
      "Fold 2, MAE: 1148.2196995976833\n",
      "3-fold CV score: 1158.2014724716807\n",
      "Model Testing: {'hidden1_dropout': 0.45263157894736844, 'hidden1_units': 437, 'hidden2_dropout': 0.5, 'hidden2_units': 120, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 30, 'optimizer': 'adam', 'wdecay': 0.055560000000000005}\n",
      "Fold 0, MAE: 1192.865180594892\n",
      "Fold 1, MAE: 1170.481112811505\n",
      "Fold 2, MAE: 1171.0399158641085\n",
      "3-fold CV score: 1178.128736423502\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 368, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 237, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 34, 'optimizer': 'adadelta', 'wdecay': 0.082830000000000001}\n",
      "Fold 0, MAE: 1163.6607907634843\n",
      "Fold 1, MAE: 1170.6899247157944\n",
      "Fold 2, MAE: 1149.6886504229296\n",
      "3-fold CV score: 1161.3464553007361\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 394, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 217, 'hidden3_dropout': 0.5, 'hidden3_units': 26, 'optimizer': 'adadelta', 'wdecay': 0.086870000000000003}\n",
      "Fold 0, MAE: 1186.010471694171\n",
      "Fold 1, MAE: 1194.6887222751257\n",
      "Fold 2, MAE: 1178.986956053622\n",
      "3-fold CV score: 1186.5620500076395\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 550, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 175, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 30, 'optimizer': 'adadelta', 'wdecay': 0.054550000000000008}\n",
      "Fold 0, MAE: 1173.4874602173836\n",
      "Fold 1, MAE: 1168.5583186899196\n",
      "Fold 2, MAE: 1157.0938108456683\n",
      "3-fold CV score: 1166.3798632509906\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 420, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 272, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 20, 'optimizer': 'adam', 'wdecay': 0.051520000000000003}\n",
      "Fold 0, MAE: 1187.3547460026052\n",
      "Fold 1, MAE: 1165.0640505257604\n",
      "Fold 2, MAE: 1152.0578003723883\n",
      "3-fold CV score: 1168.1588656335846\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 506, 'hidden2_dropout': 0.5, 'hidden2_units': 155, 'hidden3_dropout': 0.5, 'hidden3_units': 38, 'optimizer': 'adam', 'wdecay': 0.0030400000000000002}\n",
      "Fold 0, MAE: 1165.5917111922117\n",
      "Fold 1, MAE: 1157.0224820727203\n",
      "Fold 2, MAE: 1145.9990145378135\n",
      "3-fold CV score: 1156.2044026009153\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 550, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 217, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 46, 'optimizer': 'adadelta', 'wdecay': 0.053540000000000004}\n",
      "Fold 0, MAE: 1178.285182548254\n",
      "Fold 1, MAE: 1161.227306544953\n",
      "Fold 2, MAE: 1146.3369486137735\n",
      "3-fold CV score: 1161.9498125689936\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 308, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 100, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 48, 'optimizer': 'adadelta', 'wdecay': 0.083839999999999998}\n",
      "Fold 0, MAE: 1175.8002146167726\n",
      "Fold 1, MAE: 1172.0227174765078\n",
      "Fold 2, MAE: 1154.2503788092345\n",
      "3-fold CV score: 1167.3577703008384\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 446, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 217, 'hidden3_dropout': 0.5, 'hidden3_units': 22, 'optimizer': 'adadelta', 'wdecay': 0.089899999999999994}\n",
      "Fold 0, MAE: 1198.0498968117809\n",
      "Fold 1, MAE: 1192.423294968826\n",
      "Fold 2, MAE: 1191.1537727940815\n",
      "3-fold CV score: 1193.8756548582294\n",
      "Model Testing: {'hidden1_dropout': 0.40000000000000002, 'hidden1_units': 541, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 258, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 75, 'optimizer': 'adadelta', 'wdecay': 0.02324}\n",
      "Fold 0, MAE: 1162.2846737751456\n",
      "Fold 1, MAE: 1157.0434973514984\n",
      "Fold 2, MAE: 1149.8406610896955\n",
      "3-fold CV score: 1156.3896107387798\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 412, 'hidden2_dropout': 0.5, 'hidden2_units': 203, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 24, 'optimizer': 'adadelta', 'wdecay': 0.065659999999999996}\n",
      "Fold 0, MAE: 1162.6868081304262\n",
      "Fold 1, MAE: 1207.019499323167\n",
      "Fold 2, MAE: 1164.6480082339472\n",
      "3-fold CV score: 1178.1181052291802\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 386, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 113, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 44, 'optimizer': 'adam', 'wdecay': 0.01213}\n",
      "Fold 0, MAE: 1162.20426789193\n",
      "Fold 1, MAE: 1165.871551874401\n",
      "Fold 2, MAE: 1144.635762708466\n",
      "3-fold CV score: 1157.570527491599\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 489, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 272, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 42, 'optimizer': 'adadelta', 'wdecay': 0.047480000000000008}\n",
      "Fold 0, MAE: 1159.1230678129568\n",
      "Fold 1, MAE: 1170.1456261211185\n",
      "Fold 2, MAE: 1141.1387421620404\n",
      "3-fold CV score: 1156.8024786987053\n",
      "Model Testing: {'hidden1_dropout': 0.47368421052631582, 'hidden1_units': 368, 'hidden2_dropout': 0.5, 'hidden2_units': 196, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 61, 'optimizer': 'adam', 'wdecay': 0.076770000000000005}\n",
      "Fold 0, MAE: 1167.3144952524326\n",
      "Fold 1, MAE: 1158.442539164428\n",
      "Fold 2, MAE: 1170.1439974149687\n",
      "3-fold CV score: 1165.3003439439433\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 325, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.070709999999999995}\n",
      "Fold 0, MAE: 1161.177618771013\n",
      "Fold 1, MAE: 1157.4865109404855\n",
      "Fold 2, MAE: 1143.5037899814306\n",
      "3-fold CV score: 1154.0559732309764\n",
      "Model Testing: {'hidden1_dropout': 0.58947368421052637, 'hidden1_units': 325, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 148, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 69, 'optimizer': 'adadelta', 'wdecay': 0.070709999999999995}\n",
      "Fold 0, MAE: 1169.0057906375498\n",
      "Fold 1, MAE: 1163.4061472146307\n",
      "Fold 2, MAE: 1150.80821222944\n",
      "3-fold CV score: 1161.0733833605402\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 455, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.032330000000000005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, MAE: 1160.1148852933434\n",
      "Fold 1, MAE: 1153.5388360438872\n",
      "Fold 2, MAE: 1146.284142179796\n",
      "3-fold CV score: 1153.312621172342\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 325, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.058590000000000003}\n",
      "Fold 0, MAE: 1159.533359263806\n",
      "Fold 1, MAE: 1154.5401700530679\n",
      "Fold 2, MAE: 1152.34695090863\n",
      "3-fold CV score: 1155.473493408501\n",
      "Model Testing: {'hidden1_dropout': 0.43157894736842106, 'hidden1_units': 377, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 189, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.081820000000000004}\n",
      "Fold 0, MAE: 1162.9307342647628\n",
      "Fold 1, MAE: 1177.8290937843656\n",
      "Fold 2, MAE: 1148.0584482485735\n",
      "3-fold CV score: 1162.9394254325673\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 455, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 162, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.091920000000000002}\n",
      "Fold 0, MAE: 1161.4301880915023\n",
      "Fold 1, MAE: 1169.6419220691284\n",
      "Fold 2, MAE: 1144.48012878803\n",
      "3-fold CV score: 1158.5174129828868\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 472, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 279, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 77, 'optimizer': 'adadelta', 'wdecay': 0.069699999999999998}\n",
      "Fold 0, MAE: 1158.2613808574185\n",
      "Fold 1, MAE: 1156.7999195958928\n",
      "Fold 2, MAE: 1153.1107720208292\n",
      "3-fold CV score: 1156.0573574913803\n",
      "Model Testing: {'hidden1_dropout': 0.4631578947368421, 'hidden1_units': 524, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 265, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 40, 'optimizer': 'adadelta', 'wdecay': 0.094950000000000007}\n",
      "Fold 0, MAE: 1195.4596557820985\n",
      "Fold 1, MAE: 1166.5828679966726\n",
      "Fold 2, MAE: 1151.5350493940605\n",
      "3-fold CV score: 1171.1925243909438\n",
      "Model Testing: {'hidden1_dropout': 0.51578947368421058, 'hidden1_units': 498, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 231, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.08788}\n",
      "Fold 0, MAE: 1166.894567545835\n",
      "Fold 1, MAE: 1158.6302274524644\n",
      "Fold 2, MAE: 1146.4025209941412\n",
      "3-fold CV score: 1157.3091053308135\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 334, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 106, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.035360000000000003}\n",
      "Fold 0, MAE: 1156.7171541562705\n",
      "Fold 1, MAE: 1158.617184146834\n",
      "Fold 2, MAE: 1140.9087310499317\n",
      "3-fold CV score: 1152.0810231176786\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 515, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 286, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 71, 'optimizer': 'adadelta', 'wdecay': 0.035360000000000003}\n",
      "Fold 0, MAE: 1166.3465921313127\n",
      "Fold 1, MAE: 1155.4647161447238\n",
      "Fold 2, MAE: 1145.8189915304222\n",
      "3-fold CV score: 1155.8767666021529\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 334, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 106, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 65, 'optimizer': 'adadelta', 'wdecay': 0.052530000000000007}\n",
      "Fold 0, MAE: 1165.2972734181083\n",
      "Fold 1, MAE: 1162.3415273140827\n",
      "Fold 2, MAE: 1143.2022154553692\n",
      "3-fold CV score: 1156.9470053958535\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.00809}\n",
      "Fold 0, MAE: 1159.2449543552357\n",
      "Fold 1, MAE: 1154.8565114282767\n",
      "Fold 2, MAE: 1140.933696843291\n",
      "3-fold CV score: 1151.6783875422677\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 334, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.014149999999999999}\n",
      "Fold 0, MAE: 1165.0503812670863\n",
      "Fold 1, MAE: 1157.1801844593206\n",
      "Fold 2, MAE: 1141.5077789837922\n",
      "3-fold CV score: 1154.579448236733\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.00809}\n",
      "Fold 0, MAE: 1162.7622893053533\n",
      "Fold 1, MAE: 1156.484460623302\n",
      "Fold 2, MAE: 1141.2796353609326\n",
      "3-fold CV score: 1153.5087950965292\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 141, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 53, 'optimizer': 'adam', 'wdecay': 0.025260000000000001}\n",
      "Fold 0, MAE: 1173.398777121047\n",
      "Fold 1, MAE: 1163.4376312509926\n",
      "Fold 2, MAE: 1149.4355696684008\n",
      "3-fold CV score: 1162.0906593468135\n",
      "Model Testing: {'hidden1_dropout': 0.56842105263157894, 'hidden1_units': 343, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 224, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 57, 'optimizer': 'adadelta', 'wdecay': 0.01617}\n",
      "Fold 0, MAE: 1157.8129235479112\n",
      "Fold 1, MAE: 1162.9604955281113\n",
      "Fold 2, MAE: 1137.8848549871118\n",
      "3-fold CV score: 1152.886091354378\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 463, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 210, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 67, 'optimizer': 'adam', 'wdecay': 0.028289999999999999}\n",
      "Fold 0, MAE: 1167.1458696599193\n",
      "Fold 1, MAE: 1160.2728405505222\n",
      "Fold 2, MAE: 1154.656251596023\n",
      "3-fold CV score: 1160.6916539354881\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 360, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 300, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 32, 'optimizer': 'adadelta', 'wdecay': 0.071720000000000006}\n",
      "Fold 0, MAE: 1157.0820389802964\n",
      "Fold 1, MAE: 1173.8628986425185\n",
      "Fold 2, MAE: 1147.7328339035314\n",
      "3-fold CV score: 1159.5592571754487\n",
      "Model Testing: {'hidden1_dropout': 0.45263157894736844, 'hidden1_units': 317, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 293, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 36, 'optimizer': 'adadelta', 'wdecay': 0.027280000000000002}\n",
      "Fold 0, MAE: 1162.285383334579\n",
      "Fold 1, MAE: 1160.938020819047\n",
      "Fold 2, MAE: 1145.0485755819968\n",
      "3-fold CV score: 1156.0906599118741\n",
      "Model Testing: {'hidden1_dropout': 0.51578947368421058, 'hidden1_units': 429, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 244, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 80, 'optimizer': 'adam', 'wdecay': 0.097979999999999998}\n",
      "Fold 0, MAE: 1187.3501504499832\n",
      "Fold 1, MAE: 1177.8064867953847\n",
      "Fold 2, MAE: 1153.1573311127709\n",
      "3-fold CV score: 1172.7713227860463\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1157.9619118652226\n",
      "Fold 1, MAE: 1155.7216872165664\n",
      "Fold 2, MAE: 1136.927755550967\n",
      "3-fold CV score: 1150.2037848775853\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1156.0632860191683\n",
      "Fold 1, MAE: 1151.4671627447715\n",
      "Fold 2, MAE: 1140.1617350157348\n",
      "3-fold CV score: 1149.230727926558\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adam', 'wdecay': 0.059600000000000007}\n",
      "Fold 0, MAE: 1168.5160545404665\n",
      "Fold 1, MAE: 1175.4194884359572\n",
      "Fold 2, MAE: 1152.5397784139907\n",
      "3-fold CV score: 1165.4917737968046\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.062630000000000005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, MAE: 1163.881212321844\n",
      "Fold 1, MAE: 1164.1301011722203\n",
      "Fold 2, MAE: 1150.2623383121809\n",
      "3-fold CV score: 1159.4245506020818\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 532, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 251, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adadelta', 'wdecay': 0.074749999999999997}\n",
      "Fold 0, MAE: 1161.926003949864\n",
      "Fold 1, MAE: 1171.028653969578\n",
      "Fold 2, MAE: 1147.243582406554\n",
      "3-fold CV score: 1160.0660801086653\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 127, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adam', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1157.4905136832626\n",
      "Fold 1, MAE: 1157.7959576055189\n",
      "Fold 2, MAE: 1145.165175664648\n",
      "3-fold CV score: 1153.4838823178097\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1154.1358029742491\n",
      "Fold 1, MAE: 1150.8801407907292\n",
      "Fold 2, MAE: 1136.143128436801\n",
      "3-fold CV score: 1147.0530240672597\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 134, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.088889999999999997}\n",
      "Fold 0, MAE: 1174.4271644173484\n",
      "Fold 1, MAE: 1160.2759364815581\n",
      "Fold 2, MAE: 1143.464337864193\n",
      "3-fold CV score: 1159.3891462543666\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 403, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 120, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 26, 'optimizer': 'adadelta', 'wdecay': 0.079799999999999996}\n",
      "Fold 0, MAE: 1177.3892872402876\n",
      "Fold 1, MAE: 1162.1597504698693\n",
      "Fold 2, MAE: 1156.8215401050213\n",
      "3-fold CV score: 1165.456859271726\n",
      "Model Testing: {'hidden1_dropout': 0.40000000000000002, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 168, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 73, 'optimizer': 'adam', 'wdecay': 0.072730000000000003}\n",
      "Fold 0, MAE: 1210.498220541357\n",
      "Fold 1, MAE: 1184.6350615099377\n",
      "Fold 2, MAE: 1151.241476172358\n",
      "3-fold CV score: 1182.1249194078841\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 351, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 182, 'hidden3_dropout': 0.5, 'hidden3_units': 20, 'optimizer': 'adadelta', 'wdecay': 0.0293}\n",
      "Fold 0, MAE: 1171.7247792783894\n",
      "Fold 1, MAE: 1176.3316548287394\n",
      "Fold 2, MAE: 1154.8692943033595\n",
      "3-fold CV score: 1167.6419094701625\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 394, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 155, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 34, 'optimizer': 'adadelta', 'wdecay': 0.01516}\n",
      "Fold 0, MAE: 1168.154443232974\n",
      "Fold 1, MAE: 1157.7412081981763\n",
      "Fold 2, MAE: 1141.040754590281\n",
      "3-fold CV score: 1155.6454686738105\n",
      "Model Testing: {'hidden1_dropout': 0.47368421052631582, 'hidden1_units': 420, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 237, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.046470000000000004}\n",
      "Fold 0, MAE: 1164.3777441806067\n",
      "Fold 1, MAE: 1177.209987984398\n",
      "Fold 2, MAE: 1155.6756505514757\n",
      "3-fold CV score: 1165.7544609054933\n",
      "Model Testing: {'hidden1_dropout': 0.58947368421052637, 'hidden1_units': 437, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 175, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 38, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1156.644832466085\n",
      "Fold 1, MAE: 1157.6123849948424\n",
      "Fold 2, MAE: 1137.0233535830582\n",
      "3-fold CV score: 1150.4268570146621\n",
      "Model Testing: {'hidden1_dropout': 0.43157894736842106, 'hidden1_units': 506, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 100, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 46, 'optimizer': 'adadelta', 'wdecay': 0.018190000000000001}\n",
      "Fold 0, MAE: 1156.7325890863394\n",
      "Fold 1, MAE: 1163.3614326980687\n",
      "Fold 2, MAE: 1140.6892231279783\n",
      "3-fold CV score: 1153.5944149707955\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 308, 'hidden2_dropout': 0.5, 'hidden2_units': 203, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 48, 'optimizer': 'adadelta', 'wdecay': 0.067680000000000004}\n",
      "Fold 0, MAE: 1187.777674460418\n",
      "Fold 1, MAE: 1164.7113053835535\n",
      "Fold 2, MAE: 1148.0043875643821\n",
      "3-fold CV score: 1166.8311224694514\n",
      "Model Testing: {'hidden1_dropout': 0.4631578947368421, 'hidden1_units': 386, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.080810000000000007}\n",
      "Fold 0, MAE: 1165.0194917910676\n",
      "Fold 1, MAE: 1158.2729473606164\n",
      "Fold 2, MAE: 1152.9800377245338\n",
      "3-fold CV score: 1158.7574922920726\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 412, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 196, 'hidden3_dropout': 0.5, 'hidden3_units': 75, 'optimizer': 'adam', 'wdecay': 0.038390000000000007}\n",
      "Fold 0, MAE: 1161.5217347156527\n",
      "Fold 1, MAE: 1162.386833252451\n",
      "Fold 2, MAE: 1145.6365952047393\n",
      "3-fold CV score: 1156.5150543909476\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 446, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 148, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.096970000000000001}\n",
      "Fold 0, MAE: 1160.2965376990726\n",
      "Fold 1, MAE: 1168.4996048508942\n",
      "Fold 2, MAE: 1149.110823150734\n",
      "3-fold CV score: 1159.3023219002337\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 489, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 113, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 42, 'optimizer': 'adadelta', 'wdecay': 0.060610000000000004}\n",
      "Fold 0, MAE: 1183.286138317614\n",
      "Fold 1, MAE: 1177.9553902909124\n",
      "Fold 2, MAE: 1155.3975773011248\n",
      "3-fold CV score: 1172.2130353032169\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 550, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 162, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 24, 'optimizer': 'adadelta', 'wdecay': 0.056570000000000002}\n",
      "Fold 0, MAE: 1161.7203151536628\n",
      "Fold 1, MAE: 1179.3700442916124\n"
     ]
    }
   ],
   "source": [
    "# VERSION 4. Insights:\n",
    "#  why not to test 4-layer architectures?\n",
    "# we need to introduce new optimizers\n",
    "#  adding batch normalization (https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "        'hidden3_dropout': hp.choice('hidden3_dropout', np.linspace(0.1,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int32')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int32')),\n",
    "         'hidden3_units': hp.choice('hidden3_units', np.linspace(20,80,30,dtype='int32')),\n",
    "         'optimizer': hp.choice('optimizer', ['adam','adadelta']),\n",
    "         'wdecay':hp.choice('wdecay', np.linspace(0.00001,0.1,1000)),\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print ('Model Testing:', params)\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1], init='he_normal',\n",
    "                        W_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units'], init='he_normal',W_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(params['hidden3_units'], init='he_normal',W_regularizer=l2(params['wdecay']))) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden3_dropout']))\n",
    "        \n",
    "        model.add(Dense(1, init='he_normal',W_regularizer=l2(params['wdecay'])))\n",
    "        model.compile(loss='mae', optimizer=params['optimizer'])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 100, trials=trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
