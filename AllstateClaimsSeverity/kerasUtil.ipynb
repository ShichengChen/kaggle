{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "#sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a callback function to be used with training of Keras models.\n",
    "# It create an exponential moving average of a model (trainable) weights.\n",
    "# This functionlity is already available in TensorFlow:\n",
    "# https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#ExponentialMovingAverage\n",
    "# and can often be used to get better validation/test performance. For an\n",
    "# intuitive explantion on why to use this, see 'Model Ensembles\" section here:\n",
    "# http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "\n",
    "class ExponentialMovingAverage(Callback):\n",
    "    \"\"\"create a copy of trainable weights which gets updated at every\n",
    "       batch using exponential weight decay. The moving average weights along\n",
    "       with the other states of original model(except original model trainable\n",
    "       weights) will be saved at every epoch if save_mv_ave_model is True.\n",
    "       If both save_mv_ave_model and save_best_only are True, the latest\n",
    "       best moving average model according to the quantity monitored\n",
    "       will not be overwritten. Of course, save_best_only can be True\n",
    "       only if there is a validation set.\n",
    "       This is equivalent to save_best_only mode of ModelCheckpoint\n",
    "       callback with similar code. custom_objects is a dictionary\n",
    "       holding name and Class implementation for custom layers.\n",
    "       At end of every batch, the update is as follows:\n",
    "       mv_weight -= (1 - decay) * (mv_weight - weight)\n",
    "       where weight and mv_weight is the ordinal model weight and the moving\n",
    "       averaged weight respectively. At the end of the training, the moving\n",
    "       averaged weights are transferred to the original model.\n",
    "       \"\"\"\n",
    "    def __init__(self, decay=0.999, filepath='temp_weight.hdf5',\n",
    "                 save_mv_ave_model=True, verbose=0,\n",
    "                 save_best_only=False, monitor='val_loss', mode='auto',\n",
    "                 save_weights_only=False, custom_objects={}):\n",
    "        self.decay = decay\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_mv_ave_model = save_mv_ave_model\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.save_best_only = save_best_only\n",
    "        self.monitor = monitor\n",
    "        self.custom_objects = custom_objects  # dictionary of custom layers\n",
    "        self.sym_trainable_weights = None  # trainable weights of model\n",
    "        self.mv_trainable_weights_vals = None  # moving averaged values\n",
    "        super(ExponentialMovingAverage, self).__init__()\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.sym_trainable_weights = self.model.trainable_weights\n",
    "        # Initialize moving averaged weights using original model values\n",
    "        self.mv_trainable_weights_vals = {x.name: K.get_value(x) for x in\n",
    "                                          self.sym_trainable_weights}\n",
    "        if self.verbose:\n",
    "            print('Created a copy of model weights to initialize moving'\n",
    "                  ' averaged weights.')\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        for weight in self.sym_trainable_weights:\n",
    "            old_val = self.mv_trainable_weights_vals[weight.name]\n",
    "            self.mv_trainable_weights_vals[weight.name] -= \\\n",
    "                (1.0 - self.decay) * (old_val - K.get_value(weight))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"After each epoch, we can optionally save the moving averaged model,\n",
    "        but the weights will NOT be transferred to the original model. This\n",
    "        happens only at the end of training. We also need to transfer state of\n",
    "        original model to model2 as model2 only gets updated trainable weight\n",
    "        at end of each batch and non-trainable weights are not transferred\n",
    "        (for example mean and var for batch normalization layers).\"\"\"\n",
    "        if self.save_mv_ave_model:\n",
    "            filepath = self.filepath.format(epoch=epoch, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best moving averaged model only '\n",
    "                                  'with %s available, skipping.'\n",
    "                                  % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('saving moving average model to %s'\n",
    "                                  % (filepath))\n",
    "                        self.best = current\n",
    "                        model2 = self._make_mv_model(filepath)\n",
    "                        if self.save_weights_only:\n",
    "                            model2.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            model2.save(filepath, overwrite=True)\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving moving average model to %s' % (epoch, filepath))\n",
    "                model2 = self._make_mv_model(filepath)\n",
    "                if self.save_weights_only:\n",
    "                    model2.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    model2.save(filepath, overwrite=True)\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        for weight in self.sym_trainable_weights:\n",
    "            K.set_value(weight, self.mv_trainable_weights_vals[weight.name])\n",
    "\n",
    "    def _make_mv_model(self, filepath):\n",
    "        \"\"\" Create a model with moving averaged weights. Other variables are\n",
    "        the same as original mode. We first save original model to save its\n",
    "        state. Then copy moving averaged weights over.\"\"\"\n",
    "        self.model.save(filepath, overwrite=True)\n",
    "        model2 = load_model(filepath, custom_objects=self.custom_objects)\n",
    "\n",
    "        for w2, w in zip(model2.trainable_weights, self.model.trainable_weights):\n",
    "            K.set_value(w2, self.mv_trainable_weights_vals[w.name])\n",
    "\n",
    "        return model2\n",
    "\n",
    "\n",
    "def batch_generator(X, y=None, batch_size=128, shuffle=False):\n",
    "    index = np.arange(X.shape[0])\n",
    "\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index)\n",
    "\n",
    "        batch_start = 0\n",
    "        while batch_start < X.shape[0]:\n",
    "            batch_index = index[batch_start:batch_start + batch_size]\n",
    "            batch_start += batch_size\n",
    "\n",
    "            X_batch = X[batch_index, :]\n",
    "\n",
    "            if sp.issparse(X_batch):\n",
    "                X_batch = X_batch.toarray()\n",
    "\n",
    "            if y is None:\n",
    "                yield X_batch\n",
    "            else:\n",
    "                yield X_batch, y[batch_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1153)\n",
      "(188318,)\n",
      "(125546, 131)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "cat_names = [c for c in train.columns if 'cat' in c]\n",
    "\n",
    "train = pd.get_dummies(data=train, columns=cat_names)\n",
    "\n",
    "features = [x for x in train.columns if x not in ['id','loss']]\n",
    "\n",
    "train_x = np.array(train[features])\n",
    "\n",
    "ntrain = train_x.shape[0]\n",
    "\n",
    "# np.log(train['loss'] + 200) provides\n",
    "# a better score, but let's keep it simple now\n",
    "train_y = np.array(train['loss'])\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)\n",
    "print (test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cdn.rawgit.com/dnkirill/allstate_capstone/master/images/mlp3.svg\"></td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.initializers import he_normal\n",
    "from keras.regularizers import l2\n",
    "def hyper_model(seed = None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(437, input_dim=train_x.shape[1], init=he_normal(seed = seed)\n",
    "                    ,W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.536))\n",
    "    \n",
    "    model.add(Dense(182, init=he_normal(seed = seed),W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(73, init=he_normal(seed = seed),W_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.233))\n",
    "    \n",
    "    model.add(Dense(1, init=he_normal(seed = seed),W_regularizer=l2(0.002)))\n",
    "    model.compile(loss='mae', optimizer='adadelta')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel={}\n",
    "def cross_validate_mlp(mlp_func, nfolds=10,nbags=10):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nbags,))\n",
    "    stack_train = np.zeros((nbags,len(train_y)))\n",
    "    stack_test = np.zeros((nbags,len(test)))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        for bag in range(nbags):\n",
    "            nnmodel['nn%d',k*10+bag*1] = mlp_func(seed = k*10+bag*1)\n",
    "            #early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            fit = nnmodel['nn%d',k*10+bag*1].fit(xtr, ytr, validation_split=0.2, batch_size=128,\n",
    "                          nb_epoch=30, verbose=1, callbacks=[ExponentialMovingAverage(save_mv_ave_model=False)])\n",
    "            pred = nnmodel['nn%d',k*10+bag*1].predict(xte, batch_size=256)\n",
    "            score = mean_absolute_error(yte, pred)\n",
    "            val_scores[bag] += score\n",
    "            #stack_train[bag][test_index] = pred[:,0]\n",
    "    for bag in range(nbags):\n",
    "        avg_score = val_scores[bag] / float(nfolds)\n",
    "    print (avg_score)\n",
    "    return val_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(437, input_dim=1153, kernel_initializer=<keras.ini..., kernel_regularizer=<keras.reg...)`\n",
      "  import sys\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(182, kernel_initializer=<keras.ini..., kernel_regularizer=<keras.reg...)`\n",
      "  if sys.path[0] == '':\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(73, kernel_initializer=<keras.ini..., kernel_regularizer=<keras.reg...)`\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=<keras.ini..., kernel_regularizer=<keras.reg...)`\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 2136.8445 - val_loss: 1211.0096\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1368.0154 - val_loss: 1174.2123\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1348.4634 - val_loss: 1162.3615\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1335.5915 - val_loss: 1160.5855\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1324.9138 - val_loss: 1157.1709\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1320.9364 - val_loss: 1151.4905\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1311.3297 - val_loss: 1151.7594\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1301.4330 - val_loss: 1148.3886\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1300.3361 - val_loss: 1147.8567\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1289.7363 - val_loss: 1144.9172\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1281.4588 - val_loss: 1144.4496\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1275.7259 - val_loss: 1142.8026\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1270.5858 - val_loss: 1142.3689\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1267.0577 - val_loss: 1142.7023\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1259.0393 - val_loss: 1141.8307\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1253.4687 - val_loss: 1139.5708\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1251.9243 - val_loss: 1138.0351\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1243.9948 - val_loss: 1138.8213\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1237.9289 - val_loss: 1139.6857\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1237.1984 - val_loss: 1137.4180\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1229.2090 - val_loss: 1139.1652\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1223.4740 - val_loss: 1138.3114\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1221.0365 - val_loss: 1137.6708\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1214.8621 - val_loss: 1139.3900\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1209.2493 - val_loss: 1139.7740\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 17s 123us/step - loss: 1208.9019 - val_loss: 1140.4242\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 17s 123us/step - loss: 1201.7587 - val_loss: 1139.4725\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1197.0751 - val_loss: 1136.9500\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 16s 121us/step - loss: 1195.2439 - val_loss: 1139.4542\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1192.8264 - val_loss: 1135.9406\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 2071.9987 - val_loss: 1191.1037\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1373.6178 - val_loss: 1177.2841\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1350.5437 - val_loss: 1169.8130\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1340.1130 - val_loss: 1163.3499\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1329.6134 - val_loss: 1158.7227\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1320.6215 - val_loss: 1160.9704\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1314.2004 - val_loss: 1154.1383\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1308.0010 - val_loss: 1149.6000\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1296.8040 - val_loss: 1150.4370\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1293.1222 - val_loss: 1146.9354\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1288.3336 - val_loss: 1146.1717\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1274.9677 - val_loss: 1145.8856\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1272.4592 - val_loss: 1142.3313\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1266.2087 - val_loss: 1141.7189\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1259.9649 - val_loss: 1143.9279\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1251.0817 - val_loss: 1140.5869\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1247.1934 - val_loss: 1140.8960\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1243.6663 - val_loss: 1138.4738\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1237.0576 - val_loss: 1139.4121\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1238.2472 - val_loss: 1139.1498\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1229.1950 - val_loss: 1138.0963\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1220.6741 - val_loss: 1138.3992\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 17s 129us/step - loss: 1219.4447 - val_loss: 1140.1042\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 17s 128us/step - loss: 1215.5227 - val_loss: 1137.3535\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1212.5046 - val_loss: 1138.9575\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1206.6602 - val_loss: 1138.5141\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 130us/step - loss: 1201.2265 - val_loss: 1138.5985\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1197.6683 - val_loss: 1139.2052\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1194.8515 - val_loss: 1139.7844\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1187.7812 - val_loss: 1138.2683\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 2202.9307 - val_loss: 1196.6266\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1319.3926 - val_loss: 1178.6024\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1290.9399 - val_loss: 1160.0900\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1283.6839 - val_loss: 1157.9371\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1273.6734 - val_loss: 1153.1740\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1268.1974 - val_loss: 1150.0075\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1262.8743 - val_loss: 1147.7431\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1255.3436 - val_loss: 1147.6894\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1247.6848 - val_loss: 1149.6518\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 19s 139us/step - loss: 1246.4067 - val_loss: 1146.0803\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1244.1793 - val_loss: 1145.7803\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1238.8154 - val_loss: 1142.4776\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1233.1015 - val_loss: 1140.2750\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1225.1271 - val_loss: 1145.3307\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1226.5269 - val_loss: 1140.2690\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1220.3472 - val_loss: 1139.6590\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1219.2973 - val_loss: 1140.1490\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1215.3646 - val_loss: 1140.9080\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1208.6995 - val_loss: 1140.5112\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1207.5341 - val_loss: 1138.8828\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1200.9377 - val_loss: 1136.9395\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1197.3192 - val_loss: 1138.0422\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1194.9309 - val_loss: 1139.2039\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1191.8178 - val_loss: 1137.4358\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1193.0763 - val_loss: 1137.8918\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1186.3307 - val_loss: 1141.3406\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1182.1618 - val_loss: 1137.3830\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1180.5306 - val_loss: 1136.4484\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1175.1119 - val_loss: 1137.6777\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1172.4210 - val_loss: 1137.8159\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 2239.8089 - val_loss: 1261.3705\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1405.6007 - val_loss: 1181.7165\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1360.0286 - val_loss: 1170.7594\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1344.5429 - val_loss: 1166.4485\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1332.3695 - val_loss: 1160.8927\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1322.0602 - val_loss: 1154.6104\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1309.5312 - val_loss: 1153.0482\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1304.4959 - val_loss: 1149.4469\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1295.1163 - val_loss: 1150.1527\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1289.2891 - val_loss: 1149.9420\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1282.1020 - val_loss: 1146.0686\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1277.1969 - val_loss: 1143.3649\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1267.3412 - val_loss: 1143.2648\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1258.1859 - val_loss: 1142.8621\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1256.2362 - val_loss: 1143.6921\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1251.4130 - val_loss: 1142.9598\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1242.3992 - val_loss: 1139.4462\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1235.5188 - val_loss: 1146.4538\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1234.1471 - val_loss: 1140.7385\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1228.9016 - val_loss: 1139.2347\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1220.7735 - val_loss: 1140.2686\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1217.5269 - val_loss: 1141.0856\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1210.3100 - val_loss: 1138.8559\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1211.1652 - val_loss: 1138.3019\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1202.4185 - val_loss: 1138.5907\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1198.8208 - val_loss: 1141.9370\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1197.4445 - val_loss: 1136.7956\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1191.7102 - val_loss: 1138.1676\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1188.7447 - val_loss: 1138.8293\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1181.7614 - val_loss: 1138.6433\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 2054.1842 - val_loss: 1189.8430\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1293.4916 - val_loss: 1165.0696\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1274.0341 - val_loss: 1163.8447\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1269.2701 - val_loss: 1159.0957\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1265.1267 - val_loss: 1152.4797\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1256.4220 - val_loss: 1150.0192\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1250.7680 - val_loss: 1150.3947\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1242.7094 - val_loss: 1148.5117\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1244.7631 - val_loss: 1146.4831\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1241.7470 - val_loss: 1145.5030\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1234.6014 - val_loss: 1144.4723\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 136us/step - loss: 1230.1546 - val_loss: 1141.8863\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1223.1537 - val_loss: 1143.4956\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1221.0883 - val_loss: 1141.5019\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1218.5322 - val_loss: 1141.6885\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1216.2401 - val_loss: 1141.3383\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1213.3989 - val_loss: 1140.9270\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1207.6255 - val_loss: 1140.8451\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1206.9145 - val_loss: 1139.8961\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 18s 136us/step - loss: 1207.8705 - val_loss: 1139.8911\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1200.6096 - val_loss: 1138.6058\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1195.8688 - val_loss: 1139.5665\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1196.3434 - val_loss: 1141.5087\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1190.3771 - val_loss: 1138.4707\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1190.0456 - val_loss: 1140.5115\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1188.0599 - val_loss: 1137.1866\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1182.8858 - val_loss: 1137.5942\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1181.6512 - val_loss: 1141.3081\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1176.5385 - val_loss: 1142.1583\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1172.9276 - val_loss: 1140.6044\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 2144.6456 - val_loss: 1205.0859\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1393.1877 - val_loss: 1183.4481\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1361.7959 - val_loss: 1174.5665\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1350.6557 - val_loss: 1157.9183\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1334.6091 - val_loss: 1156.0701\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1333.8713 - val_loss: 1155.0103\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1322.1236 - val_loss: 1151.0199\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1317.2489 - val_loss: 1147.8993\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1304.8178 - val_loss: 1145.7375\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1300.3364 - val_loss: 1145.0189\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1291.2443 - val_loss: 1143.7636\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1287.8572 - val_loss: 1146.1831\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1276.5684 - val_loss: 1143.4395\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1269.3415 - val_loss: 1141.9166\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1261.9161 - val_loss: 1139.9074\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1260.4330 - val_loss: 1139.2969\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1254.1178 - val_loss: 1138.7508\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1246.7647 - val_loss: 1139.0403\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1244.7637 - val_loss: 1138.0484\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1238.5137 - val_loss: 1139.2734\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1232.3958 - val_loss: 1139.3193\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1225.7392 - val_loss: 1140.2724\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1217.6422 - val_loss: 1138.9323\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1217.9581 - val_loss: 1140.6241\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1212.0300 - val_loss: 1137.9776\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1207.9516 - val_loss: 1137.3329\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1202.1639 - val_loss: 1137.4370\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1202.4276 - val_loss: 1137.7540\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1193.1760 - val_loss: 1137.7798\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1191.9403 - val_loss: 1139.0902\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 2259.5790 - val_loss: 1273.8967\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1386.2424 - val_loss: 1173.3188\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1357.7661 - val_loss: 1165.9096\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1338.8833 - val_loss: 1160.6365\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1334.3216 - val_loss: 1155.1606\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1325.3766 - val_loss: 1150.9470\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1316.5108 - val_loss: 1147.0473\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1309.5231 - val_loss: 1149.8965\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1304.0151 - val_loss: 1149.4891\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1296.9002 - val_loss: 1146.4517\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1284.7752 - val_loss: 1146.2554\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1277.7216 - val_loss: 1141.9715\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1273.9915 - val_loss: 1141.7806\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1267.6184 - val_loss: 1139.4200\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1263.4858 - val_loss: 1141.6157\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1254.7418 - val_loss: 1140.1980\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1248.3885 - val_loss: 1140.9212\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1244.0434 - val_loss: 1139.7264\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1237.5558 - val_loss: 1137.0410\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1233.1224 - val_loss: 1139.0818\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1229.6060 - val_loss: 1141.9743\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 139us/step - loss: 1225.4729 - val_loss: 1138.6432\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1221.4579 - val_loss: 1141.4246\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1215.6216 - val_loss: 1138.1773\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1211.9904 - val_loss: 1137.7429\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1206.8490 - val_loss: 1137.9544\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1202.2623 - val_loss: 1138.1697\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1197.9831 - val_loss: 1138.1826\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1191.9367 - val_loss: 1136.8100\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 18s 135us/step - loss: 1185.6241 - val_loss: 1138.3091\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 2116.1998 - val_loss: 1206.5501\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1416.3263 - val_loss: 1188.1446\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1391.3182 - val_loss: 1168.8870\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1372.9346 - val_loss: 1163.3368\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1362.2281 - val_loss: 1160.8660\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1351.6266 - val_loss: 1155.5287\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1340.0294 - val_loss: 1155.8911\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1329.2509 - val_loss: 1153.1184\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1322.5075 - val_loss: 1148.5872\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1317.1500 - val_loss: 1150.8609\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1308.1978 - val_loss: 1144.5067\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1298.5340 - val_loss: 1147.7585\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1283.9518 - val_loss: 1144.9616\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1281.9258 - val_loss: 1141.0839\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1275.9014 - val_loss: 1145.0863\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1270.5010 - val_loss: 1142.1581\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1261.2836 - val_loss: 1141.0372\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1256.9469 - val_loss: 1143.8497\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1249.1253 - val_loss: 1139.1511\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1241.7271 - val_loss: 1139.8702\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1239.5082 - val_loss: 1139.1748\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1228.5063 - val_loss: 1139.3975\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1225.2605 - val_loss: 1144.9654\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1221.1980 - val_loss: 1139.6081\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 17s 123us/step - loss: 1216.6648 - val_loss: 1136.2978\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 17s 123us/step - loss: 1212.0503 - val_loss: 1139.5642\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1204.8533 - val_loss: 1138.0810\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1200.4940 - val_loss: 1137.9618\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1198.8334 - val_loss: 1136.7573\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 17s 123us/step - loss: 1192.4135 - val_loss: 1139.0997\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 2401.0848 - val_loss: 1224.5170\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 17s 127us/step - loss: 1356.2255 - val_loss: 1172.5580\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1308.5034 - val_loss: 1161.1835\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1290.3898 - val_loss: 1157.2568\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1283.3145 - val_loss: 1156.0636\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 17s 126us/step - loss: 1273.2695 - val_loss: 1151.9786\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1268.1939 - val_loss: 1149.0146\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1258.7028 - val_loss: 1146.3087\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1254.2957 - val_loss: 1144.9875\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 17s 124us/step - loss: 1248.5598 - val_loss: 1142.7323\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 17s 125us/step - loss: 1243.4486 - val_loss: 1143.6926\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1238.4497 - val_loss: 1143.4853\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1229.9385 - val_loss: 1142.3484\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1225.6958 - val_loss: 1144.5561\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1225.5084 - val_loss: 1141.2525\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1219.1575 - val_loss: 1140.0488\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1215.8338 - val_loss: 1137.9409\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1212.6819 - val_loss: 1138.0875\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1202.4094 - val_loss: 1139.0625\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1202.5107 - val_loss: 1138.9029\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1200.6747 - val_loss: 1135.6996\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1197.4651 - val_loss: 1140.3760\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1192.6379 - val_loss: 1138.5232\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1190.1032 - val_loss: 1136.5888\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1186.4231 - val_loss: 1135.8767\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1184.2307 - val_loss: 1136.1152\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1177.3387 - val_loss: 1137.6353\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1174.8337 - val_loss: 1137.6458\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1172.2990 - val_loss: 1139.1157\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1171.1520 - val_loss: 1139.2082\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 2139.8526 - val_loss: 1223.8553\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1444.7186 - val_loss: 1195.2099\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1414.4648 - val_loss: 1174.6934\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1396.3485 - val_loss: 1165.2481\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1384.7283 - val_loss: 1156.8351\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1372.4452 - val_loss: 1153.8240\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1360.4773 - val_loss: 1152.6097\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1348.8893 - val_loss: 1148.4774\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 20s 144us/step - loss: 1339.5999 - val_loss: 1149.4132\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1328.8798 - val_loss: 1145.8619\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1318.2340 - val_loss: 1149.0200\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1309.0212 - val_loss: 1146.1824\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1301.3928 - val_loss: 1141.8263\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1294.1061 - val_loss: 1142.0574\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1283.4924 - val_loss: 1141.3929\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1275.6001 - val_loss: 1141.6425\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1269.4110 - val_loss: 1141.2354\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1261.1906 - val_loss: 1140.0344\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1251.4727 - val_loss: 1139.0714\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1246.7520 - val_loss: 1138.9207\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1241.4903 - val_loss: 1140.3766\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1233.3801 - val_loss: 1138.6017\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1226.4705 - val_loss: 1138.1054\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1223.9407 - val_loss: 1138.1458\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1212.7051 - val_loss: 1137.1654\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1209.5951 - val_loss: 1137.6452\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1205.9194 - val_loss: 1138.1650\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1200.9238 - val_loss: 1138.6567\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1197.4594 - val_loss: 1139.2257\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1192.1606 - val_loss: 1137.5088\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 2080.5096 - val_loss: 1192.2342\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1338.0614 - val_loss: 1179.1285\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1320.3221 - val_loss: 1166.5608\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1307.7686 - val_loss: 1162.6286\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1302.6220 - val_loss: 1162.4527\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1295.8355 - val_loss: 1157.9150\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1287.6342 - val_loss: 1157.9189\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1280.2614 - val_loss: 1152.1224\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1274.6111 - val_loss: 1151.7157\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1273.5514 - val_loss: 1149.9900\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1266.2022 - val_loss: 1149.7387\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1258.9221 - val_loss: 1145.2888\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1256.6607 - val_loss: 1148.5226\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1250.4111 - val_loss: 1146.5484\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1248.3392 - val_loss: 1142.7793\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1238.2470 - val_loss: 1143.4050\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1232.8492 - val_loss: 1143.0505\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1233.3225 - val_loss: 1141.6364\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1228.4537 - val_loss: 1141.9859\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1221.8401 - val_loss: 1144.6116\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1217.5883 - val_loss: 1140.3843\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1212.9389 - val_loss: 1141.9265\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1209.6576 - val_loss: 1142.8974\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 19s 142us/step - loss: 1207.0988 - val_loss: 1138.5423\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1204.0647 - val_loss: 1140.7261\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1200.5843 - val_loss: 1139.1534\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1193.3938 - val_loss: 1139.3770\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1191.4067 - val_loss: 1140.7015\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1188.6284 - val_loss: 1140.8559\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1187.4637 - val_loss: 1143.6013\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 2180.7809 - val_loss: 1198.5597\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1357.5444 - val_loss: 1183.8096\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1333.4409 - val_loss: 1171.3569\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1324.3156 - val_loss: 1169.0269\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1310.8548 - val_loss: 1162.2744\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1302.8573 - val_loss: 1156.8908\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1296.7679 - val_loss: 1153.0453\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1291.7613 - val_loss: 1152.9516\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1284.2211 - val_loss: 1149.9962\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1277.4136 - val_loss: 1149.3266\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1270.6377 - val_loss: 1146.2801\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1265.8793 - val_loss: 1146.7133\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1259.3884 - val_loss: 1146.6709\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1254.2133 - val_loss: 1144.1005\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1249.0343 - val_loss: 1143.9569\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1243.1036 - val_loss: 1141.3884\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1240.1945 - val_loss: 1141.0982\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1234.7904 - val_loss: 1142.5874\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 20s 148us/step - loss: 1226.8678 - val_loss: 1140.9769\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1225.8887 - val_loss: 1143.2001\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1220.8625 - val_loss: 1140.4929\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1215.5118 - val_loss: 1144.5725\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1211.4860 - val_loss: 1141.7082\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1206.3506 - val_loss: 1140.7772\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1204.3865 - val_loss: 1139.8759\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1200.7116 - val_loss: 1137.8204\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1197.3264 - val_loss: 1139.8530\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1190.9550 - val_loss: 1142.3493\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1187.9987 - val_loss: 1139.3492\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1186.5070 - val_loss: 1139.4628\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 2093.0484 - val_loss: 1218.0940\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1370.6530 - val_loss: 1180.6813\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1347.4654 - val_loss: 1172.4624\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1338.0473 - val_loss: 1167.6807\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1328.6341 - val_loss: 1165.3870\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1323.2645 - val_loss: 1159.2015\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1310.0378 - val_loss: 1155.7779\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1303.0881 - val_loss: 1155.5309\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1294.6448 - val_loss: 1149.6210\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1290.3113 - val_loss: 1150.7052\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1281.6666 - val_loss: 1150.8720\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1277.8513 - val_loss: 1147.2242\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1265.4234 - val_loss: 1145.2367\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1261.2304 - val_loss: 1146.5473\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1256.5983 - val_loss: 1146.8386\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1250.8268 - val_loss: 1142.7445\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1245.4351 - val_loss: 1144.8405\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1237.5059 - val_loss: 1142.8881\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1233.8803 - val_loss: 1147.6673\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1229.2459 - val_loss: 1141.7344\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1225.3234 - val_loss: 1141.5554\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1221.9932 - val_loss: 1140.9507\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1214.2093 - val_loss: 1142.2156\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1214.1466 - val_loss: 1139.4626\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1207.7134 - val_loss: 1141.2928\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1202.3824 - val_loss: 1142.5613\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1195.8209 - val_loss: 1141.2084\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1196.0790 - val_loss: 1144.3636\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1190.6730 - val_loss: 1139.9176\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1185.9987 - val_loss: 1141.0116\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 2255.8777 - val_loss: 1217.4084\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1342.6190 - val_loss: 1182.2371\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1306.9559 - val_loss: 1167.8467\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1295.2782 - val_loss: 1162.5863\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1287.3793 - val_loss: 1161.6580\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1279.6271 - val_loss: 1155.8165\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1274.1021 - val_loss: 1156.3056\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1263.1138 - val_loss: 1151.5282\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1259.6152 - val_loss: 1149.5747\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1253.8198 - val_loss: 1148.4620\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1246.5467 - val_loss: 1147.5732\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1240.3110 - val_loss: 1148.0044\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1238.9223 - val_loss: 1147.1862\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1231.2306 - val_loss: 1145.4682\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1228.1969 - val_loss: 1144.0438\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1223.4971 - val_loss: 1144.6481\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1217.4410 - val_loss: 1143.7775\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1215.4994 - val_loss: 1140.0706\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1212.8732 - val_loss: 1140.4095\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1203.5535 - val_loss: 1141.9782\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1201.4498 - val_loss: 1139.3859\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1201.0387 - val_loss: 1140.5547\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1193.7364 - val_loss: 1143.3022\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1192.9607 - val_loss: 1140.8798\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1187.1434 - val_loss: 1138.9340\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1185.8573 - val_loss: 1140.9027\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1183.0876 - val_loss: 1142.9216\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1178.1873 - val_loss: 1141.9839\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 21s 155us/step - loss: 1175.5232 - val_loss: 1138.9356\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1174.0161 - val_loss: 1140.5321\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 2147.2077 - val_loss: 1216.1312\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1345.9201 - val_loss: 1177.8828\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1322.4136 - val_loss: 1166.5009\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1307.2376 - val_loss: 1164.2600\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1301.5108 - val_loss: 1165.3679\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1291.7414 - val_loss: 1157.6468\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1285.6936 - val_loss: 1154.0171\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1277.7172 - val_loss: 1156.3814\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1273.1281 - val_loss: 1150.0939\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1268.2979 - val_loss: 1150.6910\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1262.1722 - val_loss: 1151.1151\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1253.8915 - val_loss: 1148.0074\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1249.5013 - val_loss: 1145.3231\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1244.9875 - val_loss: 1146.2847\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1241.2343 - val_loss: 1146.1855\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1240.4054 - val_loss: 1144.8186\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1232.6831 - val_loss: 1142.2476\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1227.4655 - val_loss: 1141.3577\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1223.7188 - val_loss: 1144.8882\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1218.4115 - val_loss: 1142.6775\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1210.9768 - val_loss: 1139.3348\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1211.1141 - val_loss: 1141.9712\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 20s 149us/step - loss: 1207.7171 - val_loss: 1139.4574\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1202.5967 - val_loss: 1141.7724\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1199.6262 - val_loss: 1141.1489\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1196.7168 - val_loss: 1138.5500\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1191.3406 - val_loss: 1142.0968\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1188.0865 - val_loss: 1140.1666\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1182.4108 - val_loss: 1141.6725\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1184.0403 - val_loss: 1140.2516\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 2365.5027 - val_loss: 1249.4559\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1331.4758 - val_loss: 1174.7065\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1295.2610 - val_loss: 1169.7754\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1286.7799 - val_loss: 1159.9901\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1280.5261 - val_loss: 1158.2572\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1266.2622 - val_loss: 1154.4041\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1262.4311 - val_loss: 1155.8020\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1254.3317 - val_loss: 1148.9341\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1251.6974 - val_loss: 1148.4464\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1246.1637 - val_loss: 1147.7779\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1240.6203 - val_loss: 1151.6991\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1236.8367 - val_loss: 1147.7616\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1231.1649 - val_loss: 1146.9737\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1227.2170 - val_loss: 1142.8992\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1220.0997 - val_loss: 1142.2697\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1217.6618 - val_loss: 1141.6737\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1215.1164 - val_loss: 1143.7383\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1210.6173 - val_loss: 1141.3455\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1208.7027 - val_loss: 1142.4917\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1204.0034 - val_loss: 1141.2139\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1199.9128 - val_loss: 1142.9258\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1193.1261 - val_loss: 1144.8682\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1192.3292 - val_loss: 1142.4172\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1188.4009 - val_loss: 1144.6633\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1185.4779 - val_loss: 1140.5768\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1182.3696 - val_loss: 1139.5654\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1178.7680 - val_loss: 1139.5454\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1176.8493 - val_loss: 1142.7311\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1173.2694 - val_loss: 1142.8651\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1170.2819 - val_loss: 1143.4017\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 2105.5297 - val_loss: 1222.9976\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1372.8211 - val_loss: 1180.8977\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1340.1012 - val_loss: 1169.3496\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1323.4044 - val_loss: 1163.3379\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1316.8666 - val_loss: 1160.7935\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1305.2072 - val_loss: 1156.3535\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1298.2856 - val_loss: 1157.5047\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 21s 156us/step - loss: 1290.8912 - val_loss: 1153.2921\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1285.5711 - val_loss: 1149.4696\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1278.1549 - val_loss: 1150.2797\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1271.9824 - val_loss: 1148.7377\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1261.5733 - val_loss: 1145.9346\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1257.0192 - val_loss: 1145.7802\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1249.9101 - val_loss: 1143.6802\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1244.9385 - val_loss: 1142.7173\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1242.1184 - val_loss: 1142.2903\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1235.4647 - val_loss: 1141.6690\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1229.1584 - val_loss: 1141.9394\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1226.0554 - val_loss: 1141.0141\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1220.9057 - val_loss: 1140.9010\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1217.3895 - val_loss: 1144.0481\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1212.6104 - val_loss: 1141.5894\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1207.5278 - val_loss: 1142.8118\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1205.4956 - val_loss: 1141.5291\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1195.4889 - val_loss: 1141.6714\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1192.7770 - val_loss: 1144.1076\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1194.3883 - val_loss: 1140.6189\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1186.6244 - val_loss: 1145.1974\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1184.7016 - val_loss: 1144.3846\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1178.3220 - val_loss: 1142.8261\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 2234.0737 - val_loss: 1256.1521\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1459.3577 - val_loss: 1200.7456\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1395.9707 - val_loss: 1185.3987\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1378.3326 - val_loss: 1172.1493\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1361.0259 - val_loss: 1166.0856\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1349.6780 - val_loss: 1163.1922\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1336.6088 - val_loss: 1156.2963\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1326.8123 - val_loss: 1154.9885\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1322.2682 - val_loss: 1152.7721\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1310.1498 - val_loss: 1153.8900\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1298.5658 - val_loss: 1149.4608\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1289.0979 - val_loss: 1148.8099\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1286.5286 - val_loss: 1146.4799\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1277.6999 - val_loss: 1146.8259\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1267.1064 - val_loss: 1147.2707\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1258.7495 - val_loss: 1144.9857\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1260.6155 - val_loss: 1144.6132\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1251.4368 - val_loss: 1145.5845\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1242.7713 - val_loss: 1142.7295\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1236.4847 - val_loss: 1142.3007\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1230.1033 - val_loss: 1141.7813\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1226.6643 - val_loss: 1143.5773\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1219.7777 - val_loss: 1140.8791\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1216.8313 - val_loss: 1141.6794\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1206.8980 - val_loss: 1140.9951\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1204.4610 - val_loss: 1140.1711\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1199.4619 - val_loss: 1144.0464\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1193.3943 - val_loss: 1140.8976\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1192.5659 - val_loss: 1139.3792\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1185.8749 - val_loss: 1140.4160\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 2383.3366 - val_loss: 1254.9669\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1287.9994 - val_loss: 1178.3898\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1265.0894 - val_loss: 1167.3440\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1254.1733 - val_loss: 1160.2639\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1248.6903 - val_loss: 1158.1389\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1241.3954 - val_loss: 1154.4100\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1235.8213 - val_loss: 1153.3420\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1231.9717 - val_loss: 1150.4609\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1226.8071 - val_loss: 1148.3645\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1223.8510 - val_loss: 1146.4120\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1218.8811 - val_loss: 1150.2537\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1214.8175 - val_loss: 1146.0519\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1212.5828 - val_loss: 1143.0057\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1206.8930 - val_loss: 1145.3341\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1204.4217 - val_loss: 1142.9417\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1199.3534 - val_loss: 1140.8290\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1197.4615 - val_loss: 1142.2968\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 22s 163us/step - loss: 1192.3928 - val_loss: 1141.5536\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1193.0373 - val_loss: 1141.3309\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1184.5280 - val_loss: 1141.1112\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1184.8262 - val_loss: 1139.3838\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1182.8956 - val_loss: 1143.5424\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1181.6165 - val_loss: 1138.5262\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1178.1562 - val_loss: 1139.1054\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1172.3481 - val_loss: 1138.9556\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1169.4656 - val_loss: 1141.5226\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1168.0417 - val_loss: 1142.3261\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1165.2191 - val_loss: 1139.8699\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1161.4531 - val_loss: 1140.0307\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1162.5075 - val_loss: 1139.9770\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 2256.1171 - val_loss: 1211.8288\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1360.3788 - val_loss: 1189.2651\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1319.6163 - val_loss: 1174.4262\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1307.3961 - val_loss: 1163.9166\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1298.7974 - val_loss: 1162.6933\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1298.4280 - val_loss: 1158.1885\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1284.8905 - val_loss: 1155.9153\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1277.2515 - val_loss: 1154.0686\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1272.3039 - val_loss: 1152.9855\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1262.7143 - val_loss: 1149.5085\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1259.1469 - val_loss: 1147.5364\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1252.7235 - val_loss: 1150.5005\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1249.8079 - val_loss: 1147.8916\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1245.7733 - val_loss: 1147.2764\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1236.5540 - val_loss: 1147.3916\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1236.8363 - val_loss: 1143.1503\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1229.1232 - val_loss: 1145.1819\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1222.9487 - val_loss: 1143.4946\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1219.5715 - val_loss: 1143.7088\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1215.5794 - val_loss: 1142.5448\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1211.4442 - val_loss: 1141.8287\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1206.1141 - val_loss: 1140.2398\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1206.4449 - val_loss: 1142.8516\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1201.2517 - val_loss: 1140.2793\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1198.2260 - val_loss: 1139.4334\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1191.7694 - val_loss: 1139.8811\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1189.2092 - val_loss: 1141.5490\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1187.1601 - val_loss: 1139.6029\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1184.2296 - val_loss: 1142.4155\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1179.4432 - val_loss: 1141.0451\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 2034.9011 - val_loss: 1203.9018\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1332.4132 - val_loss: 1170.5583\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1313.6549 - val_loss: 1164.2973\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1307.5390 - val_loss: 1164.6135\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1299.8958 - val_loss: 1156.9298\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1294.1060 - val_loss: 1155.3851\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1285.4897 - val_loss: 1153.0942\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1278.5993 - val_loss: 1149.6091\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1278.1461 - val_loss: 1148.4746\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1271.9489 - val_loss: 1150.1425\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1264.7783 - val_loss: 1149.3143\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1257.3532 - val_loss: 1147.6076\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1254.4251 - val_loss: 1147.8805\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1250.1933 - val_loss: 1144.7971\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1248.3679 - val_loss: 1141.4639\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1243.5547 - val_loss: 1141.5391\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1240.1375 - val_loss: 1143.9364\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1233.9919 - val_loss: 1141.3181\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1231.4996 - val_loss: 1141.7484\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1223.9136 - val_loss: 1140.3293\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1218.6557 - val_loss: 1139.6347\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1218.9299 - val_loss: 1143.1891\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1215.4584 - val_loss: 1139.1593\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1210.2025 - val_loss: 1140.4503\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1207.2918 - val_loss: 1139.4028\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1202.5126 - val_loss: 1141.5896\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1197.9598 - val_loss: 1137.2568\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 22s 162us/step - loss: 1199.0304 - val_loss: 1139.0730\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1192.3693 - val_loss: 1139.3508\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1191.1737 - val_loss: 1139.7606\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 2100.0017 - val_loss: 1192.6818\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1359.7053 - val_loss: 1178.7630\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1344.8710 - val_loss: 1166.7011\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1331.3354 - val_loss: 1163.9995\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1324.4374 - val_loss: 1166.4363\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1318.3366 - val_loss: 1154.9403\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1307.5504 - val_loss: 1151.8574\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1299.9217 - val_loss: 1151.3453\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1294.4053 - val_loss: 1147.0472\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1288.6576 - val_loss: 1148.7342\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1281.7593 - val_loss: 1151.5212\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1278.5299 - val_loss: 1144.7419\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1271.6133 - val_loss: 1145.7800\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1262.8527 - val_loss: 1144.2239\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1258.9186 - val_loss: 1143.6070\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1250.7160 - val_loss: 1142.1362\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1250.4661 - val_loss: 1144.6042\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1245.7972 - val_loss: 1142.8767\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1238.5394 - val_loss: 1145.4274\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1233.1601 - val_loss: 1141.2093\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1230.2488 - val_loss: 1140.7749\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1228.4528 - val_loss: 1146.4996\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1224.0186 - val_loss: 1142.5306\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1215.5764 - val_loss: 1139.9480\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1211.7350 - val_loss: 1143.7930\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1206.0922 - val_loss: 1138.7071\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1206.4334 - val_loss: 1138.6397\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1196.6314 - val_loss: 1141.6512\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1194.4735 - val_loss: 1139.0394\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1189.5157 - val_loss: 1141.4074\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 2147.9327 - val_loss: 1196.0825\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1307.9252 - val_loss: 1172.1868\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1286.8103 - val_loss: 1164.5397\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1278.5308 - val_loss: 1158.6159\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1272.8472 - val_loss: 1156.1688\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1270.5167 - val_loss: 1154.0742\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1260.7968 - val_loss: 1151.0262\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1257.1251 - val_loss: 1149.6143\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1254.6591 - val_loss: 1149.2270\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1246.8920 - val_loss: 1145.4950\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1240.7546 - val_loss: 1147.5341\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1235.8872 - val_loss: 1144.4856\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1234.0184 - val_loss: 1144.8669\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1228.3834 - val_loss: 1143.2458\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1226.4863 - val_loss: 1141.1303\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1224.6542 - val_loss: 1143.5105\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1221.4708 - val_loss: 1140.6775\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1213.4790 - val_loss: 1145.0518\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1210.2678 - val_loss: 1141.6874\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1206.1423 - val_loss: 1139.9814\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1206.6084 - val_loss: 1140.2222\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1197.7364 - val_loss: 1145.0491\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 166us/step - loss: 1199.3194 - val_loss: 1141.9824\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1195.6880 - val_loss: 1139.3795\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1190.9437 - val_loss: 1139.8770\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1190.8962 - val_loss: 1141.1665\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1187.1393 - val_loss: 1139.3036\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1182.0961 - val_loss: 1139.9623\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1179.7126 - val_loss: 1138.9682\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1178.5556 - val_loss: 1139.5680\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 2333.6825 - val_loss: 1259.7246\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1356.8241 - val_loss: 1180.2160\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1316.8490 - val_loss: 1170.1382\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1309.2448 - val_loss: 1161.8194\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1295.2498 - val_loss: 1157.7531\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1287.9789 - val_loss: 1156.7034\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 22s 166us/step - loss: 1281.6174 - val_loss: 1152.2525\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1274.2277 - val_loss: 1152.1073\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1267.6171 - val_loss: 1148.9417\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1258.5676 - val_loss: 1147.4802\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1257.4411 - val_loss: 1145.4021\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1250.0476 - val_loss: 1144.6865\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1244.8574 - val_loss: 1147.2538\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1239.9595 - val_loss: 1144.9095\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1230.9934 - val_loss: 1142.0156\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1230.0402 - val_loss: 1141.4190\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1225.5349 - val_loss: 1141.7214\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1216.8204 - val_loss: 1141.2360\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1216.6491 - val_loss: 1140.1290\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1210.9287 - val_loss: 1139.0509\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1208.9203 - val_loss: 1141.7318\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1203.5288 - val_loss: 1141.5618\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1201.2013 - val_loss: 1139.8702\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1197.2225 - val_loss: 1140.0092\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1192.0574 - val_loss: 1140.0054\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1187.5524 - val_loss: 1138.4644\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1182.8614 - val_loss: 1137.6458\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1183.6936 - val_loss: 1141.0566\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1178.8698 - val_loss: 1139.4316\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1173.9023 - val_loss: 1139.1504\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 2119.0023 - val_loss: 1226.4717\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1401.2566 - val_loss: 1186.5337\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1369.4236 - val_loss: 1172.6102\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1361.1437 - val_loss: 1165.7124\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1346.3026 - val_loss: 1159.1278\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1338.6545 - val_loss: 1158.4711\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1326.7067 - val_loss: 1157.5467\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1320.9259 - val_loss: 1154.4882\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1310.0364 - val_loss: 1151.3694\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1303.7201 - val_loss: 1149.3519\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1294.6993 - val_loss: 1149.5601\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1289.0602 - val_loss: 1146.0732\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1282.8337 - val_loss: 1147.1513\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1276.7566 - val_loss: 1148.6044\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1268.2584 - val_loss: 1145.5449\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1261.2637 - val_loss: 1144.0845\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1252.3861 - val_loss: 1141.4931\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1244.5870 - val_loss: 1143.2586\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1242.1016 - val_loss: 1143.2576\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1235.9384 - val_loss: 1140.6002\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1231.2858 - val_loss: 1142.6776\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1225.7809 - val_loss: 1142.8736\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1222.4941 - val_loss: 1139.6117\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1214.8982 - val_loss: 1140.3045\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1210.9057 - val_loss: 1140.9867\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1206.3136 - val_loss: 1138.7366\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1201.6481 - val_loss: 1138.7210\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1199.6909 - val_loss: 1138.6499\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1193.2919 - val_loss: 1139.3846\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1188.5737 - val_loss: 1140.7262\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 2116.8552 - val_loss: 1202.0588\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1407.0285 - val_loss: 1184.7887\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1370.9780 - val_loss: 1176.0732\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1360.7913 - val_loss: 1169.1318\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1344.2768 - val_loss: 1161.2708\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1341.3243 - val_loss: 1156.7612\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1330.0275 - val_loss: 1158.4899\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1320.9683 - val_loss: 1153.7393\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1310.0426 - val_loss: 1153.6968\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1303.8946 - val_loss: 1149.3667\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1295.1530 - val_loss: 1148.8388\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1284.2992 - val_loss: 1147.9167\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1280.5723 - val_loss: 1147.9640\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1274.0301 - val_loss: 1145.7604\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1264.9663 - val_loss: 1146.1986\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1256.9347 - val_loss: 1144.7120\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 23s 169us/step - loss: 1255.3298 - val_loss: 1143.8761\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1247.7166 - val_loss: 1141.8440\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1243.7259 - val_loss: 1143.3253\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1237.9905 - val_loss: 1142.3659\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1233.0028 - val_loss: 1142.7846\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1228.2233 - val_loss: 1141.4073\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1223.4317 - val_loss: 1141.7517\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1216.9908 - val_loss: 1141.1341\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1215.4683 - val_loss: 1141.8963\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1210.0747 - val_loss: 1140.7464\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1203.3887 - val_loss: 1141.2263\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1199.3825 - val_loss: 1143.1496\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1196.2933 - val_loss: 1139.0760\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1189.6990 - val_loss: 1140.2779\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 2220.9501 - val_loss: 1216.0153\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1388.2553 - val_loss: 1179.6635\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1357.8936 - val_loss: 1171.3413\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1340.3985 - val_loss: 1170.5467\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1333.0844 - val_loss: 1162.2258\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1327.3689 - val_loss: 1158.0323\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1313.0400 - val_loss: 1154.4376\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1307.0524 - val_loss: 1154.0914\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1300.9281 - val_loss: 1151.9698\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1292.6269 - val_loss: 1148.6066\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1285.2253 - val_loss: 1151.1838\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1279.5728 - val_loss: 1148.7945\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1273.8475 - val_loss: 1144.6709\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1268.1261 - val_loss: 1146.0759\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1263.1972 - val_loss: 1143.2231\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1253.6931 - val_loss: 1142.8184\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1247.2731 - val_loss: 1144.9749\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1247.5082 - val_loss: 1146.1203\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1238.2259 - val_loss: 1142.3902\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1234.4803 - val_loss: 1141.7416\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1231.6512 - val_loss: 1139.4291\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1221.1566 - val_loss: 1140.3375\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1219.3146 - val_loss: 1140.6679\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1215.4081 - val_loss: 1140.8396\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1214.5107 - val_loss: 1139.6834\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1209.0979 - val_loss: 1140.2115\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1201.6998 - val_loss: 1138.6454\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1197.8054 - val_loss: 1139.2500\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1196.0473 - val_loss: 1139.6577\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1192.8082 - val_loss: 1139.0067\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 2348.6217 - val_loss: 1222.9275\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1328.7704 - val_loss: 1179.2607\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1306.7510 - val_loss: 1168.0186\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1293.8585 - val_loss: 1160.2324\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1285.4735 - val_loss: 1158.3099\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1277.3651 - val_loss: 1153.4641\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1269.3047 - val_loss: 1150.0850\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1262.0504 - val_loss: 1150.0496\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1263.5440 - val_loss: 1147.6691\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1253.5538 - val_loss: 1144.8898\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1247.9313 - val_loss: 1150.5998\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1247.7662 - val_loss: 1144.8886\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1237.7462 - val_loss: 1144.2389\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1237.6364 - val_loss: 1144.7115\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1230.2218 - val_loss: 1142.2781\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1224.4876 - val_loss: 1142.1812\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1219.7974 - val_loss: 1144.6937\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1220.1853 - val_loss: 1140.0027\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1214.6820 - val_loss: 1140.4623\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1212.7210 - val_loss: 1140.9684\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1209.9776 - val_loss: 1139.0554\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1203.5119 - val_loss: 1140.3867\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1200.5344 - val_loss: 1139.2506\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1195.0830 - val_loss: 1141.4939\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1192.5566 - val_loss: 1139.0139\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1187.5310 - val_loss: 1141.6095\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 23s 172us/step - loss: 1187.9801 - val_loss: 1139.7127\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1183.1028 - val_loss: 1139.4130\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1179.6988 - val_loss: 1142.8902\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1177.3562 - val_loss: 1141.3969\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 2382.0871 - val_loss: 1297.8367\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1482.1941 - val_loss: 1200.0791\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1401.2403 - val_loss: 1173.3932\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1378.5670 - val_loss: 1163.1230\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 22s 166us/step - loss: 1359.3236 - val_loss: 1162.5708\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1349.7490 - val_loss: 1159.5843\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1339.9250 - val_loss: 1157.1414\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1325.9131 - val_loss: 1157.0144\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1315.8230 - val_loss: 1148.4699\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1305.2816 - val_loss: 1150.2026\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1297.3189 - val_loss: 1147.9939\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1290.3711 - val_loss: 1148.0039\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1283.1050 - val_loss: 1146.5376\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1274.1005 - val_loss: 1145.1784\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1266.4166 - val_loss: 1142.5867\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1256.1920 - val_loss: 1144.1732\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1254.5083 - val_loss: 1143.5701\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1244.7274 - val_loss: 1141.3236\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1241.0715 - val_loss: 1143.2553\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1234.4241 - val_loss: 1145.7793\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1225.6434 - val_loss: 1141.4982\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1220.6094 - val_loss: 1142.4797\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1216.6374 - val_loss: 1141.1774\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1211.4507 - val_loss: 1139.5378\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1206.5678 - val_loss: 1143.6845\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1202.5120 - val_loss: 1139.9898\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1198.4230 - val_loss: 1140.4216\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1194.0287 - val_loss: 1139.6218\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1187.8691 - val_loss: 1141.8018\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1184.5891 - val_loss: 1138.9315\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 2136.6325 - val_loss: 1217.3013\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1406.0265 - val_loss: 1192.7748\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1381.0424 - val_loss: 1176.7000\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1360.6598 - val_loss: 1165.7681\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1352.3093 - val_loss: 1163.3076\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1340.1142 - val_loss: 1157.3881\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1332.9276 - val_loss: 1156.8616\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1319.3740 - val_loss: 1155.9340\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1315.3211 - val_loss: 1153.5912\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1306.3137 - val_loss: 1148.6359\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1302.7256 - val_loss: 1147.1135\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1295.5100 - val_loss: 1146.1458\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1289.0526 - val_loss: 1147.9852\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1277.7314 - val_loss: 1144.6653\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1271.2002 - val_loss: 1143.3052\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1266.1978 - val_loss: 1144.1677\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1256.7149 - val_loss: 1142.0878\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1249.4514 - val_loss: 1142.2717\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 24s 173us/step - loss: 1246.9436 - val_loss: 1143.0873\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1240.7946 - val_loss: 1142.2608\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1237.5697 - val_loss: 1140.9173\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 24s 181us/step - loss: 1231.3077 - val_loss: 1142.4357\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1224.5568 - val_loss: 1140.6379\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1222.2999 - val_loss: 1141.8429\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1214.6928 - val_loss: 1140.1376\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1212.1474 - val_loss: 1144.3336\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 168us/step - loss: 1204.9091 - val_loss: 1142.5301\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1203.0380 - val_loss: 1143.1652\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1198.1187 - val_loss: 1143.1442\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1195.4944 - val_loss: 1141.8558\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 2150.7554 - val_loss: 1218.5906\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1337.6193 - val_loss: 1168.1573\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1316.9279 - val_loss: 1165.0101\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1302.3673 - val_loss: 1155.7774\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1296.7557 - val_loss: 1152.1913\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 22s 159us/step - loss: 1288.0358 - val_loss: 1148.6099\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1285.9567 - val_loss: 1146.4016\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1277.5689 - val_loss: 1146.8923\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1273.9930 - val_loss: 1144.5518\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1266.2390 - val_loss: 1143.5545\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1257.9490 - val_loss: 1140.6536\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1254.1664 - val_loss: 1141.4035\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1251.1137 - val_loss: 1138.7257\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1243.5756 - val_loss: 1137.8118\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1242.1920 - val_loss: 1137.9008\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1240.0191 - val_loss: 1136.0388\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1231.8885 - val_loss: 1137.6115\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1230.4291 - val_loss: 1137.0768\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1224.7801 - val_loss: 1138.8258\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1222.3671 - val_loss: 1137.8631\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1219.6334 - val_loss: 1134.3788\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1213.6910 - val_loss: 1135.8046\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 21s 152us/step - loss: 1211.3181 - val_loss: 1133.7670\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1207.3723 - val_loss: 1139.2819\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1200.4809 - val_loss: 1134.5985\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1198.4281 - val_loss: 1135.9121\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1194.3169 - val_loss: 1134.7186\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1191.0764 - val_loss: 1134.1696\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1191.1723 - val_loss: 1136.7093\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1186.6310 - val_loss: 1135.2024\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 2137.8200 - val_loss: 1238.5988\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1354.4085 - val_loss: 1172.8460\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1330.5786 - val_loss: 1162.1122\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1317.3301 - val_loss: 1156.1483\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1310.4688 - val_loss: 1151.0516\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1300.7974 - val_loss: 1152.1132\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1292.4015 - val_loss: 1148.0159\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1287.1564 - val_loss: 1146.3032\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1279.7984 - val_loss: 1145.6582\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1275.6331 - val_loss: 1143.8991\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1270.1752 - val_loss: 1140.0675\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1262.0570 - val_loss: 1142.8682\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1257.2029 - val_loss: 1137.3925\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1253.4822 - val_loss: 1137.0605\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1251.4829 - val_loss: 1137.1938\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1239.2431 - val_loss: 1136.1043\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1241.5203 - val_loss: 1135.9891\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1235.3821 - val_loss: 1135.1123\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1228.9067 - val_loss: 1135.1234\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1225.8304 - val_loss: 1134.4474\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1220.6729 - val_loss: 1136.9281\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1216.6972 - val_loss: 1134.1761\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1213.9392 - val_loss: 1133.8111\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1207.6816 - val_loss: 1136.5704\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1207.0470 - val_loss: 1132.5717\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1198.5207 - val_loss: 1132.0727\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1199.1369 - val_loss: 1132.9116\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1194.8174 - val_loss: 1133.3289\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1189.2008 - val_loss: 1133.4879\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1187.1007 - val_loss: 1134.4655\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 2162.8052 - val_loss: 1227.0686\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 24s 181us/step - loss: 1434.3886 - val_loss: 1180.4347\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1406.8078 - val_loss: 1169.9269\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 24s 181us/step - loss: 1385.5911 - val_loss: 1162.9549\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1372.9176 - val_loss: 1156.9346\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1358.9106 - val_loss: 1155.1454\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1354.4319 - val_loss: 1150.7613\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 24s 181us/step - loss: 1339.1520 - val_loss: 1148.6646\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1329.6377 - val_loss: 1147.1386\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1322.8724 - val_loss: 1143.0695\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1315.2449 - val_loss: 1143.1224\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1300.2102 - val_loss: 1142.9096\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1295.9037 - val_loss: 1140.4770\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1283.5396 - val_loss: 1140.8644\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1279.2466 - val_loss: 1139.9055\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 24s 179us/step - loss: 1271.4302 - val_loss: 1138.5872\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1262.0754 - val_loss: 1139.8653\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1255.4919 - val_loss: 1136.8744\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1253.5671 - val_loss: 1136.2325\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1240.8924 - val_loss: 1137.5931\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1237.9902 - val_loss: 1137.2019\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1234.5486 - val_loss: 1142.9789\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1224.6369 - val_loss: 1139.5720\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1220.4593 - val_loss: 1135.7518\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1217.6171 - val_loss: 1136.0897\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1208.7182 - val_loss: 1135.0067\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1204.8671 - val_loss: 1138.1816\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1199.6844 - val_loss: 1135.4394\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1196.1094 - val_loss: 1135.1158\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1192.6365 - val_loss: 1135.6284\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 2247.0454 - val_loss: 1276.2062\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1443.4534 - val_loss: 1200.9349\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1387.9025 - val_loss: 1167.9063\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1369.8993 - val_loss: 1157.9230\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1359.0763 - val_loss: 1158.5290\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1341.0877 - val_loss: 1150.7262\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1334.0948 - val_loss: 1147.9963\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1327.5079 - val_loss: 1151.6210\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1319.8499 - val_loss: 1147.1125\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1303.1539 - val_loss: 1148.0195\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1298.2799 - val_loss: 1140.1153\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1291.3443 - val_loss: 1139.3731\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1281.6168 - val_loss: 1138.2417\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1276.6810 - val_loss: 1137.9395\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1266.5799 - val_loss: 1138.3344\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1264.1834 - val_loss: 1137.6315\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1255.2206 - val_loss: 1140.7335\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1248.3322 - val_loss: 1135.3960\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1241.9511 - val_loss: 1134.9452\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1235.2474 - val_loss: 1135.1656\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1228.0507 - val_loss: 1139.9084\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1226.0318 - val_loss: 1134.8137\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1220.8629 - val_loss: 1134.4010\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1214.7932 - val_loss: 1136.9629\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1209.3952 - val_loss: 1135.8377\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1206.4600 - val_loss: 1135.7673\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1201.2001 - val_loss: 1133.9951\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1197.8145 - val_loss: 1133.8902\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1192.5017 - val_loss: 1134.5573\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1189.1592 - val_loss: 1136.1815\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 2231.6264 - val_loss: 1270.8577\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1401.2818 - val_loss: 1179.1528\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1358.5306 - val_loss: 1163.7818\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1346.3119 - val_loss: 1159.4069\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1337.6175 - val_loss: 1162.0990\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1324.8670 - val_loss: 1154.5274\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1316.0526 - val_loss: 1151.2882\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1313.1357 - val_loss: 1145.4066\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1301.7768 - val_loss: 1148.5227\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1294.2434 - val_loss: 1144.2390\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1286.9119 - val_loss: 1142.2359\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1277.9290 - val_loss: 1143.0856\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1273.2614 - val_loss: 1140.3006\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1268.6459 - val_loss: 1139.3744\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 181us/step - loss: 1260.0090 - val_loss: 1137.6214\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1253.9823 - val_loss: 1139.3100\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1253.6677 - val_loss: 1139.5390\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1243.7684 - val_loss: 1142.1280\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1239.3260 - val_loss: 1138.8639\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1237.0601 - val_loss: 1135.2604\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1229.5796 - val_loss: 1135.1959\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1225.3937 - val_loss: 1134.3974\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1222.2129 - val_loss: 1135.6165\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1215.4012 - val_loss: 1134.6523\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1209.5748 - val_loss: 1136.6134\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 25s 182us/step - loss: 1207.7953 - val_loss: 1134.7996\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1204.4468 - val_loss: 1133.6029\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1196.1200 - val_loss: 1137.4640\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1190.1622 - val_loss: 1135.9380\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1190.7313 - val_loss: 1137.6763\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 2399.7670 - val_loss: 1199.2722\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1360.9423 - val_loss: 1172.8074\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1323.9588 - val_loss: 1161.5460\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1308.8405 - val_loss: 1156.9199\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1298.3028 - val_loss: 1150.9087\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1293.9025 - val_loss: 1149.9351\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1286.2469 - val_loss: 1145.9763\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1276.8648 - val_loss: 1144.3715\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1269.4994 - val_loss: 1141.4496\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1265.2723 - val_loss: 1139.1267\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1256.3775 - val_loss: 1140.6564\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1252.5934 - val_loss: 1139.3502\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1248.8758 - val_loss: 1137.2624\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1243.3110 - val_loss: 1137.0733\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1236.6544 - val_loss: 1137.6719\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1235.4073 - val_loss: 1135.0696\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1232.9461 - val_loss: 1135.3563\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1221.6017 - val_loss: 1136.1141\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1222.2149 - val_loss: 1135.4381\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1217.9568 - val_loss: 1135.0602\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1212.3172 - val_loss: 1134.8438\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1206.5242 - val_loss: 1135.3943\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1203.3371 - val_loss: 1135.7859\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1197.9126 - val_loss: 1133.6888\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1194.3925 - val_loss: 1134.9339\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1192.8401 - val_loss: 1135.3469\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1189.6974 - val_loss: 1136.4309\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1185.8625 - val_loss: 1137.0564\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1183.3449 - val_loss: 1135.6314\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1179.1200 - val_loss: 1134.7365\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 2257.9343 - val_loss: 1216.4538\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1358.9859 - val_loss: 1176.9543\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1329.9601 - val_loss: 1161.8217\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1320.4262 - val_loss: 1156.3290\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1309.3995 - val_loss: 1152.1958\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1302.2914 - val_loss: 1148.9254\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1294.0514 - val_loss: 1148.3516\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1285.9851 - val_loss: 1144.1450\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1278.4023 - val_loss: 1142.6979\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1273.0185 - val_loss: 1144.6629\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1268.6262 - val_loss: 1140.2898\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1263.2435 - val_loss: 1143.9021\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1254.1171 - val_loss: 1139.5358\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1251.5893 - val_loss: 1139.6577\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1245.0213 - val_loss: 1138.8009\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1240.6156 - val_loss: 1139.6155\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1234.2733 - val_loss: 1140.3524\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1230.8265 - val_loss: 1135.5823\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1225.1202 - val_loss: 1136.5791\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1217.9498 - val_loss: 1134.5477\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1214.2232 - val_loss: 1133.6068\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1209.1843 - val_loss: 1134.8668\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1207.2334 - val_loss: 1134.1058\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1199.7483 - val_loss: 1134.4171\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1199.8255 - val_loss: 1134.8137\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1194.4734 - val_loss: 1135.1795\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1192.9823 - val_loss: 1136.2403\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1188.4906 - val_loss: 1134.3999\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1186.0999 - val_loss: 1134.2084\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1180.2667 - val_loss: 1135.0918\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 2231.0560 - val_loss: 1221.7923\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1395.5699 - val_loss: 1177.7883\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1363.0087 - val_loss: 1170.3886\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1343.2608 - val_loss: 1167.0487\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 26s 190us/step - loss: 1336.2131 - val_loss: 1155.2089\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1325.5337 - val_loss: 1150.3131\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1316.2327 - val_loss: 1151.8518\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1306.8499 - val_loss: 1148.7234\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1301.9900 - val_loss: 1146.6094\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1295.9935 - val_loss: 1142.4471\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1285.4035 - val_loss: 1141.1082\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1278.8677 - val_loss: 1141.9623\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1274.9436 - val_loss: 1138.5003\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1267.8177 - val_loss: 1140.2103\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1258.7978 - val_loss: 1138.7572\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1255.6999 - val_loss: 1137.6747\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1248.2895 - val_loss: 1135.9723\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1244.3030 - val_loss: 1137.0528\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1236.9265 - val_loss: 1136.1199\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1233.4858 - val_loss: 1136.1362\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1226.6678 - val_loss: 1136.1826\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1224.0757 - val_loss: 1136.9778\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1220.8039 - val_loss: 1136.0552\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1213.6778 - val_loss: 1133.3669\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1210.5224 - val_loss: 1134.4722\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1205.1429 - val_loss: 1137.2578\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1202.8507 - val_loss: 1133.7011\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1196.9191 - val_loss: 1133.9942\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1191.9434 - val_loss: 1134.1313\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1192.0566 - val_loss: 1135.8262\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 2123.1279 - val_loss: 1194.0726\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1365.3409 - val_loss: 1172.1292\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1343.6354 - val_loss: 1163.9114\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1332.9708 - val_loss: 1157.8483\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1326.3108 - val_loss: 1154.4716\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1317.4911 - val_loss: 1149.2828\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1307.9246 - val_loss: 1147.3889\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1305.3285 - val_loss: 1145.6017\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1291.9428 - val_loss: 1146.3004\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 27s 195us/step - loss: 1285.2433 - val_loss: 1143.2673\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1286.2957 - val_loss: 1142.5994\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1272.2372 - val_loss: 1141.1181\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1271.2869 - val_loss: 1142.9535\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1263.1654 - val_loss: 1137.3754\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1256.2174 - val_loss: 1138.6315\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1250.7328 - val_loss: 1137.8397\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1247.3647 - val_loss: 1135.8721\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1244.2701 - val_loss: 1136.4415\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1237.6987 - val_loss: 1138.5873\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1231.7513 - val_loss: 1141.8907\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1226.9613 - val_loss: 1135.7910\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1224.7625 - val_loss: 1134.1653\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1220.6654 - val_loss: 1135.3570\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1215.3763 - val_loss: 1133.1820\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1210.4528 - val_loss: 1135.5808\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1206.5290 - val_loss: 1135.3258\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1205.1399 - val_loss: 1134.1899\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1200.3792 - val_loss: 1134.4549\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1194.2650 - val_loss: 1134.1474\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1191.5147 - val_loss: 1134.7793\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 2315.9363 - val_loss: 1246.3444\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1348.2726 - val_loss: 1172.7584\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1314.1286 - val_loss: 1161.3514\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1296.9086 - val_loss: 1155.2754\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1289.9891 - val_loss: 1155.5279\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1282.2770 - val_loss: 1151.9204\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1274.4245 - val_loss: 1152.0402\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1269.7314 - val_loss: 1145.3519\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1265.4118 - val_loss: 1144.0530\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1258.0639 - val_loss: 1143.2411\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1252.4041 - val_loss: 1140.1012\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1247.7559 - val_loss: 1139.7658\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1242.3785 - val_loss: 1139.0514\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1238.5546 - val_loss: 1139.6671\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 23s 171us/step - loss: 1231.8014 - val_loss: 1138.1341\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1227.1302 - val_loss: 1138.0328\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1221.6375 - val_loss: 1137.2202\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1219.8223 - val_loss: 1136.4811\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1212.0009 - val_loss: 1134.8001\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1209.2380 - val_loss: 1134.7716\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1209.4117 - val_loss: 1138.7590\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1203.2573 - val_loss: 1134.8075\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1200.0183 - val_loss: 1132.0902\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1193.5411 - val_loss: 1134.0624\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1193.8754 - val_loss: 1134.3741\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 23s 167us/step - loss: 1187.8556 - val_loss: 1133.1747\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1183.9901 - val_loss: 1133.3543\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1182.7803 - val_loss: 1134.2723\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1177.2137 - val_loss: 1136.3665\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1172.8399 - val_loss: 1135.2122\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 2119.8819 - val_loss: 1194.4821\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1405.8359 - val_loss: 1182.3650\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1376.1590 - val_loss: 1170.8408\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1361.8107 - val_loss: 1161.8713\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1355.6598 - val_loss: 1159.6668\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1342.6538 - val_loss: 1155.0459\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1331.3761 - val_loss: 1151.4928\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1325.0076 - val_loss: 1155.5556\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1318.8168 - val_loss: 1147.9070\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1314.2236 - val_loss: 1144.6997\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1299.3371 - val_loss: 1143.8599\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1295.4286 - val_loss: 1144.6929\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1289.2017 - val_loss: 1144.5741\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1283.2358 - val_loss: 1143.8941\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1274.6807 - val_loss: 1143.2736\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1269.8257 - val_loss: 1139.9973\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1262.9886 - val_loss: 1141.4330\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1255.3742 - val_loss: 1144.0118\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1252.4933 - val_loss: 1138.9296\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1245.0330 - val_loss: 1140.1186\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1241.4009 - val_loss: 1138.7455\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1228.5252 - val_loss: 1139.0851\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1228.2283 - val_loss: 1138.6091\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1227.9143 - val_loss: 1139.5106\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1214.9725 - val_loss: 1138.2308\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1216.5827 - val_loss: 1138.0127\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1211.1561 - val_loss: 1137.9382\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1202.7174 - val_loss: 1139.1038\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1200.8916 - val_loss: 1138.6964\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1196.2251 - val_loss: 1138.4317\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 2058.7266 - val_loss: 1210.6392\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1362.0712 - val_loss: 1176.4068\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1338.2727 - val_loss: 1163.4924\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1330.0991 - val_loss: 1160.8837\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1321.2615 - val_loss: 1155.2870\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1310.9638 - val_loss: 1153.5328\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1304.6191 - val_loss: 1155.4419\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1296.6437 - val_loss: 1150.9436\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1290.5392 - val_loss: 1147.2804\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1285.1821 - val_loss: 1145.9830\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1275.7951 - val_loss: 1142.8934\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1271.7433 - val_loss: 1146.2561\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1263.2045 - val_loss: 1143.1479\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1257.5321 - val_loss: 1141.0088\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1251.0744 - val_loss: 1140.5256\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 195us/step - loss: 1247.0277 - val_loss: 1140.3327\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1244.1866 - val_loss: 1139.8641\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1237.4472 - val_loss: 1139.6823\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1234.8529 - val_loss: 1138.8476\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1227.5581 - val_loss: 1141.2740\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1224.7067 - val_loss: 1141.1454\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1220.5128 - val_loss: 1140.5297\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1214.9180 - val_loss: 1139.1980\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1210.3987 - val_loss: 1139.0360\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 27s 196us/step - loss: 1208.4064 - val_loss: 1138.6302\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1202.7660 - val_loss: 1139.9994\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1200.5558 - val_loss: 1139.0949\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1196.5490 - val_loss: 1137.6133\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1192.6623 - val_loss: 1136.8856\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1187.4095 - val_loss: 1139.3184\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 2143.3225 - val_loss: 1223.0584\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1396.6574 - val_loss: 1176.8533\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1362.8177 - val_loss: 1171.1728\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1352.2580 - val_loss: 1160.1845\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1338.7341 - val_loss: 1156.3071\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1330.7281 - val_loss: 1151.7455\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1319.6216 - val_loss: 1149.9891\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1312.7453 - val_loss: 1146.2460\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1301.5571 - val_loss: 1145.8570\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1299.5234 - val_loss: 1144.4651\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1292.1326 - val_loss: 1142.1866\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1282.6187 - val_loss: 1140.7201\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1278.4729 - val_loss: 1140.6687\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1274.6750 - val_loss: 1143.2288\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1263.4349 - val_loss: 1140.1012\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1257.2066 - val_loss: 1138.7968\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 195us/step - loss: 1252.1870 - val_loss: 1138.5492\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1247.3475 - val_loss: 1137.0383\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1241.2335 - val_loss: 1139.3306\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1236.2495 - val_loss: 1137.6008\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1230.1871 - val_loss: 1138.5061\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1227.0297 - val_loss: 1140.1835\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1219.5007 - val_loss: 1137.4734\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1216.9567 - val_loss: 1138.1158\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1214.3305 - val_loss: 1138.6148\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1205.9531 - val_loss: 1137.6241\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1204.0271 - val_loss: 1139.6345\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1197.0604 - val_loss: 1136.9987\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 173us/step - loss: 1192.0726 - val_loss: 1136.9450\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1191.0919 - val_loss: 1139.2107\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 2229.3819 - val_loss: 1222.5681\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1385.2633 - val_loss: 1174.7287\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1353.9616 - val_loss: 1167.1850\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1343.3512 - val_loss: 1160.1881\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1331.1470 - val_loss: 1155.9693\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1320.1535 - val_loss: 1152.5035\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1314.4057 - val_loss: 1151.3282\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 23s 172us/step - loss: 1308.6014 - val_loss: 1147.7081\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1301.1007 - val_loss: 1146.7121\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1293.5914 - val_loss: 1144.6988\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1284.9942 - val_loss: 1144.8881\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1279.3222 - val_loss: 1144.8622\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1272.4146 - val_loss: 1142.3985\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1267.5382 - val_loss: 1144.1635\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 24s 173us/step - loss: 1260.8431 - val_loss: 1141.3549\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1259.8891 - val_loss: 1141.8456\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1252.1426 - val_loss: 1145.1915\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1244.4759 - val_loss: 1138.0828\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1245.7835 - val_loss: 1139.5605\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1234.7230 - val_loss: 1140.1163\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1231.5912 - val_loss: 1138.4347\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1223.8864 - val_loss: 1137.8257\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1217.6875 - val_loss: 1140.1399\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1217.1261 - val_loss: 1139.8565\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1210.8337 - val_loss: 1139.4166\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1208.0396 - val_loss: 1140.3689\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1199.5468 - val_loss: 1140.9729\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1197.7481 - val_loss: 1140.5967\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1197.2517 - val_loss: 1140.5268\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1187.4950 - val_loss: 1140.5959\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 2117.7509 - val_loss: 1188.3820\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1351.2629 - val_loss: 1171.5252\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1330.6674 - val_loss: 1161.2257\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 27s 197us/step - loss: 1320.9553 - val_loss: 1162.2288\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1313.7769 - val_loss: 1158.2093\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1307.6909 - val_loss: 1154.3673\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1302.4767 - val_loss: 1148.5886\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1292.9136 - val_loss: 1146.9842\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1288.4818 - val_loss: 1146.7981\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1282.4441 - val_loss: 1143.7657\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1275.0979 - val_loss: 1142.5530\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1272.1019 - val_loss: 1143.6944\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1265.5457 - val_loss: 1142.1469\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1259.6254 - val_loss: 1141.6474\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1254.4519 - val_loss: 1139.2320\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1246.2412 - val_loss: 1139.9090\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1247.6035 - val_loss: 1139.8621\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1238.4679 - val_loss: 1140.2654\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1238.4043 - val_loss: 1139.5112\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1230.1526 - val_loss: 1137.8408\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1227.1184 - val_loss: 1138.4272\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1221.6110 - val_loss: 1137.6395\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1217.8624 - val_loss: 1137.6166\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1213.0633 - val_loss: 1137.1708\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1210.6226 - val_loss: 1137.9935\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1203.8849 - val_loss: 1138.4474\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1198.7497 - val_loss: 1137.3018\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1199.8873 - val_loss: 1138.1491\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1196.1182 - val_loss: 1138.4718\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1189.8646 - val_loss: 1136.9649\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 2210.7753 - val_loss: 1194.5046\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1367.2779 - val_loss: 1173.4402\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1334.9072 - val_loss: 1162.8977\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1324.8923 - val_loss: 1157.6844\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1315.7341 - val_loss: 1154.1269\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1306.2067 - val_loss: 1151.1058\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1298.4695 - val_loss: 1148.4885\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1290.5954 - val_loss: 1145.9251\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1288.6569 - val_loss: 1148.5169\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1280.2974 - val_loss: 1145.1625\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1275.2360 - val_loss: 1142.9207\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1268.5319 - val_loss: 1141.8622\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1260.2250 - val_loss: 1143.3668\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1256.2468 - val_loss: 1139.9396\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1250.7697 - val_loss: 1139.1088\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1246.1969 - val_loss: 1140.7155\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1242.9728 - val_loss: 1140.4807\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1237.3649 - val_loss: 1140.4696\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1233.2380 - val_loss: 1139.7548\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1234.0182 - val_loss: 1138.4653\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1226.6939 - val_loss: 1138.6684\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1219.0730 - val_loss: 1139.4863\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1218.3039 - val_loss: 1141.1735\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1213.2167 - val_loss: 1139.0437\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1204.9375 - val_loss: 1138.3296\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1202.9025 - val_loss: 1139.3475\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1200.6418 - val_loss: 1138.8130\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1196.0235 - val_loss: 1139.6944\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1189.9345 - val_loss: 1144.0235\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1190.7949 - val_loss: 1139.2755\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 2208.8805 - val_loss: 1217.4072\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1438.8807 - val_loss: 1186.4877\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1406.8743 - val_loss: 1173.2867\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1394.3292 - val_loss: 1164.4795\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1373.9850 - val_loss: 1156.5272\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1361.4980 - val_loss: 1159.4360\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1351.3763 - val_loss: 1151.3070\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1343.2142 - val_loss: 1149.1684\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1330.7210 - val_loss: 1148.1963\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1326.4153 - val_loss: 1148.2704\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1312.2252 - val_loss: 1142.5857\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1303.4366 - val_loss: 1142.4458\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1290.7794 - val_loss: 1143.3750\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 26s 193us/step - loss: 1285.3136 - val_loss: 1141.8669\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1277.7448 - val_loss: 1141.2003\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1268.6298 - val_loss: 1138.0944\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1259.6569 - val_loss: 1140.4263\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1258.0721 - val_loss: 1139.2137\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1246.5468 - val_loss: 1136.7053\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1242.2924 - val_loss: 1138.9585\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1233.3920 - val_loss: 1138.4483\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1229.4082 - val_loss: 1138.7144\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1222.3051 - val_loss: 1136.2622\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1218.4511 - val_loss: 1135.8634\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1213.2927 - val_loss: 1136.9935\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1210.7032 - val_loss: 1137.4692\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1201.8186 - val_loss: 1134.8936\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1198.5790 - val_loss: 1136.7928\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1196.1293 - val_loss: 1136.4506\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1187.2432 - val_loss: 1138.2078\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 2225.0285 - val_loss: 1226.8992\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1379.2322 - val_loss: 1179.3933\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1330.0514 - val_loss: 1165.0979\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1316.5846 - val_loss: 1161.0115\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1310.2529 - val_loss: 1154.2001\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1301.6538 - val_loss: 1150.5695\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1292.6943 - val_loss: 1147.5347\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1289.1383 - val_loss: 1147.1736\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1280.3895 - val_loss: 1146.0002\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1274.3101 - val_loss: 1146.4013\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1269.0429 - val_loss: 1143.5017\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1261.9823 - val_loss: 1140.9196\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1255.2607 - val_loss: 1139.8235\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1250.0769 - val_loss: 1141.3495\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1244.9609 - val_loss: 1141.1079\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1240.2968 - val_loss: 1138.2292\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1232.7281 - val_loss: 1138.6211\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1227.7774 - val_loss: 1138.1282\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1226.0016 - val_loss: 1137.4584\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1223.4699 - val_loss: 1136.4774\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1218.4343 - val_loss: 1136.9504\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1211.3894 - val_loss: 1139.2673\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1211.3454 - val_loss: 1137.4087\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1204.4977 - val_loss: 1138.8977\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1199.6846 - val_loss: 1137.5056\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1198.4386 - val_loss: 1138.8720\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1196.4151 - val_loss: 1138.0305\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1194.6859 - val_loss: 1137.6819\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1186.0152 - val_loss: 1137.3684\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 203us/step - loss: 1183.5068 - val_loss: 1136.8684\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 2166.1662 - val_loss: 1211.1478\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1416.5839 - val_loss: 1181.4985\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1375.0309 - val_loss: 1168.2798\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1360.0407 - val_loss: 1164.5965\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1343.6309 - val_loss: 1156.4169\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1336.7691 - val_loss: 1153.4500\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 27s 199us/step - loss: 1324.7019 - val_loss: 1150.8049\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1322.1867 - val_loss: 1148.5812\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1307.3589 - val_loss: 1147.6221\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1302.0814 - val_loss: 1147.2628\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1294.0592 - val_loss: 1144.2332\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 27s 198us/step - loss: 1286.9429 - val_loss: 1144.1646\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1276.2711 - val_loss: 1142.1613\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1271.3867 - val_loss: 1143.6357\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1264.2764 - val_loss: 1140.1884\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1260.9044 - val_loss: 1140.8612\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 27s 197us/step - loss: 1251.9465 - val_loss: 1140.2528\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1248.7042 - val_loss: 1143.9852\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1245.9201 - val_loss: 1139.6576\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1237.9901 - val_loss: 1140.1635\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1231.2807 - val_loss: 1139.7014\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1225.2963 - val_loss: 1138.2412\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1221.1284 - val_loss: 1138.9125\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 28s 207us/step - loss: 1214.3793 - val_loss: 1137.7560\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1211.4282 - val_loss: 1138.3809\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1205.7220 - val_loss: 1137.4328\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1198.5535 - val_loss: 1140.2237\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 210us/step - loss: 1199.0898 - val_loss: 1139.3490\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1195.3664 - val_loss: 1137.6377\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1188.1626 - val_loss: 1138.4660\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 2110.6237 - val_loss: 1202.9859\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1321.5487 - val_loss: 1169.3168\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1295.5121 - val_loss: 1163.0227\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1291.1214 - val_loss: 1154.8265\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1284.5286 - val_loss: 1155.7259\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1276.3342 - val_loss: 1150.3673\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1273.3806 - val_loss: 1145.7933\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1266.3195 - val_loss: 1148.9002\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 27s 200us/step - loss: 1261.6914 - val_loss: 1143.4361\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1254.6408 - val_loss: 1144.9004\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1251.4528 - val_loss: 1144.3901\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1246.9110 - val_loss: 1140.7811\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1241.6050 - val_loss: 1140.4449\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1238.0881 - val_loss: 1140.3977\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1234.0017 - val_loss: 1140.4056\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 27s 201us/step - loss: 1227.2950 - val_loss: 1139.2909\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1226.4265 - val_loss: 1138.1376\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1222.1350 - val_loss: 1138.8979\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1218.0611 - val_loss: 1136.9313\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1214.4086 - val_loss: 1136.2844\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1212.1309 - val_loss: 1137.1798\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1208.0192 - val_loss: 1139.1303\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1201.6307 - val_loss: 1139.0193\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1201.2602 - val_loss: 1136.3630\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1196.1595 - val_loss: 1138.8439\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1194.1919 - val_loss: 1137.1678\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 29s 210us/step - loss: 1189.8928 - val_loss: 1141.2983\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1186.3237 - val_loss: 1137.3094\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1185.3166 - val_loss: 1140.4204\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1183.4705 - val_loss: 1138.0533\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 31s 232us/step - loss: 2478.3934 - val_loss: 1250.9345\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1299.9280 - val_loss: 1178.4042\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1272.3376 - val_loss: 1165.5807\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1259.4314 - val_loss: 1159.9623\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1252.1883 - val_loss: 1154.0606\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1250.1511 - val_loss: 1150.0897\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1241.1078 - val_loss: 1149.8771\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1236.4914 - val_loss: 1148.5137\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1230.5231 - val_loss: 1147.9498\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1226.6566 - val_loss: 1148.5952\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1220.4389 - val_loss: 1142.8652\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 31s 225us/step - loss: 1217.0519 - val_loss: 1143.8728\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1217.7694 - val_loss: 1145.0589\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1210.8607 - val_loss: 1141.1218\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1206.8329 - val_loss: 1141.4598\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 31s 225us/step - loss: 1205.6875 - val_loss: 1139.3641\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1202.1325 - val_loss: 1141.2275\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1197.3062 - val_loss: 1139.5277\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1194.5207 - val_loss: 1141.0751\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1189.0660 - val_loss: 1140.0018\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1187.2046 - val_loss: 1139.4508\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1185.3288 - val_loss: 1141.2821\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1181.0768 - val_loss: 1138.6269\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1177.4975 - val_loss: 1139.4061\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1176.8418 - val_loss: 1138.3612\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1172.7958 - val_loss: 1139.5065\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1169.3915 - val_loss: 1139.8714\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1170.1606 - val_loss: 1141.1851\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1164.9770 - val_loss: 1140.8300\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1162.3168 - val_loss: 1143.9714\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 2213.2782 - val_loss: 1204.0083\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1359.5050 - val_loss: 1175.7657\n",
      "Epoch 3/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 29s 210us/step - loss: 1340.6361 - val_loss: 1170.0717\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1329.2369 - val_loss: 1164.9789\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 27s 202us/step - loss: 1319.7367 - val_loss: 1157.2249\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1313.9019 - val_loss: 1158.8802\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1306.4386 - val_loss: 1156.8475\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1300.3337 - val_loss: 1150.6682\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 29s 210us/step - loss: 1293.7483 - val_loss: 1152.3449\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1292.8610 - val_loss: 1148.3129\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1284.3098 - val_loss: 1146.0417\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 28s 203us/step - loss: 1275.4350 - val_loss: 1146.2212\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1269.5617 - val_loss: 1148.8915\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1261.4161 - val_loss: 1145.8265\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1254.7872 - val_loss: 1145.3940\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1254.7638 - val_loss: 1145.3468\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1250.3471 - val_loss: 1142.2477\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1248.9515 - val_loss: 1146.2376\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1239.6966 - val_loss: 1142.6859\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1236.8208 - val_loss: 1144.4479\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1229.1412 - val_loss: 1140.5348\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1225.6267 - val_loss: 1141.7637\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1222.4746 - val_loss: 1141.1830\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1215.7486 - val_loss: 1141.1086\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 28s 204us/step - loss: 1214.2153 - val_loss: 1143.4635\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1209.8692 - val_loss: 1143.4942\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1201.1119 - val_loss: 1140.3715\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1201.5538 - val_loss: 1140.5487\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1196.2482 - val_loss: 1140.8551\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1195.5310 - val_loss: 1141.6465\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 2403.4611 - val_loss: 1222.5932\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1350.6097 - val_loss: 1181.7151\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1315.1078 - val_loss: 1172.4174\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1302.6074 - val_loss: 1160.3553\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 28s 205us/step - loss: 1293.2152 - val_loss: 1155.1906\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1285.9067 - val_loss: 1153.5984\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1279.9027 - val_loss: 1152.6726\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1273.8241 - val_loss: 1150.8581\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1265.2476 - val_loss: 1147.3254\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1259.3178 - val_loss: 1147.0026\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1256.3874 - val_loss: 1146.8509\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1248.0055 - val_loss: 1144.7638\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1247.4970 - val_loss: 1146.5832\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1236.9143 - val_loss: 1146.4789\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1234.0068 - val_loss: 1142.9681\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1228.1752 - val_loss: 1142.3401\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1223.8849 - val_loss: 1140.9063\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1221.5401 - val_loss: 1139.6206\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1216.0960 - val_loss: 1139.6875\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1214.0507 - val_loss: 1139.9570\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1204.2745 - val_loss: 1144.6685\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 28s 207us/step - loss: 1205.4736 - val_loss: 1140.5850\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1199.6498 - val_loss: 1140.6716\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1196.3599 - val_loss: 1138.4945\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1193.9488 - val_loss: 1139.9082\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1190.0329 - val_loss: 1140.9365\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1187.8683 - val_loss: 1140.4057\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1183.7084 - val_loss: 1140.8115\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1176.9835 - val_loss: 1141.9920\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1176.0988 - val_loss: 1139.4715\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 2206.9705 - val_loss: 1224.3242\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1420.9081 - val_loss: 1189.9901\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1381.0591 - val_loss: 1183.4847\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1361.9225 - val_loss: 1163.0760\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1352.9526 - val_loss: 1156.9602\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1340.6881 - val_loss: 1156.2331\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 30s 225us/step - loss: 1328.0171 - val_loss: 1158.6738\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1321.4901 - val_loss: 1151.1068\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1306.6787 - val_loss: 1150.4987\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1304.8293 - val_loss: 1148.0075\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1292.0210 - val_loss: 1148.2700\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1289.1873 - val_loss: 1145.2074\n",
      "Epoch 13/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 29s 210us/step - loss: 1280.3386 - val_loss: 1145.4167\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1274.2175 - val_loss: 1144.0480\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 28s 206us/step - loss: 1266.6754 - val_loss: 1144.4092\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1255.7963 - val_loss: 1145.6502\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1256.7863 - val_loss: 1142.7322\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1245.5386 - val_loss: 1141.9579\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1240.0401 - val_loss: 1141.3946\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 29s 210us/step - loss: 1234.6653 - val_loss: 1141.5920\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1227.4265 - val_loss: 1141.9765\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1225.8812 - val_loss: 1140.8832\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1219.6962 - val_loss: 1140.9574\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1213.2723 - val_loss: 1139.9243\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1206.8629 - val_loss: 1138.6005\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1205.3950 - val_loss: 1139.6748\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1200.0933 - val_loss: 1139.2768\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1196.3112 - val_loss: 1140.5089\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1191.7802 - val_loss: 1140.5166\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1191.3796 - val_loss: 1138.9885\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 2334.5562 - val_loss: 1207.2376\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1345.5222 - val_loss: 1181.2840\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1307.5086 - val_loss: 1168.1299\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1295.8659 - val_loss: 1159.4075\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1288.6547 - val_loss: 1163.1556\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1277.4811 - val_loss: 1153.2618\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1270.3106 - val_loss: 1151.6025\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1266.1365 - val_loss: 1150.4470\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1256.9934 - val_loss: 1149.1967\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 29s 210us/step - loss: 1254.1280 - val_loss: 1147.1384\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1250.5286 - val_loss: 1144.6311\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1239.5930 - val_loss: 1145.2959\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1240.5103 - val_loss: 1144.4882\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1231.0763 - val_loss: 1143.1312\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1229.3477 - val_loss: 1143.4941\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1224.1102 - val_loss: 1139.9562\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1218.9078 - val_loss: 1141.0352\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 28s 208us/step - loss: 1215.3486 - val_loss: 1140.5115\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1207.3102 - val_loss: 1142.4131\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1208.9546 - val_loss: 1141.4172\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1202.3730 - val_loss: 1140.0183\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1203.7813 - val_loss: 1142.2081\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1197.8011 - val_loss: 1139.9729\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1193.1325 - val_loss: 1138.6469\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 29s 210us/step - loss: 1191.9483 - val_loss: 1138.6611\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1187.6633 - val_loss: 1140.6833\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1182.3336 - val_loss: 1141.1480\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 28s 210us/step - loss: 1181.2056 - val_loss: 1140.7481\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1176.0020 - val_loss: 1141.5913\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1173.4360 - val_loss: 1141.4053\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 2160.4651 - val_loss: 1200.2200\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1331.4129 - val_loss: 1174.7328\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1309.0994 - val_loss: 1164.4652\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1296.3830 - val_loss: 1162.8591\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1289.7116 - val_loss: 1158.9441\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 28s 209us/step - loss: 1280.1790 - val_loss: 1158.0851\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1272.2469 - val_loss: 1153.1010\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1271.6794 - val_loss: 1151.0992\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1264.4960 - val_loss: 1149.8742\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1256.2898 - val_loss: 1148.9000\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1252.3428 - val_loss: 1148.4815\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1246.4871 - val_loss: 1146.5995\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1244.6982 - val_loss: 1145.9646\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1239.5235 - val_loss: 1144.6541\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1234.4907 - val_loss: 1142.1808\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1233.3149 - val_loss: 1143.7115\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1224.2986 - val_loss: 1141.7627\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1221.3283 - val_loss: 1142.7937\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 29s 213us/step - loss: 1217.9708 - val_loss: 1141.1199\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 29s 211us/step - loss: 1214.6711 - val_loss: 1140.2275\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1210.4642 - val_loss: 1141.3591\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 30s 225us/step - loss: 1208.1021 - val_loss: 1143.7375\n",
      "Epoch 23/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 31s 225us/step - loss: 1203.6233 - val_loss: 1142.2702\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 29s 212us/step - loss: 1199.4590 - val_loss: 1140.0724\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1198.5694 - val_loss: 1140.5301\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1190.4251 - val_loss: 1139.9148\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1191.6103 - val_loss: 1142.1868\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1188.5767 - val_loss: 1142.0126\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1187.0146 - val_loss: 1141.1724\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 29s 218us/step - loss: 1181.6518 - val_loss: 1139.8911\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 2324.8215 - val_loss: 1217.0995\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1331.5356 - val_loss: 1174.0434\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1295.6031 - val_loss: 1164.1909\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1282.0627 - val_loss: 1161.1586\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1273.3151 - val_loss: 1156.6547\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1266.0513 - val_loss: 1152.1007\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1259.7512 - val_loss: 1150.3306\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1250.8541 - val_loss: 1151.2271\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1248.6749 - val_loss: 1147.5978\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1240.8989 - val_loss: 1147.2023\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1238.8499 - val_loss: 1144.9031\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1233.6482 - val_loss: 1143.7884\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1226.0827 - val_loss: 1145.2968\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1225.5408 - val_loss: 1142.0533\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1222.3210 - val_loss: 1141.6121\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 31s 225us/step - loss: 1216.5962 - val_loss: 1141.1985\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1213.9877 - val_loss: 1141.8926\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 31s 226us/step - loss: 1206.2670 - val_loss: 1142.4656\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 31s 229us/step - loss: 1204.5777 - val_loss: 1139.4351\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 1197.4027 - val_loss: 1140.3406\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 31s 225us/step - loss: 1196.8437 - val_loss: 1140.6146\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 31s 228us/step - loss: 1193.2059 - val_loss: 1141.2834\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 31s 226us/step - loss: 1191.4510 - val_loss: 1139.1829\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1185.2538 - val_loss: 1141.7775\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1182.4704 - val_loss: 1139.4202\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1178.9861 - val_loss: 1141.4880\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1179.9553 - val_loss: 1140.8881\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1172.9378 - val_loss: 1143.0132\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1169.8538 - val_loss: 1140.6581\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 31s 226us/step - loss: 1166.5785 - val_loss: 1141.8940\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 32s 232us/step - loss: 2106.5312 - val_loss: 1231.1062\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 31s 226us/step - loss: 1381.8717 - val_loss: 1178.2751\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1363.3448 - val_loss: 1173.3056\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1350.8161 - val_loss: 1165.8350\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1341.2278 - val_loss: 1165.0612\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1329.9282 - val_loss: 1160.5590\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 31s 225us/step - loss: 1320.6513 - val_loss: 1151.9352\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1315.0017 - val_loss: 1152.7783\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1305.9533 - val_loss: 1153.8096\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1304.4440 - val_loss: 1147.2299\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1293.5937 - val_loss: 1149.4166\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1286.4331 - val_loss: 1147.4327\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1283.9948 - val_loss: 1147.3683\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1273.9287 - val_loss: 1144.2738\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 31s 226us/step - loss: 1267.1973 - val_loss: 1144.6047\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 31s 228us/step - loss: 1263.2738 - val_loss: 1142.8252\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1256.9276 - val_loss: 1142.0846\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1247.1035 - val_loss: 1145.0365\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1246.1464 - val_loss: 1142.5914\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1238.9349 - val_loss: 1143.2218\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1232.2035 - val_loss: 1138.4174\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1231.0563 - val_loss: 1140.3167\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1222.0422 - val_loss: 1139.9853\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1217.3026 - val_loss: 1143.5889\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1213.6760 - val_loss: 1141.5202\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 30s 225us/step - loss: 1211.3735 - val_loss: 1139.9517\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 1203.2496 - val_loss: 1143.5735\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 31s 230us/step - loss: 1200.3362 - val_loss: 1140.2287\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1196.7272 - val_loss: 1141.4907\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1192.4644 - val_loss: 1140.8385\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 31s 231us/step - loss: 2111.6631 - val_loss: 1222.8816\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 30s 222us/step - loss: 1394.5234 - val_loss: 1179.9220\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1371.7671 - val_loss: 1176.8228\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 31s 228us/step - loss: 1359.1613 - val_loss: 1169.8034\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 32s 232us/step - loss: 1350.1370 - val_loss: 1159.8024\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 29s 216us/step - loss: 1340.2194 - val_loss: 1161.9867\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1325.5196 - val_loss: 1154.4028\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 31s 232us/step - loss: 1320.4781 - val_loss: 1151.5985\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 1311.8714 - val_loss: 1151.7297\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1303.0771 - val_loss: 1148.5335\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1298.8945 - val_loss: 1149.4555\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1290.1170 - val_loss: 1151.8954\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1281.5881 - val_loss: 1146.1223\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1274.2382 - val_loss: 1145.2511\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1270.8506 - val_loss: 1146.6479\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1262.0257 - val_loss: 1142.8103\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 30s 219us/step - loss: 1261.9970 - val_loss: 1143.1413\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1254.7487 - val_loss: 1144.4214\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1246.7229 - val_loss: 1142.1153\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1242.1188 - val_loss: 1142.2357\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1236.4142 - val_loss: 1143.3833\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1228.5802 - val_loss: 1140.2620\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 30s 218us/step - loss: 1223.3584 - val_loss: 1142.4757\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1217.6442 - val_loss: 1143.1571\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1209.8272 - val_loss: 1141.9688\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1211.5236 - val_loss: 1140.9254\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1206.3368 - val_loss: 1140.4117\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1202.0728 - val_loss: 1141.7579\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 30s 221us/step - loss: 1194.0182 - val_loss: 1141.2428\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1189.4503 - val_loss: 1141.5842\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 32s 238us/step - loss: 2065.2054 - val_loss: 1201.6887\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 31s 226us/step - loss: 1362.7118 - val_loss: 1176.3110\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 30s 222us/step - loss: 1343.8090 - val_loss: 1169.6035\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 29s 214us/step - loss: 1330.7180 - val_loss: 1160.8279\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 29s 218us/step - loss: 1326.2812 - val_loss: 1160.0577\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 1315.8941 - val_loss: 1155.1889\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1305.4850 - val_loss: 1153.6724\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1298.5302 - val_loss: 1156.4198\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 29s 215us/step - loss: 1295.3976 - val_loss: 1150.0607\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 30s 225us/step - loss: 1288.8976 - val_loss: 1147.9945\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 30s 224us/step - loss: 1277.0573 - val_loss: 1148.1765\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 31s 230us/step - loss: 1270.7507 - val_loss: 1149.1229\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1264.3006 - val_loss: 1145.2700\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 30s 223us/step - loss: 1261.5320 - val_loss: 1146.6106\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 31s 227us/step - loss: 1253.5686 - val_loss: 1144.6155\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 31s 232us/step - loss: 1250.5707 - val_loss: 1140.9040\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 31s 230us/step - loss: 1249.8867 - val_loss: 1141.0310\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 30s 225us/step - loss: 1237.9784 - val_loss: 1141.2939\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 30s 225us/step - loss: 1237.9494 - val_loss: 1144.0555\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1230.4504 - val_loss: 1142.1979\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 29s 217us/step - loss: 1226.2212 - val_loss: 1141.4618\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 30s 220us/step - loss: 1218.5283 - val_loss: 1141.4514\n",
      "Epoch 23/30\n",
      " 52096/135588 [==========>...................] - ETA: 18s - loss: 1214.9054"
     ]
    }
   ],
   "source": [
    "cv_score = cross_validate_mlp(hyper_model)\n",
    "print (\"CV score for the final model:\", cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, \n",
    " 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta',\n",
    " 'wdecay': 0.0020300000000000001}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'l3-nn': {\n",
    "        'predictions': l2_predictions,\n",
    "        'n_bags': 4,\n",
    "        'model': Keras(nn_lr, lambda: {'l1': 1e-5, 'l2': 1e-5, 'n_epoch': 30, 'batch_size': 128, 'optimizer': SGD(3e-2, momentum=0.8, nesterov=True, decay=3e-5), 'callbacks': [ExponentialMovingAverage(save_mv_ave_model=False)]}),\n",
    "    },"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.2, batch_size=128, \n",
    "                      nb_epoch=30, verbose=0, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=256)\n",
    "        score = mean_absolute_error(yte, pred)\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 455, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 272, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 69, 'optimizer': 'adadelta', 'wdecay': 0.032330000000000005}\n",
      "Fold 0, MAE: 1163.148839039124\n",
      "Fold 1, MAE: 1159.9899238609737\n",
      "Fold 2, MAE: 1140.9771315408389\n",
      "3-fold CV score: 1154.7052981469787\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 541, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 134, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 30, 'optimizer': 'adadelta', 'wdecay': 0.082830000000000001}\n",
      "Fold 0, MAE: 1181.2090776051534\n",
      "Fold 1, MAE: 1175.7242010975745\n",
      "Fold 2, MAE: 1149.1617817128606\n",
      "3-fold CV score: 1168.698353471863\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 541, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 113, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 22, 'optimizer': 'adam', 'wdecay': 0.010110000000000001}\n",
      "Fold 0, MAE: 1161.0922305512295\n",
      "Fold 1, MAE: 1171.8751691438747\n",
      "Fold 2, MAE: 1142.5130553590734\n",
      "3-fold CV score: 1158.4934850180591\n",
      "Model Testing: {'hidden1_dropout': 0.56842105263157894, 'hidden1_units': 351, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 168, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 44, 'optimizer': 'adam', 'wdecay': 0.066669999999999993}\n",
      "Fold 0, MAE: 1175.3901692187044\n",
      "Fold 1, MAE: 1176.3282762184622\n",
      "Fold 2, MAE: 1171.493253623609\n",
      "3-fold CV score: 1174.4038996869251\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 351, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 237, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.0040499999999999998}\n",
      "Fold 0, MAE: 1167.663476455973\n",
      "Fold 1, MAE: 1158.721241361386\n",
      "Fold 2, MAE: 1148.2196995976833\n",
      "3-fold CV score: 1158.2014724716807\n",
      "Model Testing: {'hidden1_dropout': 0.45263157894736844, 'hidden1_units': 437, 'hidden2_dropout': 0.5, 'hidden2_units': 120, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 30, 'optimizer': 'adam', 'wdecay': 0.055560000000000005}\n",
      "Fold 0, MAE: 1192.865180594892\n",
      "Fold 1, MAE: 1170.481112811505\n",
      "Fold 2, MAE: 1171.0399158641085\n",
      "3-fold CV score: 1178.128736423502\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 368, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 237, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 34, 'optimizer': 'adadelta', 'wdecay': 0.082830000000000001}\n",
      "Fold 0, MAE: 1163.6607907634843\n",
      "Fold 1, MAE: 1170.6899247157944\n",
      "Fold 2, MAE: 1149.6886504229296\n",
      "3-fold CV score: 1161.3464553007361\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 394, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 217, 'hidden3_dropout': 0.5, 'hidden3_units': 26, 'optimizer': 'adadelta', 'wdecay': 0.086870000000000003}\n",
      "Fold 0, MAE: 1186.010471694171\n",
      "Fold 1, MAE: 1194.6887222751257\n",
      "Fold 2, MAE: 1178.986956053622\n",
      "3-fold CV score: 1186.5620500076395\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 550, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 175, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 30, 'optimizer': 'adadelta', 'wdecay': 0.054550000000000008}\n",
      "Fold 0, MAE: 1173.4874602173836\n",
      "Fold 1, MAE: 1168.5583186899196\n",
      "Fold 2, MAE: 1157.0938108456683\n",
      "3-fold CV score: 1166.3798632509906\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 420, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 272, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 20, 'optimizer': 'adam', 'wdecay': 0.051520000000000003}\n",
      "Fold 0, MAE: 1187.3547460026052\n",
      "Fold 1, MAE: 1165.0640505257604\n",
      "Fold 2, MAE: 1152.0578003723883\n",
      "3-fold CV score: 1168.1588656335846\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 506, 'hidden2_dropout': 0.5, 'hidden2_units': 155, 'hidden3_dropout': 0.5, 'hidden3_units': 38, 'optimizer': 'adam', 'wdecay': 0.0030400000000000002}\n",
      "Fold 0, MAE: 1165.5917111922117\n",
      "Fold 1, MAE: 1157.0224820727203\n",
      "Fold 2, MAE: 1145.9990145378135\n",
      "3-fold CV score: 1156.2044026009153\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 550, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 217, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 46, 'optimizer': 'adadelta', 'wdecay': 0.053540000000000004}\n",
      "Fold 0, MAE: 1178.285182548254\n",
      "Fold 1, MAE: 1161.227306544953\n",
      "Fold 2, MAE: 1146.3369486137735\n",
      "3-fold CV score: 1161.9498125689936\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 308, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 100, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 48, 'optimizer': 'adadelta', 'wdecay': 0.083839999999999998}\n",
      "Fold 0, MAE: 1175.8002146167726\n",
      "Fold 1, MAE: 1172.0227174765078\n",
      "Fold 2, MAE: 1154.2503788092345\n",
      "3-fold CV score: 1167.3577703008384\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 446, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 217, 'hidden3_dropout': 0.5, 'hidden3_units': 22, 'optimizer': 'adadelta', 'wdecay': 0.089899999999999994}\n",
      "Fold 0, MAE: 1198.0498968117809\n",
      "Fold 1, MAE: 1192.423294968826\n",
      "Fold 2, MAE: 1191.1537727940815\n",
      "3-fold CV score: 1193.8756548582294\n",
      "Model Testing: {'hidden1_dropout': 0.40000000000000002, 'hidden1_units': 541, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 258, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 75, 'optimizer': 'adadelta', 'wdecay': 0.02324}\n",
      "Fold 0, MAE: 1162.2846737751456\n",
      "Fold 1, MAE: 1157.0434973514984\n",
      "Fold 2, MAE: 1149.8406610896955\n",
      "3-fold CV score: 1156.3896107387798\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 412, 'hidden2_dropout': 0.5, 'hidden2_units': 203, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 24, 'optimizer': 'adadelta', 'wdecay': 0.065659999999999996}\n",
      "Fold 0, MAE: 1162.6868081304262\n",
      "Fold 1, MAE: 1207.019499323167\n",
      "Fold 2, MAE: 1164.6480082339472\n",
      "3-fold CV score: 1178.1181052291802\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 386, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 113, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 44, 'optimizer': 'adam', 'wdecay': 0.01213}\n",
      "Fold 0, MAE: 1162.20426789193\n",
      "Fold 1, MAE: 1165.871551874401\n",
      "Fold 2, MAE: 1144.635762708466\n",
      "3-fold CV score: 1157.570527491599\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 489, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 272, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 42, 'optimizer': 'adadelta', 'wdecay': 0.047480000000000008}\n",
      "Fold 0, MAE: 1159.1230678129568\n",
      "Fold 1, MAE: 1170.1456261211185\n",
      "Fold 2, MAE: 1141.1387421620404\n",
      "3-fold CV score: 1156.8024786987053\n",
      "Model Testing: {'hidden1_dropout': 0.47368421052631582, 'hidden1_units': 368, 'hidden2_dropout': 0.5, 'hidden2_units': 196, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 61, 'optimizer': 'adam', 'wdecay': 0.076770000000000005}\n",
      "Fold 0, MAE: 1167.3144952524326\n",
      "Fold 1, MAE: 1158.442539164428\n",
      "Fold 2, MAE: 1170.1439974149687\n",
      "3-fold CV score: 1165.3003439439433\n",
      "Model Testing: {'hidden1_dropout': 0.57894736842105265, 'hidden1_units': 325, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.070709999999999995}\n",
      "Fold 0, MAE: 1161.177618771013\n",
      "Fold 1, MAE: 1157.4865109404855\n",
      "Fold 2, MAE: 1143.5037899814306\n",
      "3-fold CV score: 1154.0559732309764\n",
      "Model Testing: {'hidden1_dropout': 0.58947368421052637, 'hidden1_units': 325, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 148, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 69, 'optimizer': 'adadelta', 'wdecay': 0.070709999999999995}\n",
      "Fold 0, MAE: 1169.0057906375498\n",
      "Fold 1, MAE: 1163.4061472146307\n",
      "Fold 2, MAE: 1150.80821222944\n",
      "3-fold CV score: 1161.0733833605402\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 455, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.032330000000000005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, MAE: 1160.1148852933434\n",
      "Fold 1, MAE: 1153.5388360438872\n",
      "Fold 2, MAE: 1146.284142179796\n",
      "3-fold CV score: 1153.312621172342\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 325, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 258, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.058590000000000003}\n",
      "Fold 0, MAE: 1159.533359263806\n",
      "Fold 1, MAE: 1154.5401700530679\n",
      "Fold 2, MAE: 1152.34695090863\n",
      "3-fold CV score: 1155.473493408501\n",
      "Model Testing: {'hidden1_dropout': 0.43157894736842106, 'hidden1_units': 377, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 189, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.081820000000000004}\n",
      "Fold 0, MAE: 1162.9307342647628\n",
      "Fold 1, MAE: 1177.8290937843656\n",
      "Fold 2, MAE: 1148.0584482485735\n",
      "3-fold CV score: 1162.9394254325673\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 455, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 162, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 63, 'optimizer': 'adadelta', 'wdecay': 0.091920000000000002}\n",
      "Fold 0, MAE: 1161.4301880915023\n",
      "Fold 1, MAE: 1169.6419220691284\n",
      "Fold 2, MAE: 1144.48012878803\n",
      "3-fold CV score: 1158.5174129828868\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 472, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 279, 'hidden3_dropout': 0.27777777777777779, 'hidden3_units': 77, 'optimizer': 'adadelta', 'wdecay': 0.069699999999999998}\n",
      "Fold 0, MAE: 1158.2613808574185\n",
      "Fold 1, MAE: 1156.7999195958928\n",
      "Fold 2, MAE: 1153.1107720208292\n",
      "3-fold CV score: 1156.0573574913803\n",
      "Model Testing: {'hidden1_dropout': 0.4631578947368421, 'hidden1_units': 524, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 265, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 40, 'optimizer': 'adadelta', 'wdecay': 0.094950000000000007}\n",
      "Fold 0, MAE: 1195.4596557820985\n",
      "Fold 1, MAE: 1166.5828679966726\n",
      "Fold 2, MAE: 1151.5350493940605\n",
      "3-fold CV score: 1171.1925243909438\n",
      "Model Testing: {'hidden1_dropout': 0.51578947368421058, 'hidden1_units': 498, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 231, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 59, 'optimizer': 'adadelta', 'wdecay': 0.08788}\n",
      "Fold 0, MAE: 1166.894567545835\n",
      "Fold 1, MAE: 1158.6302274524644\n",
      "Fold 2, MAE: 1146.4025209941412\n",
      "3-fold CV score: 1157.3091053308135\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 334, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 106, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.035360000000000003}\n",
      "Fold 0, MAE: 1156.7171541562705\n",
      "Fold 1, MAE: 1158.617184146834\n",
      "Fold 2, MAE: 1140.9087310499317\n",
      "3-fold CV score: 1152.0810231176786\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 515, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 286, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 71, 'optimizer': 'adadelta', 'wdecay': 0.035360000000000003}\n",
      "Fold 0, MAE: 1166.3465921313127\n",
      "Fold 1, MAE: 1155.4647161447238\n",
      "Fold 2, MAE: 1145.8189915304222\n",
      "3-fold CV score: 1155.8767666021529\n",
      "Model Testing: {'hidden1_dropout': 0.44210526315789478, 'hidden1_units': 334, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 106, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 65, 'optimizer': 'adadelta', 'wdecay': 0.052530000000000007}\n",
      "Fold 0, MAE: 1165.2972734181083\n",
      "Fold 1, MAE: 1162.3415273140827\n",
      "Fold 2, MAE: 1143.2022154553692\n",
      "3-fold CV score: 1156.9470053958535\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.00809}\n",
      "Fold 0, MAE: 1159.2449543552357\n",
      "Fold 1, MAE: 1154.8565114282767\n",
      "Fold 2, MAE: 1140.933696843291\n",
      "3-fold CV score: 1151.6783875422677\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 334, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.014149999999999999}\n",
      "Fold 0, MAE: 1165.0503812670863\n",
      "Fold 1, MAE: 1157.1801844593206\n",
      "Fold 2, MAE: 1141.5077789837922\n",
      "3-fold CV score: 1154.579448236733\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 106, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 53, 'optimizer': 'adadelta', 'wdecay': 0.00809}\n",
      "Fold 0, MAE: 1162.7622893053533\n",
      "Fold 1, MAE: 1156.484460623302\n",
      "Fold 2, MAE: 1141.2796353609326\n",
      "3-fold CV score: 1153.5087950965292\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 481, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 141, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 53, 'optimizer': 'adam', 'wdecay': 0.025260000000000001}\n",
      "Fold 0, MAE: 1173.398777121047\n",
      "Fold 1, MAE: 1163.4376312509926\n",
      "Fold 2, MAE: 1149.4355696684008\n",
      "3-fold CV score: 1162.0906593468135\n",
      "Model Testing: {'hidden1_dropout': 0.56842105263157894, 'hidden1_units': 343, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 224, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 57, 'optimizer': 'adadelta', 'wdecay': 0.01617}\n",
      "Fold 0, MAE: 1157.8129235479112\n",
      "Fold 1, MAE: 1162.9604955281113\n",
      "Fold 2, MAE: 1137.8848549871118\n",
      "3-fold CV score: 1152.886091354378\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 463, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 210, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 67, 'optimizer': 'adam', 'wdecay': 0.028289999999999999}\n",
      "Fold 0, MAE: 1167.1458696599193\n",
      "Fold 1, MAE: 1160.2728405505222\n",
      "Fold 2, MAE: 1154.656251596023\n",
      "3-fold CV score: 1160.6916539354881\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 360, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 300, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 32, 'optimizer': 'adadelta', 'wdecay': 0.071720000000000006}\n",
      "Fold 0, MAE: 1157.0820389802964\n",
      "Fold 1, MAE: 1173.8628986425185\n",
      "Fold 2, MAE: 1147.7328339035314\n",
      "3-fold CV score: 1159.5592571754487\n",
      "Model Testing: {'hidden1_dropout': 0.45263157894736844, 'hidden1_units': 317, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 293, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 36, 'optimizer': 'adadelta', 'wdecay': 0.027280000000000002}\n",
      "Fold 0, MAE: 1162.285383334579\n",
      "Fold 1, MAE: 1160.938020819047\n",
      "Fold 2, MAE: 1145.0485755819968\n",
      "3-fold CV score: 1156.0906599118741\n",
      "Model Testing: {'hidden1_dropout': 0.51578947368421058, 'hidden1_units': 429, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 244, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 80, 'optimizer': 'adam', 'wdecay': 0.097979999999999998}\n",
      "Fold 0, MAE: 1187.3501504499832\n",
      "Fold 1, MAE: 1177.8064867953847\n",
      "Fold 2, MAE: 1153.1573311127709\n",
      "3-fold CV score: 1172.7713227860463\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.30000000000000004, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1157.9619118652226\n",
      "Fold 1, MAE: 1155.7216872165664\n",
      "Fold 2, MAE: 1136.927755550967\n",
      "3-fold CV score: 1150.2037848775853\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1156.0632860191683\n",
      "Fold 1, MAE: 1151.4671627447715\n",
      "Fold 2, MAE: 1140.1617350157348\n",
      "3-fold CV score: 1149.230727926558\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adam', 'wdecay': 0.059600000000000007}\n",
      "Fold 0, MAE: 1168.5160545404665\n",
      "Fold 1, MAE: 1175.4194884359572\n",
      "Fold 2, MAE: 1152.5397784139907\n",
      "3-fold CV score: 1165.4917737968046\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.062630000000000005}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, MAE: 1163.881212321844\n",
      "Fold 1, MAE: 1164.1301011722203\n",
      "Fold 2, MAE: 1150.2623383121809\n",
      "3-fold CV score: 1159.4245506020818\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 532, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 251, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adadelta', 'wdecay': 0.074749999999999997}\n",
      "Fold 0, MAE: 1161.926003949864\n",
      "Fold 1, MAE: 1171.028653969578\n",
      "Fold 2, MAE: 1147.243582406554\n",
      "3-fold CV score: 1160.0660801086653\n",
      "Model Testing: {'hidden1_dropout': 0.5473684210526315, 'hidden1_units': 300, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 127, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 55, 'optimizer': 'adam', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1157.4905136832626\n",
      "Fold 1, MAE: 1157.7959576055189\n",
      "Fold 2, MAE: 1145.165175664648\n",
      "3-fold CV score: 1153.4838823178097\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1154.1358029742491\n",
      "Fold 1, MAE: 1150.8801407907292\n",
      "Fold 2, MAE: 1136.143128436801\n",
      "3-fold CV score: 1147.0530240672597\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 134, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.088889999999999997}\n",
      "Fold 0, MAE: 1174.4271644173484\n",
      "Fold 1, MAE: 1160.2759364815581\n",
      "Fold 2, MAE: 1143.464337864193\n",
      "3-fold CV score: 1159.3891462543666\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 403, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 120, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 26, 'optimizer': 'adadelta', 'wdecay': 0.079799999999999996}\n",
      "Fold 0, MAE: 1177.3892872402876\n",
      "Fold 1, MAE: 1162.1597504698693\n",
      "Fold 2, MAE: 1156.8215401050213\n",
      "3-fold CV score: 1165.456859271726\n",
      "Model Testing: {'hidden1_dropout': 0.40000000000000002, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 168, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 73, 'optimizer': 'adam', 'wdecay': 0.072730000000000003}\n",
      "Fold 0, MAE: 1210.498220541357\n",
      "Fold 1, MAE: 1184.6350615099377\n",
      "Fold 2, MAE: 1151.241476172358\n",
      "3-fold CV score: 1182.1249194078841\n",
      "Model Testing: {'hidden1_dropout': 0.41052631578947368, 'hidden1_units': 351, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 182, 'hidden3_dropout': 0.5, 'hidden3_units': 20, 'optimizer': 'adadelta', 'wdecay': 0.0293}\n",
      "Fold 0, MAE: 1171.7247792783894\n",
      "Fold 1, MAE: 1176.3316548287394\n",
      "Fold 2, MAE: 1154.8692943033595\n",
      "3-fold CV score: 1167.6419094701625\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 394, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 155, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 34, 'optimizer': 'adadelta', 'wdecay': 0.01516}\n",
      "Fold 0, MAE: 1168.154443232974\n",
      "Fold 1, MAE: 1157.7412081981763\n",
      "Fold 2, MAE: 1141.040754590281\n",
      "3-fold CV score: 1155.6454686738105\n",
      "Model Testing: {'hidden1_dropout': 0.47368421052631582, 'hidden1_units': 420, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 237, 'hidden3_dropout': 0.32222222222222224, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.046470000000000004}\n",
      "Fold 0, MAE: 1164.3777441806067\n",
      "Fold 1, MAE: 1177.209987984398\n",
      "Fold 2, MAE: 1155.6756505514757\n",
      "3-fold CV score: 1165.7544609054933\n",
      "Model Testing: {'hidden1_dropout': 0.58947368421052637, 'hidden1_units': 437, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 175, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 38, 'optimizer': 'adadelta', 'wdecay': 0.0020300000000000001}\n",
      "Fold 0, MAE: 1156.644832466085\n",
      "Fold 1, MAE: 1157.6123849948424\n",
      "Fold 2, MAE: 1137.0233535830582\n",
      "3-fold CV score: 1150.4268570146621\n",
      "Model Testing: {'hidden1_dropout': 0.43157894736842106, 'hidden1_units': 506, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 100, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 46, 'optimizer': 'adadelta', 'wdecay': 0.018190000000000001}\n",
      "Fold 0, MAE: 1156.7325890863394\n",
      "Fold 1, MAE: 1163.3614326980687\n",
      "Fold 2, MAE: 1140.6892231279783\n",
      "3-fold CV score: 1153.5944149707955\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 308, 'hidden2_dropout': 0.5, 'hidden2_units': 203, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 48, 'optimizer': 'adadelta', 'wdecay': 0.067680000000000004}\n",
      "Fold 0, MAE: 1187.777674460418\n",
      "Fold 1, MAE: 1164.7113053835535\n",
      "Fold 2, MAE: 1148.0043875643821\n",
      "3-fold CV score: 1166.8311224694514\n",
      "Model Testing: {'hidden1_dropout': 0.4631578947368421, 'hidden1_units': 386, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 182, 'hidden3_dropout': 0.10000000000000001, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.080810000000000007}\n",
      "Fold 0, MAE: 1165.0194917910676\n",
      "Fold 1, MAE: 1158.2729473606164\n",
      "Fold 2, MAE: 1152.9800377245338\n",
      "3-fold CV score: 1158.7574922920726\n",
      "Model Testing: {'hidden1_dropout': 0.49473684210526314, 'hidden1_units': 412, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 196, 'hidden3_dropout': 0.5, 'hidden3_units': 75, 'optimizer': 'adam', 'wdecay': 0.038390000000000007}\n",
      "Fold 0, MAE: 1161.5217347156527\n",
      "Fold 1, MAE: 1162.386833252451\n",
      "Fold 2, MAE: 1145.6365952047393\n",
      "3-fold CV score: 1156.5150543909476\n",
      "Model Testing: {'hidden1_dropout': 0.52631578947368418, 'hidden1_units': 446, 'hidden2_dropout': 0.40000000000000002, 'hidden2_units': 148, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta', 'wdecay': 0.096970000000000001}\n",
      "Fold 0, MAE: 1160.2965376990726\n",
      "Fold 1, MAE: 1168.4996048508942\n",
      "Fold 2, MAE: 1149.110823150734\n",
      "3-fold CV score: 1159.3023219002337\n",
      "Model Testing: {'hidden1_dropout': 0.4210526315789474, 'hidden1_units': 489, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 113, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 42, 'optimizer': 'adadelta', 'wdecay': 0.060610000000000004}\n",
      "Fold 0, MAE: 1183.286138317614\n",
      "Fold 1, MAE: 1177.9553902909124\n",
      "Fold 2, MAE: 1155.3975773011248\n",
      "3-fold CV score: 1172.2130353032169\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 550, 'hidden2_dropout': 0.26666666666666666, 'hidden2_units': 162, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 24, 'optimizer': 'adadelta', 'wdecay': 0.056570000000000002}\n",
      "Fold 0, MAE: 1161.7203151536628\n",
      "Fold 1, MAE: 1179.3700442916124\n"
     ]
    }
   ],
   "source": [
    "# VERSION 4. Insights:\n",
    "#  why not to test 4-layer architectures?\n",
    "# we need to introduce new optimizers\n",
    "#  adding batch normalization (https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "        'hidden3_dropout': hp.choice('hidden3_dropout', np.linspace(0.1,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int32')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int32')),\n",
    "         'hidden3_units': hp.choice('hidden3_units', np.linspace(20,80,30,dtype='int32')),\n",
    "         'optimizer': hp.choice('optimizer', ['adam','adadelta']),\n",
    "         'wdecay':hp.choice('wdecay', np.linspace(0.00001,0.1,1000)),\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print ('Model Testing:', params)\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1], init='he_normal',\n",
    "                        W_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units'], init='he_normal',W_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(params['hidden3_units'], init='he_normal',W_regularizer=l2(params['wdecay']))) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden3_dropout']))\n",
    "        \n",
    "        model.add(Dense(1, init='he_normal',W_regularizer=l2(params['wdecay'])))\n",
    "        model.compile(loss='mae', optimizer=params['optimizer'])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 100, trials=trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
