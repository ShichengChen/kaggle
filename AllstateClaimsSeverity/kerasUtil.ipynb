{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/coder.chenshicheng/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import h5py\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.models import Sequential\n",
    "from keras.models import save_model, load_model\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "ids = test['id']\n",
    "test['loss'] = np.nan\n",
    "joined = pd.concat([train, test])\n",
    "f_num = [f for f in train.columns if 'cont' in f]\n",
    "f_cat = [f for f in train.columns if 'cat' in f]\n",
    "for column in f_cat:\n",
    "    if (train[column].nunique() != test[column].nunique()\n",
    "        or (train[column].unique() != test[column].unique()).any()):\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        \n",
    "        remove = ((set_train - set_test)|(set_test - set_train))\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "        myfilter = lambda x: np.nan if x in remove else x\n",
    "        joined[column] = joined[column].apply(filter_cat, 1)\n",
    "\n",
    "    #joined[column] = pd.factorize(joined[column].values, sort=True)[0]\n",
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "\n",
    "shift = 200\n",
    "#train_y = np.log(train['loss'] + shift)\n",
    "train_y=train['loss']\n",
    "train_x = train.drop(['loss', 'id'], 1)\n",
    "test = test.drop(['loss', 'id'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 1065) (188318, 14)\n",
      "(188318, 1079)\n",
      "(125546, 1079)\n",
      "(188318,)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "contx = scaler.fit_transform(train_x[f_num])\n",
    "catx = pd.get_dummies(data=train_x[f_cat],columns=f_cat,dummy_na=False).values\n",
    "\n",
    "print (catx.shape,contx.shape)\n",
    "\n",
    "train_x = np.concatenate((contx,catx),axis=1)\n",
    "\n",
    "\n",
    "test = pd.get_dummies(data=test,columns=f_cat).values\n",
    "print (train_x.shape)\n",
    "print (test.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def mae_score(y_true, y_pred):\n",
    "    return K.mean(K.abs(K.exp(y_true)-K.exp(y_pred)), axis=-1)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:02d}-{val_loss:.2f}.hdf5', monitor='val_loss', \n",
    "                             verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import load_model\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "class ExponentialMovingAverage(Callback):\n",
    "    \"\"\"create a copy of trainable weights which gets updated at every\n",
    "       batch using exponential weight decay. The moving average weights along\n",
    "       with the other states of original model(except original model trainable\n",
    "       weights) will be saved at every epoch if save_mv_ave_model is True.\n",
    "       If both save_mv_ave_model and save_best_only are True, the latest\n",
    "       best moving average model according to the quantity monitored\n",
    "       will not be overwritten. Of course, save_best_only can be True\n",
    "       only if there is a validation set.\n",
    "       This is equivalent to save_best_only mode of ModelCheckpoint\n",
    "       callback with similar code. custom_objects is a dictionary\n",
    "       holding name and Class implementation for custom layers.\n",
    "       At end of every batch, the update is as follows:\n",
    "       mv_weight -= (1 - decay) * (mv_weight - weight)\n",
    "       where weight and mv_weight is the ordinal model weight and the moving\n",
    "       averaged weight respectively. At the end of the training, the moving\n",
    "       averaged weights are transferred to the original model.\n",
    "       \"\"\"\n",
    "    def __init__(self, decay=0.999, filepath='model/model{epoch:02d}-{val_loss:.2f}.hdf5',\n",
    "                 save_mv_ave_model=False, verbose=0,\n",
    "                 save_best_only=False, monitor='val_loss', mode='auto',\n",
    "                 save_weights_only=False, custom_objects={}):\n",
    "        self.decay = decay\n",
    "        self.filepath = filepath\n",
    "        self.verbose = verbose\n",
    "        self.save_mv_ave_model = save_mv_ave_model\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.save_best_only = save_best_only\n",
    "        self.monitor = monitor\n",
    "        self.custom_objects = custom_objects  # dictionary of custom layers\n",
    "        self.sym_trainable_weights = None  # trainable weights of model\n",
    "        self.mv_trainable_weights_vals = None  # moving averaged values\n",
    "        self.epochs = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor:\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.sym_trainable_weights = self.model.trainable_weights\n",
    "        # Initialize moving averaged weights using original model values\n",
    "        self.mv_trainable_weights_vals = {x.name: K.get_value(x).copy() for x in\n",
    "                                          self.sym_trainable_weights}\n",
    "        if self.verbose:\n",
    "            print('Created a copy of model weights to initialize moving averaged weights.')\n",
    "    def on_train_end(self, logs={}):\n",
    "        '''old = K.get_value(self.model.trainable_weights[0])\n",
    "        name = self.model.trainable_weights[0].name\n",
    "        assert((old != self.mv_trainable_weights_vals[name]).any())'''\n",
    "        \n",
    "        for weight in self.sym_trainable_weights:\n",
    "            K.set_value(weight, self.mv_trainable_weights_vals[weight.name])\n",
    "\n",
    "        '''old = K.get_value(self.model.trainable_weights[0])\n",
    "        name = self.model.trainable_weights[0].name\n",
    "        assert((old == self.mv_trainable_weights_vals[name]).all())'''\n",
    "            \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if(self.epochs + 5 > self.params['epochs']):\n",
    "            for weight in self.sym_trainable_weights:\n",
    "                old_val = self.mv_trainable_weights_vals[weight.name].copy()\n",
    "                self.mv_trainable_weights_vals[weight.name] = \\\n",
    "                    old_val * self.decay + (1.0 - self.decay) * K.get_value(weight).copy()\n",
    "            #assert((old_val == self.mv_trainable_weights_vals[weight.name]).all())\n",
    "            #assert((old_val == K.get_value(weight)).all())\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epochs += 1\n",
    "        if(self.epochs + 5 > self.params['epochs']):\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            self.model.save(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "from keras.initializers import he_normal\n",
    "from keras.regularizers import l2\n",
    "from keras.activations import selu\n",
    "from keras.optimizers import Adam\n",
    "def hyper_model(seed = None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(437, input_dim=train_x.shape[1], kernel_initializer=he_normal(seed = seed)\n",
    "                    ,kernel_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.536))\n",
    "    \n",
    "    model.add(Dense(182, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(73, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.233))\n",
    "    \n",
    "    model.add(Dense(1, kernel_initializer=he_normal(seed = seed),kernel_regularizer=l2(0.002)))\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "    #model.compile(optimizer='adadelta',loss = 'mae',metrics = [mae_score])\n",
    "    model.compile(optimizer='adadelta',loss = 'mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 2141.2406 - val_loss: 1227.3853\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1377.5925 - val_loss: 1180.3451\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1353.0381 - val_loss: 1166.2284\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1340.3170 - val_loss: 1158.8036\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 74us/step - loss: 1330.2459 - val_loss: 1155.6613\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 74us/step - loss: 1321.8207 - val_loss: 1152.8829\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1319.1778 - val_loss: 1153.1723\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1307.5080 - val_loss: 1150.6116\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1306.7916 - val_loss: 1146.5155\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 10s 74us/step - loss: 1296.4018 - val_loss: 1149.3142\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1288.6815 - val_loss: 1143.7124\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1281.8182 - val_loss: 1146.3470\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1277.3025 - val_loss: 1145.2112\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1272.9200 - val_loss: 1142.1048\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1267.3694 - val_loss: 1142.2416\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1259.2266 - val_loss: 1140.7994\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1253.1709 - val_loss: 1141.7598\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1248.6438 - val_loss: 1141.3252\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1245.3755 - val_loss: 1138.9090\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1240.0015 - val_loss: 1138.1516\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1234.8747 - val_loss: 1140.9408\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1232.1160 - val_loss: 1139.6693\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1225.2857 - val_loss: 1137.1292\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1220.8471 - val_loss: 1139.2637\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1216.1899 - val_loss: 1137.9408\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1210.6228 - val_loss: 1137.6000\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1206.6588 - val_loss: 1139.0537\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1204.3365 - val_loss: 1137.8286\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1199.8355 - val_loss: 1138.8150\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1197.1012 - val_loss: 1137.4390\n",
      "nfold:0,bag:0 1144.27145267\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 2060.6981 - val_loss: 1203.2664\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1366.4528 - val_loss: 1175.0522\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1345.7897 - val_loss: 1168.0366\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1337.7787 - val_loss: 1163.3327\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1328.0474 - val_loss: 1154.6840\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1315.2199 - val_loss: 1151.6413\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1306.3480 - val_loss: 1150.2309\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1306.6972 - val_loss: 1150.2725\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1295.4766 - val_loss: 1146.8892\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1290.4295 - val_loss: 1143.7180\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1283.3739 - val_loss: 1143.5689\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1277.8951 - val_loss: 1142.1550\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1269.9797 - val_loss: 1141.3012\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 10s 74us/step - loss: 1266.3062 - val_loss: 1143.8971\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1261.0069 - val_loss: 1143.1106\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1255.9736 - val_loss: 1140.3379\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1250.7077 - val_loss: 1138.8664\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1244.9449 - val_loss: 1138.0463\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1237.7666 - val_loss: 1139.4391\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1236.7931 - val_loss: 1139.7503\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1229.5360 - val_loss: 1137.2945\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1225.4793 - val_loss: 1137.0343\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1219.4177 - val_loss: 1138.9750\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1219.5122 - val_loss: 1138.5111\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1216.2307 - val_loss: 1137.1911\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1207.9175 - val_loss: 1136.3677\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1205.9193 - val_loss: 1135.7325\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 138us/step - loss: 1199.7240 - val_loss: 1137.3358\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1199.1695 - val_loss: 1134.7060\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1197.8733 - val_loss: 1134.8537\n",
      "nfold:0,bag:1 1144.49486479\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 2217.1489 - val_loss: 1223.5795\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1302.8672 - val_loss: 1170.1970\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1280.2643 - val_loss: 1158.6588\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1272.7416 - val_loss: 1154.2949\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1263.6459 - val_loss: 1151.1795\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1259.1615 - val_loss: 1147.3721\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1252.4570 - val_loss: 1146.2427\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1248.3422 - val_loss: 1145.2964\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1244.2083 - val_loss: 1147.4610\n",
      "Epoch 10/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 10s 76us/step - loss: 1236.6628 - val_loss: 1142.4172\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1233.7533 - val_loss: 1140.2531\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1229.3597 - val_loss: 1139.7299\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1228.2212 - val_loss: 1139.0084\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1221.2148 - val_loss: 1142.1605\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1221.4302 - val_loss: 1138.1203\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1217.7132 - val_loss: 1139.1325\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1214.1610 - val_loss: 1138.3226\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1210.8970 - val_loss: 1137.3544\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1207.5312 - val_loss: 1137.9699\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1202.8846 - val_loss: 1137.2686\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1201.1264 - val_loss: 1142.7230\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1198.2970 - val_loss: 1136.8153\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1195.0912 - val_loss: 1136.3657\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1190.4807 - val_loss: 1134.9028\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1191.0334 - val_loss: 1134.2140\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1184.5901 - val_loss: 1138.0177\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1183.6160 - val_loss: 1138.8421\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1182.2760 - val_loss: 1136.5087\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1177.5806 - val_loss: 1137.6581\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1177.5882 - val_loss: 1137.3166\n",
      "nfold:0,bag:2 1141.27816608\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 2249.7179 - val_loss: 1223.6256\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1405.1508 - val_loss: 1178.3282\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1363.1015 - val_loss: 1170.8170\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1344.4050 - val_loss: 1161.7973\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1332.4786 - val_loss: 1158.6749\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1327.0924 - val_loss: 1151.5126\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1311.4018 - val_loss: 1148.2860\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1305.8221 - val_loss: 1147.1361\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1298.5445 - val_loss: 1144.0724\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1291.9973 - val_loss: 1145.4666\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1283.0255 - val_loss: 1143.6694\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1273.9157 - val_loss: 1140.9063\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1269.1672 - val_loss: 1141.5110\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1262.1399 - val_loss: 1141.0995\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1254.5031 - val_loss: 1139.9005\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1249.1366 - val_loss: 1139.8769\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1247.6481 - val_loss: 1136.9538\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1242.5562 - val_loss: 1138.5772\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1234.3940 - val_loss: 1140.7368\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1230.5461 - val_loss: 1137.5814\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1226.6490 - val_loss: 1137.4369\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1221.0894 - val_loss: 1136.3499\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1212.3269 - val_loss: 1136.9452\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1209.8538 - val_loss: 1139.3106\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1207.1102 - val_loss: 1137.1268\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1206.6657 - val_loss: 1136.7219\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 135us/step - loss: 1202.1497 - val_loss: 1139.3319\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1193.9628 - val_loss: 1135.8936\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1190.0724 - val_loss: 1136.0721\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1187.7611 - val_loss: 1135.7891\n",
      "nfold:0,bag:3 1140.01761288\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 2063.5735 - val_loss: 1180.1476\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1297.8865 - val_loss: 1164.5931\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1274.4440 - val_loss: 1158.6840\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1269.2284 - val_loss: 1152.3296\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1263.6278 - val_loss: 1151.9763\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1255.5848 - val_loss: 1148.6088\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1253.0476 - val_loss: 1147.2128\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1246.1319 - val_loss: 1144.1025\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1247.8744 - val_loss: 1144.1339\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1239.2680 - val_loss: 1143.9338\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1235.3093 - val_loss: 1140.2481\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1233.0761 - val_loss: 1140.1118\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1225.8970 - val_loss: 1140.2298\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1224.1713 - val_loss: 1138.6679\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1223.1942 - val_loss: 1139.2117\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1219.3226 - val_loss: 1139.3656\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1216.0712 - val_loss: 1137.1385\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1211.7822 - val_loss: 1138.0135\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1208.9581 - val_loss: 1137.9726\n",
      "Epoch 20/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 10s 77us/step - loss: 1205.5047 - val_loss: 1136.1903\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1204.2275 - val_loss: 1138.6387\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1197.6086 - val_loss: 1137.2679\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1195.8666 - val_loss: 1138.0406\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1197.2501 - val_loss: 1136.2352\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1195.3602 - val_loss: 1136.4173\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1189.4139 - val_loss: 1137.3516\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1188.7968 - val_loss: 1135.0059\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1181.6579 - val_loss: 1136.3777\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1182.2672 - val_loss: 1137.4342\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1178.6602 - val_loss: 1136.4886\n",
      "nfold:0,bag:4 1143.13416577\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 2077.8350 - val_loss: 1207.6840\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1338.0348 - val_loss: 1177.5743\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1315.6196 - val_loss: 1166.8581\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1310.5786 - val_loss: 1162.8719\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1296.7479 - val_loss: 1158.5171\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1292.7229 - val_loss: 1155.5576\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1291.6043 - val_loss: 1154.9719\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1281.4133 - val_loss: 1152.4960\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1276.2164 - val_loss: 1149.8192\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1267.5575 - val_loss: 1148.0222\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1266.7899 - val_loss: 1147.5921\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1261.3444 - val_loss: 1146.4188\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1256.7080 - val_loss: 1144.4032\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1254.4370 - val_loss: 1146.2271\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1248.1223 - val_loss: 1143.9074\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1243.8524 - val_loss: 1141.7630\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1238.1051 - val_loss: 1143.1662\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1234.1352 - val_loss: 1142.4369\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1235.0286 - val_loss: 1141.8337\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1227.9761 - val_loss: 1140.6080\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1221.1208 - val_loss: 1142.6606\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1220.8709 - val_loss: 1142.2787\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1215.8315 - val_loss: 1141.1810\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1213.7753 - val_loss: 1140.6701\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1207.4296 - val_loss: 1140.1994\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1203.0427 - val_loss: 1139.6704\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1199.5508 - val_loss: 1141.9077\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 144us/step - loss: 1199.3780 - val_loss: 1139.0581\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1194.7566 - val_loss: 1141.4476\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1189.1021 - val_loss: 1139.1163\n",
      "nfold:1,bag:0 1161.15641124\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 2174.3640 - val_loss: 1213.2660\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1345.4315 - val_loss: 1179.8989\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1326.1751 - val_loss: 1169.8779\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1316.8761 - val_loss: 1160.3538\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1301.8004 - val_loss: 1157.9175\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1299.3806 - val_loss: 1154.3145\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1292.0315 - val_loss: 1152.4664\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1282.8851 - val_loss: 1154.4030\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1279.8620 - val_loss: 1152.9310\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1273.6783 - val_loss: 1147.5106\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1265.5944 - val_loss: 1146.8230\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1261.2298 - val_loss: 1145.6778\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1253.2759 - val_loss: 1145.3994\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1251.4714 - val_loss: 1145.4928\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1249.7549 - val_loss: 1143.6590\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1241.8265 - val_loss: 1143.3091\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1238.3309 - val_loss: 1141.1760\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1235.3723 - val_loss: 1143.5477\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1228.2291 - val_loss: 1142.3199\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1225.0576 - val_loss: 1140.3758\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1225.9902 - val_loss: 1140.5500\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1215.1281 - val_loss: 1141.8292\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1215.8037 - val_loss: 1142.9789\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1209.4002 - val_loss: 1141.5114\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1208.0873 - val_loss: 1140.7613\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1201.3052 - val_loss: 1140.4245\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1199.1830 - val_loss: 1141.4655\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1196.3359 - val_loss: 1142.6144\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 147us/step - loss: 1191.5271 - val_loss: 1140.0834\n",
      "Epoch 30/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 20s 147us/step - loss: 1188.4280 - val_loss: 1140.5027\n",
      "nfold:1,bag:1 1161.12787394\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 2102.7812 - val_loss: 1202.3455\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1368.7018 - val_loss: 1178.5006\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1342.8355 - val_loss: 1164.6004\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1329.0768 - val_loss: 1167.7070\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1317.4153 - val_loss: 1161.0539\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1311.6860 - val_loss: 1155.1814\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1307.4906 - val_loss: 1156.7080\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1298.6262 - val_loss: 1153.6576\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1290.6982 - val_loss: 1151.5499\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1284.3241 - val_loss: 1149.7113\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1279.4346 - val_loss: 1148.2026\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1274.0855 - val_loss: 1148.2958\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1266.7815 - val_loss: 1145.0307\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1259.9046 - val_loss: 1145.2871\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1253.0130 - val_loss: 1146.1560\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1249.6124 - val_loss: 1146.1348\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1244.6442 - val_loss: 1144.9488\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1239.4044 - val_loss: 1141.8125\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1233.7256 - val_loss: 1143.6095\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1230.8206 - val_loss: 1144.4841\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1228.1822 - val_loss: 1141.0488\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1223.8360 - val_loss: 1141.0065\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1214.1605 - val_loss: 1140.9560\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1212.7237 - val_loss: 1141.3082\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1208.1417 - val_loss: 1141.0271\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1203.3203 - val_loss: 1141.7736\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 146us/step - loss: 1202.6383 - val_loss: 1142.5789\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1192.7902 - val_loss: 1140.6138\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 19s 143us/step - loss: 1188.6843 - val_loss: 1141.5080\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 144us/step - loss: 1190.7610 - val_loss: 1141.3120\n",
      "nfold:1,bag:2 1160.58730328\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 2247.0830 - val_loss: 1210.4753\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1351.9264 - val_loss: 1182.1583\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1319.7243 - val_loss: 1170.6551\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1308.3455 - val_loss: 1164.6756\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1297.8898 - val_loss: 1161.1755\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1289.7468 - val_loss: 1155.4442\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1283.0543 - val_loss: 1154.9834\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1277.5539 - val_loss: 1151.7730\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1266.9118 - val_loss: 1152.9670\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1268.5262 - val_loss: 1150.6528\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1259.9125 - val_loss: 1148.9848\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1254.0169 - val_loss: 1148.9342\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1248.8742 - val_loss: 1149.6544\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1243.4334 - val_loss: 1144.8061\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1239.7908 - val_loss: 1147.4007\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1235.0300 - val_loss: 1143.9131\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1230.5774 - val_loss: 1146.1995\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1223.4480 - val_loss: 1142.9296\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1218.6759 - val_loss: 1145.5020\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1215.8680 - val_loss: 1141.9538\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1212.3429 - val_loss: 1143.4771\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1208.2705 - val_loss: 1146.6502\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1207.1281 - val_loss: 1143.9407\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1201.2162 - val_loss: 1143.1028\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1197.0100 - val_loss: 1140.3687\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1197.1221 - val_loss: 1139.9873\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 19s 141us/step - loss: 1190.9149 - val_loss: 1141.6986\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 18s 134us/step - loss: 1185.9446 - val_loss: 1141.3715\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 132us/step - loss: 1183.5840 - val_loss: 1141.1482\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 18s 133us/step - loss: 1181.2528 - val_loss: 1141.0420\n",
      "nfold:1,bag:3 1159.02462647\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 10s 70us/step - loss: 2154.1097 - val_loss: 1213.9775\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1355.2524 - val_loss: 1182.9686\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1322.7002 - val_loss: 1169.8268\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1315.0022 - val_loss: 1164.1654\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 9s 70us/step - loss: 1303.5436 - val_loss: 1158.9475\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1294.6394 - val_loss: 1159.2619\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1288.1868 - val_loss: 1155.3376\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1282.2941 - val_loss: 1153.9805\n",
      "Epoch 9/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 9s 66us/step - loss: 1280.9761 - val_loss: 1151.9654\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 9s 65us/step - loss: 1270.0743 - val_loss: 1148.6066\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1266.6186 - val_loss: 1149.1808\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 9s 65us/step - loss: 1261.7751 - val_loss: 1148.9935\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1252.8905 - val_loss: 1145.2222\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1248.4711 - val_loss: 1145.8444\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 9s 65us/step - loss: 1243.4477 - val_loss: 1143.7418\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1238.0203 - val_loss: 1144.6408\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1234.5789 - val_loss: 1144.6079\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1232.0324 - val_loss: 1141.9794\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1229.1875 - val_loss: 1140.0106\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1222.9456 - val_loss: 1140.3508\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1220.6591 - val_loss: 1143.2942\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1212.1878 - val_loss: 1141.4562\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1211.8116 - val_loss: 1141.8344\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1204.9319 - val_loss: 1140.3664\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1202.0147 - val_loss: 1139.6818\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 10s 75us/step - loss: 1199.5213 - val_loss: 1141.7268\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 18s 131us/step - loss: 1192.4782 - val_loss: 1141.0962\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 19s 137us/step - loss: 1189.4677 - val_loss: 1140.8442\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 18s 136us/step - loss: 1188.7977 - val_loss: 1139.1695\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 19s 140us/step - loss: 1184.9326 - val_loss: 1142.1464\n",
      "nfold:1,bag:4 1160.04993434\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 2042.1009 - val_loss: 1208.7062\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1328.9287 - val_loss: 1174.5379\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1309.8714 - val_loss: 1167.9578\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1300.1365 - val_loss: 1160.1134\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1292.6090 - val_loss: 1155.6091\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 9s 65us/step - loss: 1287.9816 - val_loss: 1152.7328\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1285.9700 - val_loss: 1153.2303\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 9s 65us/step - loss: 1278.9814 - val_loss: 1152.9678\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1271.2713 - val_loss: 1147.8457\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1268.8878 - val_loss: 1145.8868\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1264.0346 - val_loss: 1149.9392\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1256.7073 - val_loss: 1147.1643\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1250.8601 - val_loss: 1143.7498\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1251.0536 - val_loss: 1141.8565\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1247.1484 - val_loss: 1142.9783\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1242.0522 - val_loss: 1142.0559\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1237.9197 - val_loss: 1143.7677\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1233.2229 - val_loss: 1142.5082\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1231.5408 - val_loss: 1140.7547\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1224.9153 - val_loss: 1142.1713\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1222.4561 - val_loss: 1140.7187\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 71us/step - loss: 1217.6876 - val_loss: 1139.1881\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 73us/step - loss: 1215.2440 - val_loss: 1139.5063\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1212.2324 - val_loss: 1143.5766\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1206.5262 - val_loss: 1139.4570\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1204.0325 - val_loss: 1140.4576\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1202.1629 - val_loss: 1138.5785\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1202.4508 - val_loss: 1140.2824\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 154us/step - loss: 1195.5718 - val_loss: 1138.1510\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1192.7900 - val_loss: 1137.4410\n",
      "nfold:2,bag:0 1141.66612797\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 2096.1718 - val_loss: 1214.6492\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1365.6275 - val_loss: 1180.8881\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1345.8556 - val_loss: 1165.8208\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1339.0921 - val_loss: 1163.0848\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1330.6260 - val_loss: 1158.8668\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1320.1229 - val_loss: 1156.9312\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1312.4464 - val_loss: 1155.9393\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1309.0104 - val_loss: 1150.3437\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1296.3515 - val_loss: 1148.1315\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1293.4580 - val_loss: 1147.0412\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1287.3262 - val_loss: 1147.4744\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1277.4762 - val_loss: 1144.4911\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1277.2461 - val_loss: 1146.0713\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1269.6101 - val_loss: 1143.0928\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1262.1379 - val_loss: 1140.6659\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1256.8357 - val_loss: 1141.7524\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1251.3545 - val_loss: 1140.8438\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1247.7815 - val_loss: 1140.3022\n",
      "Epoch 19/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 10s 77us/step - loss: 1242.2802 - val_loss: 1141.4891\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1235.4813 - val_loss: 1139.5690\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1232.9353 - val_loss: 1138.5568\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1227.9805 - val_loss: 1139.4395\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1221.9259 - val_loss: 1138.2195\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1217.2536 - val_loss: 1140.2483\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1218.5393 - val_loss: 1139.0515\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1211.4195 - val_loss: 1139.5932\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1207.9314 - val_loss: 1137.7244\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1203.8749 - val_loss: 1139.4609\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1199.9610 - val_loss: 1139.9009\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1194.2981 - val_loss: 1138.3242\n",
      "nfold:2,bag:1 1140.6917343\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 2147.6066 - val_loss: 1226.1262\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1309.9450 - val_loss: 1167.6434\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1288.8879 - val_loss: 1161.6106\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1282.3980 - val_loss: 1158.7387\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1272.5846 - val_loss: 1153.6865\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1269.3369 - val_loss: 1150.4113\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1267.5875 - val_loss: 1150.8978\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1257.7370 - val_loss: 1149.7302\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1255.2513 - val_loss: 1146.7668\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1251.5759 - val_loss: 1145.5639\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1246.0775 - val_loss: 1146.6163\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1239.3549 - val_loss: 1147.3666\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1238.2325 - val_loss: 1143.9578\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1231.7799 - val_loss: 1142.8304\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1227.9454 - val_loss: 1143.0077\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1223.2143 - val_loss: 1141.7279\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1222.0255 - val_loss: 1141.0047\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1220.2566 - val_loss: 1140.2962\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1215.4092 - val_loss: 1140.8420\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1215.8903 - val_loss: 1138.4864\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1211.1158 - val_loss: 1140.1289\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1203.8715 - val_loss: 1137.9715\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1204.1151 - val_loss: 1136.9858\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1197.6732 - val_loss: 1138.8541\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1197.9753 - val_loss: 1139.1910\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1192.3482 - val_loss: 1137.6188\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1190.8059 - val_loss: 1138.6653\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1186.9121 - val_loss: 1139.0941\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 151us/step - loss: 1184.5113 - val_loss: 1138.4317\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1179.3608 - val_loss: 1139.1848\n",
      "nfold:2,bag:2 1139.81243065\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 2310.6692 - val_loss: 1284.0493\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1370.5909 - val_loss: 1182.2117\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1331.8281 - val_loss: 1170.0576\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1319.7898 - val_loss: 1159.1181\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1308.4655 - val_loss: 1154.5004\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1300.1193 - val_loss: 1155.2474\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1296.1958 - val_loss: 1152.1267\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1290.1678 - val_loss: 1149.6585\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1277.1766 - val_loss: 1149.4645\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1275.1017 - val_loss: 1146.0212\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1267.7221 - val_loss: 1146.4195\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1262.7582 - val_loss: 1142.5539\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1257.0829 - val_loss: 1143.4998\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1250.4058 - val_loss: 1145.1784\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1244.2419 - val_loss: 1140.8148\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1246.8222 - val_loss: 1143.6835\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1236.7234 - val_loss: 1144.2573\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1230.6538 - val_loss: 1139.8354\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1227.6405 - val_loss: 1143.2444\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1225.7712 - val_loss: 1140.6931\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1217.4251 - val_loss: 1138.9224\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1214.6374 - val_loss: 1138.0864\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1211.6841 - val_loss: 1140.0333\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1205.2232 - val_loss: 1139.7353\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1200.9750 - val_loss: 1141.5316\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1200.3415 - val_loss: 1139.1980\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1195.0389 - val_loss: 1141.0371\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 151us/step - loss: 1193.7271 - val_loss: 1138.4199\n",
      "Epoch 29/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 21s 155us/step - loss: 1189.7402 - val_loss: 1138.9267\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1181.2133 - val_loss: 1141.0438\n",
      "nfold:2,bag:3 1139.51615967\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 2122.0243 - val_loss: 1212.9819\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1401.4572 - val_loss: 1184.6816\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1371.9689 - val_loss: 1171.8117\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1353.2106 - val_loss: 1163.7544\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1348.4689 - val_loss: 1161.2633\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1337.2043 - val_loss: 1156.7394\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1330.6795 - val_loss: 1157.2065\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1323.2240 - val_loss: 1151.8403\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1310.6568 - val_loss: 1152.4204\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1302.0446 - val_loss: 1148.6173\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1299.8437 - val_loss: 1145.9803\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1289.4902 - val_loss: 1147.6678\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1281.6497 - val_loss: 1147.0994\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1275.6195 - val_loss: 1142.2961\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1265.6590 - val_loss: 1144.5275\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1263.0316 - val_loss: 1141.6129\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1254.9326 - val_loss: 1141.1742\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1251.8806 - val_loss: 1141.3033\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1242.8410 - val_loss: 1141.1559\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1238.7635 - val_loss: 1139.3350\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1238.4138 - val_loss: 1138.6109\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1230.8819 - val_loss: 1138.1779\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1226.0015 - val_loss: 1138.7360\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1221.1573 - val_loss: 1139.7017\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1214.7220 - val_loss: 1140.3157\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 1211.1250 - val_loss: 1140.4056\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1211.0147 - val_loss: 1140.1576\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1198.7644 - val_loss: 1140.5114\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 21s 156us/step - loss: 1201.0102 - val_loss: 1139.9155\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1193.7668 - val_loss: 1138.4142\n",
      "nfold:2,bag:4 1141.1119655\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 2160.4444 - val_loss: 1203.1102\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1330.4653 - val_loss: 1171.1714\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1305.6288 - val_loss: 1160.9101\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1296.8371 - val_loss: 1154.0058\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1289.3220 - val_loss: 1152.6128\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1284.0285 - val_loss: 1148.0515\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1277.9511 - val_loss: 1145.9496\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1270.7981 - val_loss: 1144.9199\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1266.4562 - val_loss: 1143.9974\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1261.4533 - val_loss: 1140.2117\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1253.4647 - val_loss: 1140.4663\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1249.1216 - val_loss: 1138.7087\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1248.7362 - val_loss: 1139.0343\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1245.8356 - val_loss: 1135.9982\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1238.6740 - val_loss: 1139.0011\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1236.1328 - val_loss: 1135.6208\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1231.5524 - val_loss: 1136.0409\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1226.9854 - val_loss: 1136.7710\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1222.2707 - val_loss: 1135.9186\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1220.7512 - val_loss: 1133.6402\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1216.2951 - val_loss: 1133.0205\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1211.4383 - val_loss: 1135.9326\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1210.4853 - val_loss: 1133.4414\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1206.7772 - val_loss: 1135.0875\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1205.5392 - val_loss: 1134.3550\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 1197.6994 - val_loss: 1135.2561\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1195.1946 - val_loss: 1132.4336\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 158us/step - loss: 1194.7231 - val_loss: 1134.5812\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1190.1890 - val_loss: 1135.2335\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 160us/step - loss: 1187.8110 - val_loss: 1133.1229\n",
      "nfold:3,bag:0 1146.70484132\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 2120.0297 - val_loss: 1194.5705\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1368.3443 - val_loss: 1168.7973\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1345.7176 - val_loss: 1165.5218\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1329.7516 - val_loss: 1156.9314\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1323.8470 - val_loss: 1156.1679\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1313.0293 - val_loss: 1150.0373\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1308.6231 - val_loss: 1147.0851\n",
      "Epoch 8/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 78us/step - loss: 1300.7581 - val_loss: 1144.1759\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1294.5769 - val_loss: 1142.3706\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1287.7033 - val_loss: 1142.6437\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1281.8047 - val_loss: 1140.9418\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1276.7983 - val_loss: 1141.7743\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1269.3643 - val_loss: 1139.3674\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1261.5755 - val_loss: 1139.5784\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1260.9984 - val_loss: 1139.0616\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 77us/step - loss: 1255.3381 - val_loss: 1137.3865\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1247.3217 - val_loss: 1136.7154\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1242.0301 - val_loss: 1135.5986\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1240.3319 - val_loss: 1136.6928\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1235.1446 - val_loss: 1133.7394\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1234.3222 - val_loss: 1133.9802\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1228.3060 - val_loss: 1137.1337\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1222.4451 - val_loss: 1133.5168\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1216.6999 - val_loss: 1133.3071\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1211.3746 - val_loss: 1133.7849\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1208.4780 - val_loss: 1133.1496\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1207.6119 - val_loss: 1134.9389\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 161us/step - loss: 1205.5946 - val_loss: 1133.8310\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 159us/step - loss: 1196.0697 - val_loss: 1135.3896\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1198.8106 - val_loss: 1136.1239\n",
      "nfold:3,bag:1 1145.22273204\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 2150.8525 - val_loss: 1217.2811\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1439.0880 - val_loss: 1196.1869\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1406.2296 - val_loss: 1169.1952\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1391.6584 - val_loss: 1163.4699\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1375.5723 - val_loss: 1155.1884\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1365.9905 - val_loss: 1152.0938\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1356.4880 - val_loss: 1148.7752\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1349.2811 - val_loss: 1147.8873\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1335.9789 - val_loss: 1146.4735\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1326.8350 - val_loss: 1146.3781\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1316.1312 - val_loss: 1146.7558\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1308.1476 - val_loss: 1143.5252\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1298.8030 - val_loss: 1140.5414\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1288.1821 - val_loss: 1138.6528\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1282.5757 - val_loss: 1143.8537\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1274.6101 - val_loss: 1138.8472\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1269.1464 - val_loss: 1138.3579\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1262.4691 - val_loss: 1137.8208\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1257.0064 - val_loss: 1134.5652\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1248.5318 - val_loss: 1135.7122\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1246.8159 - val_loss: 1134.7852\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1236.6811 - val_loss: 1134.8645\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1232.0037 - val_loss: 1133.3154\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1226.6693 - val_loss: 1134.5088\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1220.8118 - val_loss: 1134.7187\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 12s 92us/step - loss: 1215.2892 - val_loss: 1136.3164\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1214.5922 - val_loss: 1135.0597\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1201.9033 - val_loss: 1134.7479\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1202.8477 - val_loss: 1136.7545\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1195.2054 - val_loss: 1134.1610\n",
      "nfold:3,bag:2 1149.14862248\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 2241.2189 - val_loss: 1288.9153\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1465.9306 - val_loss: 1206.1514\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1408.3546 - val_loss: 1177.6710\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1383.4489 - val_loss: 1169.4803\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1372.3506 - val_loss: 1154.2293\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1358.8851 - val_loss: 1150.1106\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1348.2313 - val_loss: 1149.1894\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1332.9176 - val_loss: 1147.0671\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1324.2476 - val_loss: 1147.4519\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1319.7341 - val_loss: 1144.8275\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1308.3863 - val_loss: 1141.8625\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1303.8066 - val_loss: 1142.2801\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1291.3592 - val_loss: 1139.2904\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1283.5031 - val_loss: 1141.8035\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1279.6411 - val_loss: 1138.5489\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1268.6066 - val_loss: 1139.5408\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1262.4247 - val_loss: 1138.6222\n",
      "Epoch 18/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 10s 77us/step - loss: 1258.7863 - val_loss: 1136.7822\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1246.2974 - val_loss: 1137.6361\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1242.0906 - val_loss: 1138.0283\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1237.4252 - val_loss: 1135.9541\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1228.9736 - val_loss: 1135.4776\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1222.7243 - val_loss: 1136.2605\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1221.4977 - val_loss: 1136.3428\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1217.0109 - val_loss: 1135.7142\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 92us/step - loss: 1211.1367 - val_loss: 1135.4938\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1204.5729 - val_loss: 1134.4250\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1200.1755 - val_loss: 1134.1822\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1194.4945 - val_loss: 1134.1811\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 162us/step - loss: 1193.4828 - val_loss: 1134.7287\n",
      "nfold:3,bag:3 1148.42576865\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 2237.0269 - val_loss: 1228.8741\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1384.3029 - val_loss: 1175.3980\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1356.4915 - val_loss: 1166.0869\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1338.4888 - val_loss: 1157.1359\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1329.9008 - val_loss: 1152.5470\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1325.9104 - val_loss: 1149.1818\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1313.2752 - val_loss: 1147.4063\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1303.5084 - val_loss: 1146.7556\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1298.1591 - val_loss: 1147.0753\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1290.9994 - val_loss: 1142.1845\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1287.8662 - val_loss: 1142.0143\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1277.3254 - val_loss: 1140.3507\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1279.1114 - val_loss: 1139.1456\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1266.5020 - val_loss: 1140.0041\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1266.2110 - val_loss: 1139.3910\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1254.8851 - val_loss: 1137.4211\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1249.1008 - val_loss: 1136.5367\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1249.0515 - val_loss: 1137.7673\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1241.5234 - val_loss: 1136.3437\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1235.3394 - val_loss: 1136.0326\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1230.3545 - val_loss: 1136.0657\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1226.6670 - val_loss: 1136.0102\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1222.1285 - val_loss: 1134.7407\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1219.0979 - val_loss: 1136.1888\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1212.9125 - val_loss: 1133.9389\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 93us/step - loss: 1210.9335 - val_loss: 1134.5993\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 22s 165us/step - loss: 1208.8184 - val_loss: 1135.8482\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 169us/step - loss: 1201.2850 - val_loss: 1134.9583\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 22s 163us/step - loss: 1195.7249 - val_loss: 1137.3692\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 22s 164us/step - loss: 1196.2975 - val_loss: 1136.1574\n",
      "nfold:3,bag:4 1148.23337123\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 2117.1144 - val_loss: 1204.9962\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1399.3239 - val_loss: 1175.0881\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1372.3547 - val_loss: 1169.5782\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1362.7341 - val_loss: 1161.9931\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 10s 76us/step - loss: 1353.7094 - val_loss: 1160.0197\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 9s 65us/step - loss: 1346.6825 - val_loss: 1158.6933\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1334.7592 - val_loss: 1148.7440\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1329.9714 - val_loss: 1147.5246\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1323.6929 - val_loss: 1148.8737\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1309.9336 - val_loss: 1150.1205\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1301.0643 - val_loss: 1142.9941\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1297.1313 - val_loss: 1142.7104\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1291.7454 - val_loss: 1142.1538\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1285.4048 - val_loss: 1140.2641\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1277.0322 - val_loss: 1139.7629\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1269.7731 - val_loss: 1141.1605\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1263.1463 - val_loss: 1140.7155\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1258.8855 - val_loss: 1139.1833\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1254.4130 - val_loss: 1137.9625\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1243.8748 - val_loss: 1142.9575\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1241.6678 - val_loss: 1138.8189\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1234.6411 - val_loss: 1137.9819\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1232.1328 - val_loss: 1137.5803\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1225.9840 - val_loss: 1137.2769\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1223.5782 - val_loss: 1138.4112\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1214.9841 - val_loss: 1139.5171\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 20s 145us/step - loss: 1211.0852 - val_loss: 1137.6815\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 21s 153us/step - loss: 1203.7332 - val_loss: 1136.3818\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 150us/step - loss: 1202.0262 - val_loss: 1137.8344\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 21s 153us/step - loss: 1200.4211 - val_loss: 1137.0313\n",
      "nfold:4,bag:0 1140.36091231\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 10s 71us/step - loss: 2054.1808 - val_loss: 1204.0409\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1367.9826 - val_loss: 1173.6981\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1344.7759 - val_loss: 1165.7016\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1333.0385 - val_loss: 1159.7903\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1323.2605 - val_loss: 1153.9077\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1319.7076 - val_loss: 1151.7720\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1311.9670 - val_loss: 1150.1462\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1303.7851 - val_loss: 1148.5366\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1294.6338 - val_loss: 1147.1145\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1287.0514 - val_loss: 1146.8507\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1284.5576 - val_loss: 1144.1007\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1275.4531 - val_loss: 1144.9972\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1272.3951 - val_loss: 1141.2337\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 9s 70us/step - loss: 1270.2566 - val_loss: 1139.7757\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 9s 69us/step - loss: 1259.3047 - val_loss: 1142.5793\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1255.6377 - val_loss: 1141.2362\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 9s 68us/step - loss: 1248.2841 - val_loss: 1137.2954\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 9s 67us/step - loss: 1243.5427 - val_loss: 1137.8304\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1237.8600 - val_loss: 1139.6804\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1236.1484 - val_loss: 1139.3426\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1230.3669 - val_loss: 1137.6260\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1226.1797 - val_loss: 1138.5436\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1219.6495 - val_loss: 1137.2288\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1217.6094 - val_loss: 1136.6519\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 9s 66us/step - loss: 1213.7227 - val_loss: 1138.6040\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1208.1847 - val_loss: 1137.1267\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 21s 155us/step - loss: 1202.8756 - val_loss: 1136.1260\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 21s 157us/step - loss: 1202.1555 - val_loss: 1138.0743\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 20s 148us/step - loss: 1197.8985 - val_loss: 1137.1804\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1193.2836 - val_loss: 1136.4231\n",
      "nfold:4,bag:1 1140.81920164\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 2138.2942 - val_loss: 1200.7574\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1390.6338 - val_loss: 1179.8795\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1366.2077 - val_loss: 1163.1731\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1352.2279 - val_loss: 1156.9487\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1343.3088 - val_loss: 1154.3014\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1332.0886 - val_loss: 1149.6162\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1324.0786 - val_loss: 1148.2711\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1317.4758 - val_loss: 1147.1855\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1307.9662 - val_loss: 1152.1301\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1299.8916 - val_loss: 1145.7879\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1298.2255 - val_loss: 1145.0337\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1287.1592 - val_loss: 1144.8546\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1279.1264 - val_loss: 1141.7800\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1277.2404 - val_loss: 1139.9090\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1270.0929 - val_loss: 1141.5153\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1267.4115 - val_loss: 1140.7483\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1255.2487 - val_loss: 1139.8336\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1254.2368 - val_loss: 1138.5331\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1249.2250 - val_loss: 1137.2690\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1241.2974 - val_loss: 1138.4109\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1237.7420 - val_loss: 1136.6278\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1233.8170 - val_loss: 1136.9437\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1227.6695 - val_loss: 1137.9927\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1219.5457 - val_loss: 1138.4689\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1216.8169 - val_loss: 1135.4622\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1213.5844 - val_loss: 1137.4071\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1208.8990 - val_loss: 1138.1771\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1204.3518 - val_loss: 1136.4392\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1198.3823 - val_loss: 1137.9975\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1193.0377 - val_loss: 1139.5586\n",
      "nfold:4,bag:2 1141.99502434\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 2218.9547 - val_loss: 1219.2939\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1384.8425 - val_loss: 1180.7617\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1355.0093 - val_loss: 1164.2948\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1342.1491 - val_loss: 1158.6799\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1329.1686 - val_loss: 1157.8362\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1324.6896 - val_loss: 1148.3340\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 78us/step - loss: 1313.4035 - val_loss: 1149.2092\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1309.2497 - val_loss: 1148.3051\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1297.4205 - val_loss: 1145.7402\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1296.2119 - val_loss: 1144.4402\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1283.3957 - val_loss: 1141.7426\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1282.9928 - val_loss: 1142.0431\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1275.7651 - val_loss: 1140.1688\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 10s 77us/step - loss: 1269.1727 - val_loss: 1140.4012\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1262.2065 - val_loss: 1141.2832\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1257.9831 - val_loss: 1139.8339\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1251.3456 - val_loss: 1139.8348\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1250.5163 - val_loss: 1137.2441\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1240.3094 - val_loss: 1137.9094\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1233.9740 - val_loss: 1139.5277\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1231.2653 - val_loss: 1137.3140\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1230.1510 - val_loss: 1137.6990\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1222.5666 - val_loss: 1138.1346\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1220.0464 - val_loss: 1137.0534\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1215.3660 - val_loss: 1137.3911\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 97us/step - loss: 1207.9025 - val_loss: 1138.4680\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 23s 170us/step - loss: 1205.2217 - val_loss: 1136.9816\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1200.5326 - val_loss: 1137.4154\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1199.8706 - val_loss: 1137.6293\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 181us/step - loss: 1195.9002 - val_loss: 1137.6530\n",
      "nfold:4,bag:3 1143.64955181\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2105.3032 - val_loss: 1193.0732\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1356.4940 - val_loss: 1171.9890\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1339.7970 - val_loss: 1162.3293\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1331.4506 - val_loss: 1157.7277\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1319.6571 - val_loss: 1159.0056\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1312.3196 - val_loss: 1150.3308\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1306.6708 - val_loss: 1148.8084\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1300.4548 - val_loss: 1148.1426\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1294.5325 - val_loss: 1145.7598\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1286.6378 - val_loss: 1145.1673\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1283.7501 - val_loss: 1142.4821\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1273.1571 - val_loss: 1140.2331\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1272.8264 - val_loss: 1141.6308\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1267.6393 - val_loss: 1141.1219\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1262.3216 - val_loss: 1140.9662\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1256.8943 - val_loss: 1139.8006\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1253.0903 - val_loss: 1140.8294\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1242.2604 - val_loss: 1141.3136\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1242.2374 - val_loss: 1139.1762\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1238.1381 - val_loss: 1139.7065\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1231.2898 - val_loss: 1138.8321\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1231.6487 - val_loss: 1139.5906\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1222.2808 - val_loss: 1139.1674\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1221.3913 - val_loss: 1139.3760\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1214.0012 - val_loss: 1137.2998\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 99us/step - loss: 1210.0649 - val_loss: 1136.0813\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1205.7820 - val_loss: 1137.8690\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 184us/step - loss: 1205.2301 - val_loss: 1138.4011\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1200.8675 - val_loss: 1138.0939\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1196.0249 - val_loss: 1138.3447\n",
      "nfold:4,bag:4 1142.24720086\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 2476.4447 - val_loss: 1369.6813\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1302.5132 - val_loss: 1175.4207\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1273.5040 - val_loss: 1163.9233\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1263.5625 - val_loss: 1159.3798\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1256.0922 - val_loss: 1153.5629\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1253.4267 - val_loss: 1151.5363\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1245.4631 - val_loss: 1150.7145\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1241.2981 - val_loss: 1148.7997\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1235.3805 - val_loss: 1146.2523\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1231.9319 - val_loss: 1145.0762\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1224.8852 - val_loss: 1144.8017\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1221.6639 - val_loss: 1142.2424\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1219.2797 - val_loss: 1144.5664\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1210.8739 - val_loss: 1140.0517\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1213.4366 - val_loss: 1141.7421\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1211.0422 - val_loss: 1140.7247\n",
      "Epoch 17/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 82us/step - loss: 1205.8469 - val_loss: 1141.4835\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1201.5903 - val_loss: 1140.8480\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1195.9272 - val_loss: 1138.4970\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1197.7755 - val_loss: 1137.9692\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1193.7658 - val_loss: 1138.1587\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1186.7020 - val_loss: 1142.5055\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1188.4498 - val_loss: 1138.7318\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1182.7282 - val_loss: 1137.6618\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1182.4179 - val_loss: 1137.8911\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 96us/step - loss: 1180.4375 - val_loss: 1138.2507\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 173us/step - loss: 1178.0020 - val_loss: 1140.1346\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 174us/step - loss: 1174.3421 - val_loss: 1139.7724\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 23s 171us/step - loss: 1169.2341 - val_loss: 1138.5756\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 175us/step - loss: 1167.0954 - val_loss: 1138.6031\n",
      "nfold:5,bag:0 1148.0059914\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 2232.9710 - val_loss: 1205.4736\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1356.5762 - val_loss: 1188.9745\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1336.7508 - val_loss: 1167.8115\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1328.8452 - val_loss: 1161.2750\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1312.9423 - val_loss: 1157.7835\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1308.2469 - val_loss: 1158.9653\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1302.3563 - val_loss: 1152.7743\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1295.8173 - val_loss: 1149.2877\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1289.4679 - val_loss: 1148.1522\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1285.2962 - val_loss: 1146.4281\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1278.0589 - val_loss: 1146.3249\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1272.5482 - val_loss: 1145.1705\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1269.8250 - val_loss: 1146.2588\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1261.8050 - val_loss: 1143.9241\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1255.9981 - val_loss: 1144.4880\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1250.9236 - val_loss: 1142.9158\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1245.0542 - val_loss: 1142.1374\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1244.0213 - val_loss: 1140.7784\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1240.5693 - val_loss: 1141.5085\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1234.8117 - val_loss: 1141.9566\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1229.7556 - val_loss: 1142.7542\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1229.0457 - val_loss: 1141.3451\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1221.8181 - val_loss: 1140.6553\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 85us/step - loss: 1213.3596 - val_loss: 1140.6715\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1216.8004 - val_loss: 1141.6574\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 100us/step - loss: 1209.5042 - val_loss: 1138.9729\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 178us/step - loss: 1205.7060 - val_loss: 1141.2892\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 176us/step - loss: 1203.5445 - val_loss: 1138.6710\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 177us/step - loss: 1195.0564 - val_loss: 1140.5329\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1194.6568 - val_loss: 1139.6925\n",
      "nfold:5,bag:1 1147.49031014\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 2411.9253 - val_loss: 1267.0195\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1341.3298 - val_loss: 1172.3231\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1313.1925 - val_loss: 1164.4275\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1296.4630 - val_loss: 1158.0961\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1288.9478 - val_loss: 1154.5268\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1279.6983 - val_loss: 1154.1067\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1275.3474 - val_loss: 1152.1374\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1270.9100 - val_loss: 1148.9804\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1266.5685 - val_loss: 1147.4948\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1258.2350 - val_loss: 1147.3822\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1254.3436 - val_loss: 1144.6159\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1248.3689 - val_loss: 1145.3787\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1244.1113 - val_loss: 1142.6066\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1238.9680 - val_loss: 1145.8750\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1235.9132 - val_loss: 1142.3674\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1228.9954 - val_loss: 1143.2786\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1223.5108 - val_loss: 1141.3431\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1223.5601 - val_loss: 1141.1060\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1219.0638 - val_loss: 1142.5440\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1212.9409 - val_loss: 1140.5908\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1208.9261 - val_loss: 1141.1314\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1208.2679 - val_loss: 1141.6649\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1204.9238 - val_loss: 1139.2857\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1197.1107 - val_loss: 1145.6299\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1197.9065 - val_loss: 1139.3660\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 13s 98us/step - loss: 1190.6919 - val_loss: 1140.3950\n",
      "Epoch 27/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 24s 176us/step - loss: 1189.2761 - val_loss: 1138.3787\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1186.9847 - val_loss: 1138.0863\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 24s 180us/step - loss: 1181.8697 - val_loss: 1138.2355\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1181.3090 - val_loss: 1137.6052\n",
      "nfold:5,bag:2 1145.49488199\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 2187.7080 - val_loss: 1230.5975\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1412.0783 - val_loss: 1189.0657\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1377.3224 - val_loss: 1181.1255\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1365.6111 - val_loss: 1163.9113\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1353.5689 - val_loss: 1162.5395\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1344.7690 - val_loss: 1158.2873\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1331.8830 - val_loss: 1152.9102\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1323.2641 - val_loss: 1152.6543\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1315.2524 - val_loss: 1151.0236\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1305.0023 - val_loss: 1148.9827\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1299.6383 - val_loss: 1152.2551\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1290.9872 - val_loss: 1148.6668\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1285.2008 - val_loss: 1145.2800\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1279.8372 - val_loss: 1143.2246\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1275.4489 - val_loss: 1144.5944\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1264.1882 - val_loss: 1144.9872\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1255.9944 - val_loss: 1142.5049\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1256.7052 - val_loss: 1143.0481\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1247.3854 - val_loss: 1144.4871\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1239.0668 - val_loss: 1142.9995\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1235.0126 - val_loss: 1142.6481\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1231.9588 - val_loss: 1140.3748\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1227.3131 - val_loss: 1140.6284\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1219.2892 - val_loss: 1142.8531\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1218.5516 - val_loss: 1141.2558\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 101us/step - loss: 1211.6245 - val_loss: 1139.1579\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 24s 179us/step - loss: 1208.8055 - val_loss: 1137.9244\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1200.3487 - val_loss: 1140.4890\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 182us/step - loss: 1197.3994 - val_loss: 1137.7234\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1196.3851 - val_loss: 1140.7776\n",
      "nfold:5,bag:3 1145.07605618\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 85us/step - loss: 2339.6899 - val_loss: 1258.3517\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1351.0766 - val_loss: 1180.4091\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1313.4907 - val_loss: 1165.8924\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1300.2182 - val_loss: 1159.4280\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1291.6587 - val_loss: 1157.7321\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1279.5623 - val_loss: 1155.5282\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1274.8579 - val_loss: 1151.3626\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1272.3366 - val_loss: 1150.2181\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1259.6059 - val_loss: 1150.5851\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1257.6445 - val_loss: 1148.9227\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1252.3943 - val_loss: 1145.5578\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1248.5108 - val_loss: 1144.5586\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1241.4023 - val_loss: 1146.9502\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1237.8498 - val_loss: 1143.2145\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1235.6165 - val_loss: 1141.1969\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1226.4123 - val_loss: 1141.3572\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1226.8272 - val_loss: 1142.1494\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1219.7794 - val_loss: 1140.0750\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1217.3670 - val_loss: 1141.6489\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1212.1117 - val_loss: 1140.7125\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1210.5117 - val_loss: 1139.2309\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1205.5215 - val_loss: 1140.5817\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1207.4813 - val_loss: 1139.4861\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 1197.0585 - val_loss: 1138.5698\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1195.1549 - val_loss: 1140.7372\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 108us/step - loss: 1190.3727 - val_loss: 1137.7387\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1186.6443 - val_loss: 1137.9590\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1186.0039 - val_loss: 1139.1139\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1178.4497 - val_loss: 1140.4341\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1180.7635 - val_loss: 1139.5635\n",
      "nfold:5,bag:4 1146.75244\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 86us/step - loss: 2306.5931 - val_loss: 1229.2673\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1329.0819 - val_loss: 1170.8375\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1301.4151 - val_loss: 1164.4805\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1292.9470 - val_loss: 1155.9756\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1286.2568 - val_loss: 1155.9960\n",
      "Epoch 6/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 81us/step - loss: 1279.3624 - val_loss: 1150.7441\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1271.7166 - val_loss: 1150.1787\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1262.2729 - val_loss: 1147.5143\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1261.4768 - val_loss: 1146.8506\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1254.7070 - val_loss: 1145.7579\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1251.0229 - val_loss: 1143.6553\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1246.3364 - val_loss: 1144.6518\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1242.4588 - val_loss: 1140.6562\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1238.9695 - val_loss: 1144.7025\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1237.0304 - val_loss: 1140.3904\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1228.2977 - val_loss: 1141.6786\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1225.7953 - val_loss: 1140.8085\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1222.7379 - val_loss: 1140.9120\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1216.1761 - val_loss: 1138.6267\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1213.1601 - val_loss: 1139.0625\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1208.8601 - val_loss: 1139.5632\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1210.2966 - val_loss: 1140.4749\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1204.5682 - val_loss: 1141.3523\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1203.3333 - val_loss: 1140.8067\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1196.7207 - val_loss: 1138.1067\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 102us/step - loss: 1195.2443 - val_loss: 1137.7232\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 183us/step - loss: 1194.1375 - val_loss: 1138.1229\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1188.9101 - val_loss: 1139.5400\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 185us/step - loss: 1184.2143 - val_loss: 1139.5657\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 181us/step - loss: 1182.1226 - val_loss: 1137.8921\n",
      "nfold:6,bag:0 1139.13335583\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 2084.5636 - val_loss: 1187.6613\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1321.6720 - val_loss: 1170.9928\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1303.4705 - val_loss: 1162.4690\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1295.6330 - val_loss: 1159.2231\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1287.2570 - val_loss: 1156.5175\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1280.5441 - val_loss: 1152.2390\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1278.9678 - val_loss: 1148.0026\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1268.8095 - val_loss: 1148.5316\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1262.4005 - val_loss: 1147.6380\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1261.1020 - val_loss: 1146.7801\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1258.5947 - val_loss: 1143.8017\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1252.1159 - val_loss: 1143.4379\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1248.7721 - val_loss: 1142.7260\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1241.5044 - val_loss: 1141.1975\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1240.8888 - val_loss: 1142.2161\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1233.7329 - val_loss: 1139.7243\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1228.7492 - val_loss: 1140.2452\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1227.0069 - val_loss: 1138.1763\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1224.2671 - val_loss: 1139.3625\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1216.6260 - val_loss: 1139.4169\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1214.3368 - val_loss: 1138.7507\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1213.0771 - val_loss: 1138.4429\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1209.7503 - val_loss: 1140.1290\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1205.2553 - val_loss: 1138.5507\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1200.7643 - val_loss: 1139.3404\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 102us/step - loss: 1196.9196 - val_loss: 1139.3962\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1192.8911 - val_loss: 1136.9820\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1195.2483 - val_loss: 1137.1039\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1190.6136 - val_loss: 1137.5271\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1185.5279 - val_loss: 1137.4903\n",
      "nfold:6,bag:1 1142.10979126\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 88us/step - loss: 2235.2946 - val_loss: 1298.2119\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1417.5786 - val_loss: 1186.1756\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1357.7907 - val_loss: 1169.3915\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1344.8769 - val_loss: 1163.7025\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1334.6057 - val_loss: 1163.0087\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1318.7482 - val_loss: 1155.8967\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1308.5565 - val_loss: 1153.8242\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1310.2310 - val_loss: 1153.2054\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1297.2967 - val_loss: 1148.4074\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1287.3594 - val_loss: 1145.8640\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1280.2648 - val_loss: 1147.1823\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1276.6587 - val_loss: 1143.6352\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1269.6613 - val_loss: 1146.2033\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1263.6840 - val_loss: 1143.8419\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1254.7346 - val_loss: 1143.3040\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 81us/step - loss: 1250.1279 - val_loss: 1141.7251\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1243.7255 - val_loss: 1140.0821\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1238.3435 - val_loss: 1142.7404\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1235.2586 - val_loss: 1140.0189\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1227.7351 - val_loss: 1140.9628\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1225.3220 - val_loss: 1139.2282\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1220.8592 - val_loss: 1138.4278\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1214.6448 - val_loss: 1140.2056\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1212.3499 - val_loss: 1140.3331\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1205.5794 - val_loss: 1144.3409\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 103us/step - loss: 1202.3028 - val_loss: 1138.4069\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1202.4887 - val_loss: 1140.1026\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1190.4889 - val_loss: 1138.9983\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1188.4717 - val_loss: 1138.9174\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1187.9180 - val_loss: 1139.1236\n",
      "nfold:6,bag:2 1140.36597016\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2246.5786 - val_loss: 1247.1781\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1382.2431 - val_loss: 1178.8470\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1346.1072 - val_loss: 1173.6282\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1329.4276 - val_loss: 1162.2840\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1319.8153 - val_loss: 1158.4338\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1309.4050 - val_loss: 1160.7957\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1302.7386 - val_loss: 1154.2292\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1300.2906 - val_loss: 1151.4569\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1292.7977 - val_loss: 1148.9709\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1285.8089 - val_loss: 1147.9642\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1280.2512 - val_loss: 1146.8848\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1271.4470 - val_loss: 1147.9738\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1265.1817 - val_loss: 1144.6407\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1260.3203 - val_loss: 1145.9870\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1251.5176 - val_loss: 1143.5880\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1245.4563 - val_loss: 1141.7295\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1243.4445 - val_loss: 1143.6432\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1237.5669 - val_loss: 1140.9362\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1231.2037 - val_loss: 1141.6904\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1227.9461 - val_loss: 1140.7842\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1224.7504 - val_loss: 1141.0032\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1219.6990 - val_loss: 1138.9696\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1214.1166 - val_loss: 1139.2986\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1212.2990 - val_loss: 1139.0104\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1205.9613 - val_loss: 1139.1898\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 104us/step - loss: 1203.9451 - val_loss: 1139.5191\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 188us/step - loss: 1197.4548 - val_loss: 1139.3723\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1195.2661 - val_loss: 1139.5127\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 188us/step - loss: 1187.8018 - val_loss: 1139.2930\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1188.3433 - val_loss: 1139.7240\n",
      "nfold:6,bag:3 1143.55431182\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2180.7681 - val_loss: 1229.2349\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1378.6798 - val_loss: 1177.4516\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1336.7347 - val_loss: 1170.1468\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1319.7405 - val_loss: 1164.7545\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1314.6580 - val_loss: 1156.8083\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1309.1113 - val_loss: 1153.5570\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1297.6480 - val_loss: 1151.8324\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1290.5620 - val_loss: 1149.3168\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1285.2459 - val_loss: 1147.8435\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1281.5939 - val_loss: 1145.6448\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1269.1572 - val_loss: 1147.5781\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1266.5354 - val_loss: 1145.1576\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1258.6283 - val_loss: 1142.4768\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1254.9878 - val_loss: 1143.2681\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1246.8868 - val_loss: 1144.5727\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1245.3560 - val_loss: 1140.6996\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1240.1915 - val_loss: 1140.4286\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1232.9418 - val_loss: 1141.1630\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1230.6336 - val_loss: 1138.8551\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1228.5423 - val_loss: 1141.5743\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1221.7374 - val_loss: 1139.0285\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1215.9567 - val_loss: 1139.9329\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1212.4781 - val_loss: 1138.3545\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1208.1945 - val_loss: 1139.6932\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1207.0981 - val_loss: 1138.4377\n",
      "Epoch 26/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 15s 109us/step - loss: 1203.9792 - val_loss: 1137.4741\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1200.5760 - val_loss: 1139.2124\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1197.0921 - val_loss: 1139.2643\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 189us/step - loss: 1190.3663 - val_loss: 1138.9563\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 190us/step - loss: 1189.2519 - val_loss: 1140.7162\n",
      "nfold:6,bag:4 1139.92859445\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2192.7391 - val_loss: 1214.3665\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1340.5344 - val_loss: 1170.1007\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1322.1489 - val_loss: 1163.6079\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1309.1184 - val_loss: 1159.0042\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1302.3425 - val_loss: 1152.8537\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1294.6973 - val_loss: 1149.4418\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1290.4514 - val_loss: 1149.3897\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1282.0221 - val_loss: 1147.7392\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1275.6314 - val_loss: 1142.9443\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1267.7172 - val_loss: 1143.0604\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1266.0980 - val_loss: 1142.3447\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1263.2337 - val_loss: 1140.8492\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1256.5789 - val_loss: 1142.7467\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1252.3304 - val_loss: 1138.9239\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1245.5910 - val_loss: 1139.7363\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1240.9993 - val_loss: 1136.3309\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1234.4673 - val_loss: 1135.7343\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1233.1209 - val_loss: 1137.5176\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1232.5097 - val_loss: 1134.6448\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1226.4204 - val_loss: 1136.6128\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1224.1394 - val_loss: 1136.8146\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1216.9274 - val_loss: 1136.5463\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1213.2986 - val_loss: 1135.9016\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1212.8230 - val_loss: 1134.6726\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1209.0060 - val_loss: 1135.2277\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 112us/step - loss: 1204.6161 - val_loss: 1136.8373\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1200.8582 - val_loss: 1137.1905\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1196.6132 - val_loss: 1136.9103\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1194.9532 - val_loss: 1135.3292\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1192.0203 - val_loss: 1135.5158\n",
      "nfold:7,bag:0 1139.56780868\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 13s 92us/step - loss: 2143.0990 - val_loss: 1228.0617\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1434.7425 - val_loss: 1200.4084\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1400.1678 - val_loss: 1176.6370\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1384.8464 - val_loss: 1167.8145\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1371.1731 - val_loss: 1155.7117\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1361.8235 - val_loss: 1153.4243\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1347.8681 - val_loss: 1150.4878\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1338.9209 - val_loss: 1149.7062\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1332.6414 - val_loss: 1147.6607\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1319.0165 - val_loss: 1146.1412\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1308.8197 - val_loss: 1147.6049\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1300.7416 - val_loss: 1144.5367\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1294.2523 - val_loss: 1142.1492\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1285.1659 - val_loss: 1139.4820\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1276.0739 - val_loss: 1140.5491\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1274.2714 - val_loss: 1136.9043\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1262.2426 - val_loss: 1138.3544\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1255.2677 - val_loss: 1138.3959\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1253.2282 - val_loss: 1136.1809\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1245.1562 - val_loss: 1136.8347\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1238.1770 - val_loss: 1134.8324\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1229.6558 - val_loss: 1135.8082\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1225.5369 - val_loss: 1139.3910\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1221.0440 - val_loss: 1135.7279\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1217.5236 - val_loss: 1138.6690\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 106us/step - loss: 1208.8227 - val_loss: 1137.0620\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 195us/step - loss: 1204.0350 - val_loss: 1135.7564\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 193us/step - loss: 1201.0307 - val_loss: 1135.4015\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1198.4692 - val_loss: 1135.9856\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1195.4962 - val_loss: 1135.5932\n",
      "nfold:7,bag:1 1140.0651013\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2080.9869 - val_loss: 1208.5571\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1380.0113 - val_loss: 1178.8251\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1351.8454 - val_loss: 1167.3656\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1343.9087 - val_loss: 1159.0414\n",
      "Epoch 5/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 82us/step - loss: 1331.9566 - val_loss: 1152.8934\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1330.2439 - val_loss: 1154.9952\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1319.9673 - val_loss: 1147.3077\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1311.1457 - val_loss: 1148.1625\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1302.3011 - val_loss: 1144.7601\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1297.2553 - val_loss: 1143.0017\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1291.4754 - val_loss: 1143.4818\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1283.5590 - val_loss: 1142.8951\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1276.4732 - val_loss: 1141.7694\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1274.6441 - val_loss: 1140.6264\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 12s 87us/step - loss: 1266.4252 - val_loss: 1140.5603\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1258.1089 - val_loss: 1138.7416\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1255.8089 - val_loss: 1138.7241\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1245.9317 - val_loss: 1138.3387\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1242.5896 - val_loss: 1138.1984\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1234.7944 - val_loss: 1137.4281\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1233.1444 - val_loss: 1137.6891\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1230.4587 - val_loss: 1136.3955\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1223.8494 - val_loss: 1135.4065\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1215.9014 - val_loss: 1136.0637\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1215.6607 - val_loss: 1136.1596\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 14s 106us/step - loss: 1211.2596 - val_loss: 1136.0817\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1205.1402 - val_loss: 1138.0023\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1201.4942 - val_loss: 1136.6032\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 25s 187us/step - loss: 1200.3003 - val_loss: 1137.6369\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 25s 186us/step - loss: 1193.3476 - val_loss: 1136.6729\n",
      "nfold:7,bag:2 1138.35887588\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 90us/step - loss: 2157.5779 - val_loss: 1218.6904\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1376.7882 - val_loss: 1175.4686\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1345.6862 - val_loss: 1163.2800\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1331.8970 - val_loss: 1162.7439\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1322.9991 - val_loss: 1154.8147\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1315.5449 - val_loss: 1149.9389\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1304.0791 - val_loss: 1149.0465\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1301.3509 - val_loss: 1147.9745\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1292.1658 - val_loss: 1146.7507\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1284.1664 - val_loss: 1143.2349\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1280.9508 - val_loss: 1144.2115\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1271.6489 - val_loss: 1143.0943\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1268.4004 - val_loss: 1141.3338\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1261.8463 - val_loss: 1141.0459\n",
      "Epoch 15/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1258.9857 - val_loss: 1139.2593\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1252.0685 - val_loss: 1138.9412\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1242.7075 - val_loss: 1143.6213\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1240.3910 - val_loss: 1136.0014\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1236.4964 - val_loss: 1137.3170\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1231.2205 - val_loss: 1138.4533\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1227.0577 - val_loss: 1135.5651\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1222.0156 - val_loss: 1137.9609\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1218.4416 - val_loss: 1137.9201\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1213.5208 - val_loss: 1136.1762\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1209.9856 - val_loss: 1136.7216\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 109us/step - loss: 1206.5313 - val_loss: 1137.0524\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1202.2576 - val_loss: 1136.0030\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1201.7733 - val_loss: 1137.6421\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 191us/step - loss: 1193.8877 - val_loss: 1136.2811\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1192.5359 - val_loss: 1137.3436\n",
      "nfold:7,bag:3 1142.23636979\n",
      "Train on 135588 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135588/135588 [==============================] - 12s 89us/step - loss: 2179.3707 - val_loss: 1203.8130\n",
      "Epoch 2/30\n",
      "135588/135588 [==============================] - 11s 83us/step - loss: 1366.7818 - val_loss: 1187.8084\n",
      "Epoch 3/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1340.9651 - val_loss: 1163.8953\n",
      "Epoch 4/30\n",
      "135588/135588 [==============================] - 11s 84us/step - loss: 1328.4241 - val_loss: 1159.4387\n",
      "Epoch 5/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1321.7287 - val_loss: 1154.8898\n",
      "Epoch 6/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1308.8842 - val_loss: 1152.6620\n",
      "Epoch 7/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1300.3613 - val_loss: 1149.8190\n",
      "Epoch 8/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1294.9438 - val_loss: 1148.6610\n",
      "Epoch 9/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1289.9991 - val_loss: 1147.4488\n",
      "Epoch 10/30\n",
      "135588/135588 [==============================] - 11s 82us/step - loss: 1281.2793 - val_loss: 1145.0278\n",
      "Epoch 11/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1279.7808 - val_loss: 1141.4544\n",
      "Epoch 12/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1269.9361 - val_loss: 1141.6843\n",
      "Epoch 13/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1264.5116 - val_loss: 1140.7840\n",
      "Epoch 14/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1258.7409 - val_loss: 1141.9715\n",
      "Epoch 15/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135588/135588 [==============================] - 11s 79us/step - loss: 1252.7494 - val_loss: 1140.3385\n",
      "Epoch 16/30\n",
      "135588/135588 [==============================] - 11s 79us/step - loss: 1251.6632 - val_loss: 1138.7266\n",
      "Epoch 17/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1242.9754 - val_loss: 1137.6287\n",
      "Epoch 18/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1238.5133 - val_loss: 1136.6349\n",
      "Epoch 19/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1234.0214 - val_loss: 1142.3731\n",
      "Epoch 20/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1230.8911 - val_loss: 1137.1442\n",
      "Epoch 21/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1225.0861 - val_loss: 1135.3693\n",
      "Epoch 22/30\n",
      "135588/135588 [==============================] - 11s 78us/step - loss: 1221.2549 - val_loss: 1136.8301\n",
      "Epoch 23/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1217.2063 - val_loss: 1136.3174\n",
      "Epoch 24/30\n",
      "135588/135588 [==============================] - 11s 81us/step - loss: 1210.0577 - val_loss: 1135.6949\n",
      "Epoch 25/30\n",
      "135588/135588 [==============================] - 11s 80us/step - loss: 1210.3948 - val_loss: 1135.3912\n",
      "Epoch 26/30\n",
      "135588/135588 [==============================] - 15s 110us/step - loss: 1204.5910 - val_loss: 1135.1728\n",
      "Epoch 27/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1201.4349 - val_loss: 1133.3950\n",
      "Epoch 28/30\n",
      "135588/135588 [==============================] - 27s 196us/step - loss: 1199.9061 - val_loss: 1139.9488\n",
      "Epoch 29/30\n",
      "135588/135588 [==============================] - 26s 194us/step - loss: 1196.6502 - val_loss: 1135.3583\n",
      "Epoch 30/30\n",
      "135588/135588 [==============================] - 26s 192us/step - loss: 1190.3263 - val_loss: 1136.4918\n",
      "nfold:7,bag:4 1138.46346166\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 89us/step - loss: 2258.4264 - val_loss: 1241.9195\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1415.1573 - val_loss: 1185.7786\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1381.8144 - val_loss: 1175.9696\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1364.9609 - val_loss: 1167.5389\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1348.8889 - val_loss: 1161.7289\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1337.6107 - val_loss: 1158.5226\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1332.6000 - val_loss: 1154.5827\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1324.0416 - val_loss: 1155.6580\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1310.8342 - val_loss: 1151.6655\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1308.4723 - val_loss: 1152.5137\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1301.3617 - val_loss: 1150.3254\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1289.5208 - val_loss: 1148.8344\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1280.9472 - val_loss: 1147.5728\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1275.8453 - val_loss: 1146.7022\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1267.8144 - val_loss: 1144.3214\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1262.1420 - val_loss: 1145.3609\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1258.0728 - val_loss: 1144.0658\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1251.5217 - val_loss: 1144.6548\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1243.6596 - val_loss: 1142.6815\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1241.0137 - val_loss: 1144.8320\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1234.2521 - val_loss: 1141.7040\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1232.8671 - val_loss: 1142.4310\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1222.8768 - val_loss: 1141.4569\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1221.3317 - val_loss: 1142.7099\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1212.6002 - val_loss: 1142.9377\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 15s 111us/step - loss: 1211.4668 - val_loss: 1141.4638\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 27s 201us/step - loss: 1204.1692 - val_loss: 1139.9095\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 27s 203us/step - loss: 1200.9953 - val_loss: 1140.7027\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 27s 199us/step - loss: 1198.6971 - val_loss: 1140.3474\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 27s 197us/step - loss: 1196.0814 - val_loss: 1141.8245\n",
      "nfold:8,bag:0 1128.4579898\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 2116.0506 - val_loss: 1195.8719\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1332.0740 - val_loss: 1173.1476\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1315.8616 - val_loss: 1166.3427\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1301.4626 - val_loss: 1162.8685\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1297.0677 - val_loss: 1157.8189\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1289.5538 - val_loss: 1156.2214\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1285.5200 - val_loss: 1153.1409\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1280.2745 - val_loss: 1153.3785\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1271.1160 - val_loss: 1150.6668\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1269.4373 - val_loss: 1151.6821\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1262.6643 - val_loss: 1148.3148\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1256.6957 - val_loss: 1149.0194\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1256.7161 - val_loss: 1146.0244\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1246.6633 - val_loss: 1142.8569\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1247.8708 - val_loss: 1144.9695\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1242.7077 - val_loss: 1145.2463\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1236.9027 - val_loss: 1143.7099\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1235.7622 - val_loss: 1148.6967\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1228.7987 - val_loss: 1142.4442\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1226.9775 - val_loss: 1142.1562\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1221.4127 - val_loss: 1141.4838\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1220.6762 - val_loss: 1141.2752\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1215.9525 - val_loss: 1142.9342\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1210.6833 - val_loss: 1140.9675\n",
      "Epoch 25/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 11s 84us/step - loss: 1209.2536 - val_loss: 1141.3799\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 15s 111us/step - loss: 1204.2124 - val_loss: 1142.6990\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 27s 202us/step - loss: 1198.0131 - val_loss: 1140.7735\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 27s 199us/step - loss: 1193.8403 - val_loss: 1140.8204\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 28s 203us/step - loss: 1194.1936 - val_loss: 1140.4732\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 26s 195us/step - loss: 1190.3474 - val_loss: 1142.4468\n",
      "nfold:8,bag:1 1130.88282786\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 2327.7298 - val_loss: 1280.7319\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1412.1334 - val_loss: 1188.9560\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1371.8059 - val_loss: 1177.0544\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1351.7960 - val_loss: 1163.4958\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1344.5594 - val_loss: 1163.0169\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1331.8104 - val_loss: 1157.9015\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1325.5308 - val_loss: 1154.5988\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1317.3620 - val_loss: 1155.2598\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1305.6433 - val_loss: 1151.3649\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1298.4586 - val_loss: 1151.7915\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1292.0974 - val_loss: 1151.6983\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1280.6750 - val_loss: 1145.2283\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1275.5032 - val_loss: 1145.6491\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1271.3403 - val_loss: 1148.5974\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1264.0993 - val_loss: 1147.5469\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1262.0349 - val_loss: 1144.1443\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1252.8111 - val_loss: 1143.8279\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1248.0683 - val_loss: 1142.5809\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1242.4325 - val_loss: 1142.7872\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1233.7128 - val_loss: 1139.9792\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1234.6855 - val_loss: 1142.0700\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1227.2917 - val_loss: 1143.8656\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1224.2537 - val_loss: 1141.4180\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1215.0715 - val_loss: 1141.7704\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1212.7686 - val_loss: 1142.0477\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 15s 113us/step - loss: 1208.7939 - val_loss: 1142.0501\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 27s 200us/step - loss: 1204.6470 - val_loss: 1141.4387\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 27s 196us/step - loss: 1198.9786 - val_loss: 1144.2746\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 28s 209us/step - loss: 1198.3434 - val_loss: 1142.0625\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 28s 208us/step - loss: 1191.5206 - val_loss: 1142.9540\n",
      "nfold:8,bag:2 1128.84204215\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 2149.5981 - val_loss: 1239.0904\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1397.1707 - val_loss: 1185.4984\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1374.0164 - val_loss: 1171.6533\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1355.3071 - val_loss: 1168.4441\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1349.5123 - val_loss: 1165.5507\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1339.8397 - val_loss: 1160.0828\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1325.6705 - val_loss: 1158.0263\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1324.4432 - val_loss: 1154.7228\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1315.3951 - val_loss: 1152.9905\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1306.1336 - val_loss: 1152.1350\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1297.7391 - val_loss: 1150.5049\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1287.0501 - val_loss: 1149.2836\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1287.6084 - val_loss: 1150.5246\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1278.3548 - val_loss: 1147.0262\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1272.1183 - val_loss: 1145.9675\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1267.6653 - val_loss: 1146.0793\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1257.2967 - val_loss: 1148.1076\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1255.7365 - val_loss: 1145.6034\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1247.1905 - val_loss: 1144.8091\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1244.0767 - val_loss: 1145.0446\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1237.8403 - val_loss: 1143.7753\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1237.0472 - val_loss: 1143.3260\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1232.8290 - val_loss: 1143.5342\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1226.9636 - val_loss: 1142.7396\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1219.3826 - val_loss: 1142.2337\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 15s 113us/step - loss: 1215.0765 - val_loss: 1143.8249\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 27s 203us/step - loss: 1211.6500 - val_loss: 1142.0493\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 27s 202us/step - loss: 1206.1537 - val_loss: 1147.6937\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 27s 202us/step - loss: 1202.7889 - val_loss: 1140.8818\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 28s 204us/step - loss: 1200.4222 - val_loss: 1141.1996\n",
      "nfold:8,bag:3 1131.04165692\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 2265.1180 - val_loss: 1287.4720\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1438.8679 - val_loss: 1200.2460\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1402.7645 - val_loss: 1178.5542\n",
      "Epoch 4/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 11s 81us/step - loss: 1379.6914 - val_loss: 1169.2786\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1364.2350 - val_loss: 1164.8502\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1356.1164 - val_loss: 1160.4845\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1342.7500 - val_loss: 1155.5586\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1334.1648 - val_loss: 1152.3382\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1329.0797 - val_loss: 1150.6409\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1315.7353 - val_loss: 1150.3249\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1308.3846 - val_loss: 1148.4464\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1295.5100 - val_loss: 1148.9447\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1294.6578 - val_loss: 1146.7859\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1283.3411 - val_loss: 1145.7817\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1276.8124 - val_loss: 1145.6142\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1269.0134 - val_loss: 1146.5318\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1265.0683 - val_loss: 1144.6093\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1257.0942 - val_loss: 1144.7143\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1249.3867 - val_loss: 1144.3116\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1243.0488 - val_loss: 1142.9240\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 79us/step - loss: 1235.0625 - val_loss: 1142.9103\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1230.4647 - val_loss: 1144.5537\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1226.8299 - val_loss: 1141.8201\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1218.0879 - val_loss: 1140.9329\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1215.0702 - val_loss: 1142.2072\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 15s 114us/step - loss: 1210.2283 - val_loss: 1142.9628\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 27s 203us/step - loss: 1206.4327 - val_loss: 1142.2897\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 28s 204us/step - loss: 1200.8195 - val_loss: 1141.2786\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 28s 205us/step - loss: 1199.0603 - val_loss: 1145.2749\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 28s 204us/step - loss: 1196.5604 - val_loss: 1141.8452\n",
      "nfold:8,bag:4 1131.93571193\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 90us/step - loss: 2293.1668 - val_loss: 1239.7878\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1366.1452 - val_loss: 1175.9139\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1331.5666 - val_loss: 1169.0329\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1317.8804 - val_loss: 1165.4157\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1305.1629 - val_loss: 1158.9104\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1302.7516 - val_loss: 1158.2201\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1290.0420 - val_loss: 1157.5987\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1286.1339 - val_loss: 1151.2193\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1276.7802 - val_loss: 1149.7796\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1270.6456 - val_loss: 1150.7048\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1266.5651 - val_loss: 1148.2319\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1262.7168 - val_loss: 1147.7950\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1253.2318 - val_loss: 1147.3964\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1252.8426 - val_loss: 1147.9166\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1241.3228 - val_loss: 1145.6639\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1240.0264 - val_loss: 1143.7546\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1229.9855 - val_loss: 1144.1286\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1225.3805 - val_loss: 1145.9384\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1225.0442 - val_loss: 1146.5514\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 80us/step - loss: 1223.3964 - val_loss: 1145.0600\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1213.5782 - val_loss: 1144.5266\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1207.5472 - val_loss: 1143.8103\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1212.6936 - val_loss: 1143.0008\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1204.7420 - val_loss: 1143.2078\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1199.2391 - val_loss: 1144.4116\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 115us/step - loss: 1196.4825 - val_loss: 1143.0152\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 28s 208us/step - loss: 1196.5473 - val_loss: 1141.7279\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 28s 208us/step - loss: 1192.0566 - val_loss: 1144.8751\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 28s 206us/step - loss: 1187.0361 - val_loss: 1144.0577\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 29s 212us/step - loss: 1184.0088 - val_loss: 1145.0219\n",
      "nfold:9,bag:0 1141.01640898\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 13s 92us/step - loss: 2261.1467 - val_loss: 1225.4256\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1321.2006 - val_loss: 1180.2239\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1295.6897 - val_loss: 1167.1583\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1281.7384 - val_loss: 1162.3226\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1275.4952 - val_loss: 1160.2380\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1268.3641 - val_loss: 1155.8765\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1261.8795 - val_loss: 1154.9330\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1259.9805 - val_loss: 1153.6344\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1257.9470 - val_loss: 1151.5469\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1247.2481 - val_loss: 1150.7659\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1243.4294 - val_loss: 1147.8600\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1242.8757 - val_loss: 1146.7720\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1234.7304 - val_loss: 1145.7384\n",
      "Epoch 14/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 11s 83us/step - loss: 1230.1191 - val_loss: 1146.2395\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1229.5781 - val_loss: 1148.5024\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1224.9700 - val_loss: 1144.6198\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1218.3857 - val_loss: 1144.4822\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1219.2257 - val_loss: 1145.8661\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1215.4330 - val_loss: 1148.0471\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1208.7457 - val_loss: 1142.8046\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1206.3806 - val_loss: 1143.1983\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1206.7799 - val_loss: 1144.5716\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1203.4133 - val_loss: 1143.4961\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1199.3609 - val_loss: 1143.9107\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1197.1137 - val_loss: 1144.7526\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 119us/step - loss: 1190.5803 - val_loss: 1144.5083\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 28s 206us/step - loss: 1190.6491 - val_loss: 1143.7212\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 28s 207us/step - loss: 1187.1937 - val_loss: 1142.7649\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 28s 204us/step - loss: 1180.4888 - val_loss: 1142.0874\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 28s 204us/step - loss: 1179.3320 - val_loss: 1143.6749\n",
      "nfold:9,bag:1 1141.99755761\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 91us/step - loss: 2161.8744 - val_loss: 1209.4045\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1334.0290 - val_loss: 1172.1214\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1307.8344 - val_loss: 1165.0211\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1297.2572 - val_loss: 1163.3723\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1285.2222 - val_loss: 1159.9703\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 81us/step - loss: 1283.2293 - val_loss: 1158.3510\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1277.3471 - val_loss: 1156.2618\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1274.8547 - val_loss: 1154.5431\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1266.6763 - val_loss: 1152.9778\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1260.8224 - val_loss: 1150.0446\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1255.6498 - val_loss: 1150.7747\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1253.8886 - val_loss: 1148.9712\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1247.1918 - val_loss: 1147.2505\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 85us/step - loss: 1243.4127 - val_loss: 1149.2755\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1238.4956 - val_loss: 1145.4457\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1235.1703 - val_loss: 1147.7255\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1231.5968 - val_loss: 1143.8142\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1227.7047 - val_loss: 1142.9742\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1223.2325 - val_loss: 1145.0938\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1216.3005 - val_loss: 1145.7278\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1212.9304 - val_loss: 1144.9351\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1211.3315 - val_loss: 1146.2992\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1209.7093 - val_loss: 1145.5516\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1206.9740 - val_loss: 1144.4138\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1202.4165 - val_loss: 1144.7126\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 118us/step - loss: 1199.7990 - val_loss: 1143.2312\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 29s 216us/step - loss: 1194.4697 - val_loss: 1142.8584\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 30s 218us/step - loss: 1193.6269 - val_loss: 1142.4354\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 30s 219us/step - loss: 1188.1915 - val_loss: 1142.7306\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 29s 216us/step - loss: 1187.4883 - val_loss: 1145.9182\n",
      "nfold:9,bag:2 1139.91009889\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 13s 94us/step - loss: 2206.1382 - val_loss: 1239.4257\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1378.9129 - val_loss: 1182.6497\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1348.2599 - val_loss: 1170.3135\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1335.6393 - val_loss: 1166.8096\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1327.0197 - val_loss: 1161.5108\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1318.1931 - val_loss: 1157.6919\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1308.5852 - val_loss: 1154.1421\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1301.0342 - val_loss: 1153.7219\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1300.1896 - val_loss: 1153.7889\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1288.5520 - val_loss: 1154.2147\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1284.5014 - val_loss: 1153.1334\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1276.0169 - val_loss: 1148.5831\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1270.6885 - val_loss: 1150.0225\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1263.7505 - val_loss: 1148.6586\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1259.1046 - val_loss: 1146.3044\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1253.8718 - val_loss: 1146.5410\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1250.1115 - val_loss: 1147.0016\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1240.5585 - val_loss: 1143.8302\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1238.9761 - val_loss: 1145.1123\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 12s 85us/step - loss: 1233.4496 - val_loss: 1146.2885\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1226.6331 - val_loss: 1144.4967\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1221.8425 - val_loss: 1143.9560\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1218.0304 - val_loss: 1144.7348\n",
      "Epoch 24/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135589/135589 [==============================] - 11s 83us/step - loss: 1212.6539 - val_loss: 1144.7662\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1209.2545 - val_loss: 1143.2628\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 117us/step - loss: 1206.3184 - val_loss: 1143.6578\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 28s 210us/step - loss: 1202.2124 - val_loss: 1144.2421\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 30s 219us/step - loss: 1197.8196 - val_loss: 1144.5040\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 29s 212us/step - loss: 1194.9373 - val_loss: 1144.1771\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 29s 215us/step - loss: 1192.3850 - val_loss: 1144.3658\n",
      "nfold:9,bag:3 1136.60712289\n",
      "Train on 135589 samples, validate on 33898 samples\n",
      "Epoch 1/30\n",
      "135589/135589 [==============================] - 12s 92us/step - loss: 2160.3822 - val_loss: 1256.4578\n",
      "Epoch 2/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1447.6205 - val_loss: 1190.9979\n",
      "Epoch 3/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1416.2468 - val_loss: 1179.2316\n",
      "Epoch 4/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1398.9940 - val_loss: 1170.2323\n",
      "Epoch 5/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1383.0007 - val_loss: 1163.9492\n",
      "Epoch 6/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1373.4669 - val_loss: 1166.0695\n",
      "Epoch 7/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1358.6938 - val_loss: 1158.6561\n",
      "Epoch 8/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1348.6391 - val_loss: 1158.2595\n",
      "Epoch 9/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1338.2383 - val_loss: 1156.2835\n",
      "Epoch 10/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1328.7194 - val_loss: 1156.1983\n",
      "Epoch 11/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1322.0179 - val_loss: 1153.1777\n",
      "Epoch 12/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1313.0082 - val_loss: 1150.5303\n",
      "Epoch 13/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1297.9580 - val_loss: 1148.6251\n",
      "Epoch 14/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1291.8133 - val_loss: 1151.9082\n",
      "Epoch 15/30\n",
      "135589/135589 [==============================] - 11s 84us/step - loss: 1283.0481 - val_loss: 1148.5678\n",
      "Epoch 16/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1277.7793 - val_loss: 1149.4159\n",
      "Epoch 17/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1270.8096 - val_loss: 1150.0314\n",
      "Epoch 18/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1268.1715 - val_loss: 1147.2741\n",
      "Epoch 19/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1255.8710 - val_loss: 1146.5978\n",
      "Epoch 20/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1248.6017 - val_loss: 1145.9863\n",
      "Epoch 21/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1241.5782 - val_loss: 1146.2627\n",
      "Epoch 22/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1234.8678 - val_loss: 1146.6229\n",
      "Epoch 23/30\n",
      "135589/135589 [==============================] - 11s 82us/step - loss: 1226.9768 - val_loss: 1143.2457\n",
      "Epoch 24/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1222.5400 - val_loss: 1144.7274\n",
      "Epoch 25/30\n",
      "135589/135589 [==============================] - 11s 83us/step - loss: 1221.9292 - val_loss: 1144.0711\n",
      "Epoch 26/30\n",
      "135589/135589 [==============================] - 16s 117us/step - loss: 1212.5028 - val_loss: 1143.1163\n",
      "Epoch 27/30\n",
      "135589/135589 [==============================] - 29s 215us/step - loss: 1208.7210 - val_loss: 1145.9714\n",
      "Epoch 28/30\n",
      "135589/135589 [==============================] - 29s 214us/step - loss: 1207.7530 - val_loss: 1144.7345\n",
      "Epoch 29/30\n",
      "135589/135589 [==============================] - 29s 216us/step - loss: 1201.1913 - val_loss: 1143.4090\n",
      "Epoch 30/30\n",
      "135589/135589 [==============================] - 29s 214us/step - loss: 1194.8557 - val_loss: 1142.6618\n",
      "nfold:9,bag:4 1137.77578745\n",
      "CV score for the final model: [ 1143.03413002  1143.49019949  1142.57934159  1142.91492371  1142.96326332]\n"
     ]
    }
   ],
   "source": [
    "nnmodel={}\n",
    "nnmodel.clear()\n",
    "def cross_validate_mlp(mlp_func, nfolds=10,nbags=5):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nbags,))\n",
    "    stack_train = np.zeros((nbags,len(train_y)))\n",
    "    stack_test = np.zeros((nbags,len(test)))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        for bag in range(nbags):\n",
    "            nnmodel['nn%d',k*10+bag*1] = mlp_func(seed = k*10+bag*1)\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "            fit = nnmodel['nn%d',k*10+bag*1].fit(xtr, ytr, validation_split=0.2, batch_size=128,\n",
    "                          epochs=30, verbose=1, callbacks=[ExponentialMovingAverage()])\n",
    "            pred = nnmodel['nn%d',k*10+bag*1].predict(xte, batch_size=1024)\n",
    "            #score = mean_absolute_error(np.exp(yte), np.exp(pred))\n",
    "            score = mean_absolute_error(yte, pred)\n",
    "            val_scores[bag] += score\n",
    "            #stack_train[bag][test_index] = pred[:,0]\n",
    "            print (\"nfold:{},bag:{}\".format(k,bag),score)\n",
    "    for bag in range(nbags):\n",
    "        val_scores[bag] = val_scores[bag] / float(nfolds)\n",
    "    \n",
    "    \n",
    "    return val_scores\n",
    "\n",
    "cv_score = cross_validate_mlp(hyper_model)\n",
    "print (\"CV score for the final model:\", cv_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though this model is not adapted for mere 30 epochs of training, nor for 3-fold CV (I used 5-fold on Kaggle), even though this is a single unbagged model which has been cross-validated on three folds only, we see a very good score:\n",
    "`CV = 1150` (your score may vary a little).\n",
    "\n",
    "By the way, this single model, bagged, 5-fold CVed, scored 1116.28 on Kaggle LB.\n",
    "\n",
    "As we see, this model is considerably better than any other models we had so far. We now take it as the second part of our final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'l3-nn': {\n",
    "        'predictions': l2_predictions,\n",
    "        'n_bags': 4,\n",
    "        'model': Keras(nn_lr, lambda: {'l1': 1e-5, 'l2': 1e-5, 'n_epoch': 30, 'batch_size': 128, 'optimizer': SGD(3e-2, momentum=0.8, nesterov=True, decay=3e-5), 'callbacks': [ExponentialMovingAverage(save_mv_ave_model=False)]}),\n",
    "    },\n",
    "     'optimizer': SGD(1e-4, momentum=0.9, nesterov=True, decay=5e-5)\n",
    "            'optimizer': SGD(1e-5, momentum=0.9, nesterov=True, decay=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 437, 'hidden2_dropout': 0.40000000000000002, \n",
    " 'hidden2_units': 182, 'hidden3_dropout': 0.23333333333333334, 'hidden3_units': 73, 'optimizer': 'adadelta',\n",
    " 'wdecay': 0.0020300000000000001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_mlp(mlp_func, nfolds=3):\n",
    "    folds = KFold(len(train_y), n_folds=nfolds, shuffle=True, random_state = 31337)\n",
    "    val_scores = np.zeros((nfolds,))\n",
    "    for k,(train_index, test_index) in enumerate(folds):\n",
    "        xtr = train_x[train_index]\n",
    "        ytr = train_y[train_index]\n",
    "        xte = train_x[test_index]\n",
    "        yte = train_y[test_index]\n",
    "        mlp = mlp_func()\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "        fit = mlp.fit(xtr, ytr, validation_split=0.1, batch_size=128, \n",
    "                      epochs=10, verbose=0, callbacks=[early_stopping])\n",
    "        pred = mlp.predict(xte, batch_size=512)\n",
    "        score = mean_absolute_error(np.exp(yte), np.exp(pred))\n",
    "        val_scores[k] += score\n",
    "        print ('Fold {}, MAE: {}'.format(k, score))\n",
    "    avg_score = np.sum(val_scores) / float(nfolds)\n",
    "    print ('{}-fold CV score: {}'.format(nfolds, avg_score))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 463, 'hidden2_dropout': 0.46666666666666667, 'hidden2_units': 155, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 20, 'optimizer': 'adam', 'wdecay': 0.0012891891891891893}\n",
      "Fold 0, MAE: 1664.3154518123333\n",
      "Fold 1, MAE: 1789.6478768193072\n",
      "Fold 2, MAE: 2230.6679431902985\n",
      "3-fold CV score: 1894.877090607313\n",
      "Model Testing: {'hidden1_dropout': 0.50526315789473686, 'hidden1_units': 308, 'hidden2_dropout': 0.20000000000000001, 'hidden2_units': 244, 'hidden3_dropout': 0.41111111111111109, 'hidden3_units': 28, 'optimizer': 'adadelta', 'wdecay': 0.0075522522522522527}\n",
      "Fold 0, MAE: 2663.589775993532\n",
      "Fold 1, MAE: 2550.3793884253055\n",
      "Fold 2, MAE: 2698.309167392798\n",
      "3-fold CV score: 2637.426110603878\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 394, 'hidden2_dropout': 0.23333333333333334, 'hidden2_units': 120, 'hidden3_dropout': 0.18888888888888888, 'hidden3_units': 65, 'optimizer': 'adam', 'wdecay': 0.0076711711711711711}\n",
      "Fold 0, MAE: 1349.8792955749364\n",
      "Fold 1, MAE: 1348.0942437736849\n",
      "Fold 2, MAE: 1365.2535296867036\n",
      "3-fold CV score: 1354.409023011775\n",
      "Model Testing: {'hidden1_dropout': 0.59999999999999998, 'hidden1_units': 541, 'hidden2_dropout': 0.43333333333333335, 'hidden2_units': 258, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 51, 'optimizer': 'adadelta', 'wdecay': 0.0024189189189189188}\n",
      "Fold 0, MAE: 2598.4352573980386\n",
      "Fold 1, MAE: 2447.588075147961\n",
      "Fold 2, MAE: 2721.9736304441863\n",
      "3-fold CV score: 2589.332320996729\n",
      "Model Testing: {'hidden1_dropout': 0.5368421052631579, 'hidden1_units': 472, 'hidden2_dropout': 0.33333333333333337, 'hidden2_units': 217, 'hidden3_dropout': 0.14444444444444446, 'hidden3_units': 71, 'optimizer': 'adadelta', 'wdecay': 0.0055603603603603608}\n",
      "Fold 0, MAE: 1613.5498840747402\n",
      "Fold 1, MAE: 1411.0097020300386\n",
      "Fold 2, MAE: 1711.7215931226579\n",
      "3-fold CV score: 1578.7603930758123\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 429, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 224, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 36, 'optimizer': 'adam', 'wdecay': 0.0053720720720720726}\n",
      "Fold 0, MAE: 1803.0232709901081\n",
      "Fold 1, MAE: 2716.749151946116\n",
      "Fold 2, MAE: 2137.6247824488005\n",
      "3-fold CV score: 2219.1324017950083\n",
      "Model Testing: {'hidden1_dropout': 0.55789473684210522, 'hidden1_units': 308, 'hidden2_dropout': 0.5, 'hidden2_units': 148, 'hidden3_dropout': 0.4555555555555556, 'hidden3_units': 69, 'optimizer': 'adam', 'wdecay': 0.0020225225225225223}\n",
      "Fold 0, MAE: 2290.4482287011933\n",
      "Fold 1, MAE: 1908.4728142537497\n",
      "Fold 2, MAE: 1895.8826351188964\n",
      "3-fold CV score: 2031.6012260246132\n",
      "Model Testing: {'hidden1_dropout': 0.48421052631578948, 'hidden1_units': 481, 'hidden2_dropout': 0.3666666666666667, 'hidden2_units': 127, 'hidden3_dropout': 0.3666666666666667, 'hidden3_units': 28, 'optimizer': 'adam', 'wdecay': 0.0085432432432432422}\n",
      "Fold 0, MAE: 1813.761108549234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-252-f3e899cfd5b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperopt_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-252-f3e899cfd5b2>\u001b[0m in \u001b[0;36mhyperopt_search\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mcv_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcv_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'status'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-251-50776d6fa3b5>\u001b[0m in \u001b[0;36mcross_validate_mlp\u001b[0;34m(mlp_func, nfolds)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         fit = mlp.fit(xtr, ytr, validation_split=0.1, batch_size=128, \n\u001b[0;32m---> 12\u001b[0;31m                       epochs=10, verbose=0, callbacks=[early_stopping])\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_coo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# VERSION 4. Insights:\n",
    "# – why not to test 4-layer architectures?\n",
    "# — we need to introduce new optimizers\n",
    "# — adding batch normalization (https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "# Describing the search space\n",
    "space = {'hidden1_dropout': hp.choice('hidden1_dropout', np.linspace(0.4,0.6,20)),\n",
    "        'hidden2_dropout': hp.choice('hidden2_dropout', np.linspace(0.2,0.5,10)),\n",
    "        'hidden3_dropout': hp.choice('hidden3_dropout', np.linspace(0.1,0.5,10)),\n",
    "         'hidden1_units': hp.choice('hidden1_units', np.linspace(300,550,30,dtype='int32')),\n",
    "         'hidden2_units': hp.choice('hidden2_units', np.linspace(100,300,30,dtype='int32')),\n",
    "         'hidden3_units': hp.choice('hidden3_units', np.linspace(20,80,30,dtype='int32')),\n",
    "         'optimizer': hp.choice('optimizer', ['adadelta','adam']),\n",
    "         'wdecay':hp.choice('wdecay', np.linspace(0.0001,0.01,1000)),\n",
    "        }\n",
    "\n",
    "# Implementing a function to minimize\n",
    "def hyperopt_search(params):\n",
    "    print ('Model Testing:', params)\n",
    "    def mlp_model():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(params['hidden1_units'], input_dim=train_x.shape[1], kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden1_dropout']))\n",
    "        \n",
    "        model.add(Dense(params['hidden2_units'], kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(params['wdecay'])))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden2_dropout']))\n",
    "\n",
    "        model.add(Dense(params['hidden3_units'], kernel_initializer='he_normal',\n",
    "                        kernel_regularizer=l2(params['wdecay']))) \n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dropout(params['hidden3_dropout']))\n",
    "        \n",
    "        model.add(Dense(1, kernel_initializer='he_normal',kernel_regularizer=l2(params['wdecay'])))\n",
    "        model.compile(loss=mae_score,metrics=[mae_score], optimizer=params['optimizer'])\n",
    "        return model\n",
    "    \n",
    "    cv_score = cross_validate_mlp(mlp_model)\n",
    "    return {'loss': cv_score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# UNCOMMENT THE NEXT LINE TO LAUNCH HYPEROPT:\n",
    "best = fmin(hyperopt_search, space, algo=tpe.suggest, max_evals = 100, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
