{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.021724Z",
     "start_time": "2017-11-05T03:46:11.585655Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet import image\n",
    "from mxnet import init\n",
    "from mxnet import nd\n",
    "from mxnet.gluon.data import vision\n",
    "import numpy as np\n",
    "import random\n",
    "import mxnet as mx\n",
    "from netlib import *\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.087726Z",
     "start_time": "2017-11-05T03:46:12.036394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data prepare, reference to http://zh.gluon.ai/chapter_computer-vision/kaggle-gluon-cifar10.html\n",
    "\"\"\"\n",
    "demo = False\n",
    "run_first = False\n",
    "if demo:\n",
    "    import zipfile\n",
    "    for fin in ['train_tiny.zip', 'test_tiny.zip', 'trainLabels.csv.zip']:\n",
    "        with zipfile.ZipFile('../../gluon-tutorials-zh/data/kaggle_cifar10/' + fin, 'r') as zin:\n",
    "            zin.extractall('../../gluon-tutorials-zh/data/kaggle_cifar10/')\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict((int(idx), label)for idx, label in tokens)\n",
    "    labels = set(idx_label.values())\n",
    "    print labels\n",
    "    \n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    num_train_tuning = int(num_train * (1 - valid_ratio))\n",
    "    assert 0 < num_train_tuning < num_train\n",
    "    num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "    label_count = dict()\n",
    "    \n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "    \n",
    "    # copy to create train set and valid set and train_valid set (train/label, valid/label, train_valid/label)\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                  os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label_count.get(label, 0) < num_train_tuning_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                       os.path.join(data_dir, input_dir, 'valid', label))\n",
    "    \n",
    "    # copy to create test set (test/unkown)\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unkown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file), \n",
    "                   os.path.join(data_dir, input_dir, 'test', 'unkown'))\n",
    "    \n",
    "if demo:\n",
    "    data_dir = \"../../gluon-tutorials-zh/data/kaggle_cifar10/\"\n",
    "else:\n",
    "    data_dir = \"/root/Workspace/data/CIFAR10_kaggle/\"\n",
    "label_file = \"trainLabels.csv\"\n",
    "input_dir = 'train_valid_test'\n",
    "valid_ratio = 0.1\n",
    "if demo:\n",
    "    train_dir = \"train_tiny\"\n",
    "    test_dir = \"test_tiny\"\n",
    "    batch_size = 1\n",
    "else:\n",
    "    train_dir = \"train\"\n",
    "    test_dir = \"test\"\n",
    "    batch_size = 32\n",
    "\n",
    "if run_first:\n",
    "    reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:12.804734Z",
     "start_time": "2017-11-05T03:46:12.785031Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data loader\n",
    "\"\"\"\n",
    "data_dir = \"/root/Workspace/data/CIFAR10_kaggle/train_valid_test/\"\n",
    "\n",
    "def _transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def data_loader(batch_size, transform_train, transform_test=None):\n",
    "    if transform_train is None:\n",
    "        transform_train = _transform_train\n",
    "    if transform_test is None:\n",
    "        transform_test = _transform_test\n",
    "        \n",
    "    # flag=1 mean 3 channel image\n",
    "    train_ds = vision.ImageFolderDataset(data_dir + 'train', flag=1, transform=transform_train)\n",
    "    valid_ds = vision.ImageFolderDataset(data_dir + 'valid', flag=1, transform=transform_test)\n",
    "    train_valid_ds = vision.ImageFolderDataset(data_dir + 'train_valid', flag=1, transform=transform_train)\n",
    "    test_ds = vision.ImageFolderDataset(data_dir + \"test\", flag=1, transform=transform_test)\n",
    "\n",
    "    loader = gluon.data.DataLoader\n",
    "    train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "    test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')\n",
    "    return train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:14.408716Z",
     "start_time": "2017-11-05T03:46:14.361070Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data argument\n",
    "\"\"\"\n",
    "def transform_train_DA1(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "\n",
    "def transform_train_DA2(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "    \n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))\n",
    "    \n",
    "\n",
    "random_clip_rate = 0.3\n",
    "def transform_train_DA3(data, label):\n",
    "    im = data.astype(np.float32) / 255\n",
    "    auglist = [image.RandomSizedCropAug(size=(32, 32), min_area=0.49, ratio=(0.5, 2))]\n",
    "    _aug = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, \n",
    "                                rand_crop=False, rand_resize=False, rand_mirror=True,\n",
    "#                                mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "#                                std=np.array([0.2023, 0.1994, 0.2010]),\n",
    "                                brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3,\n",
    "                                pca_noise=0.01, rand_gray=0, inter_method=2)\n",
    "    auglist.append(image.RandomOrderAug(_aug))\n",
    "\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "        \n",
    "    if random.random() > random_clip_rate:\n",
    "        im = im.clip(0, 1)\n",
    "    _aug = image.ColorNormalizeAug(mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                   std=np.array([0.2023, 0.1994, 0.2010]),)\n",
    "    im = _aug(im)\n",
    "    \n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return (im, nd.array([label]).asscalar().astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:15.590464Z",
     "start_time": "2017-11-05T03:46:15.513162Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train\n",
    "\"\"\"\n",
    "import datetime\n",
    "import utils\n",
    "import sys\n",
    "\n",
    "def abs_mean(W):\n",
    "    return nd.mean(nd.abs(W)).asscalar()\n",
    "\n",
    "def in_list(e, l):\n",
    "    for i in l:\n",
    "        if i == e:\n",
    "            return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def train(net, train_data, valid_data, num_epochs, lr, lr_period, \n",
    "          lr_decay, wd, ctx, w_key, output_file=None, verbose=False, loss_f=gluon.loss.SoftmaxCrossEntropyLoss()):\n",
    "    if output_file is None:\n",
    "        output_file = sys.stdout\n",
    "        stdout = sys.stdout\n",
    "    else:\n",
    "        output_file = open(output_file, \"w\")\n",
    "        stdout = sys.stdout\n",
    "        sys.stdout = output_file\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "    prev_time = datetime.datetime.now()\n",
    "    \n",
    "    if verbose:\n",
    "        print \" #\", utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "    \n",
    "    i = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "        if in_list(epoch, lr_period):\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data.as_in_context(ctx))\n",
    "                loss = loss_f(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(data.shape[0])\n",
    "            \n",
    "            _loss = nd.mean(loss).asscalar()\n",
    "            _acc = utils.accuracy(output, label)\n",
    "            train_loss += _loss\n",
    "            train_acc += _acc\n",
    "            \n",
    "            if verbose:\n",
    "                print \" # iter\", i,\n",
    "                print \"loss %.5f\" % _loss, \"acc %.5f\" % _acc,\n",
    "                print \"w (\",\n",
    "                for k in w_key:\n",
    "                    w = net.collect_params()[k]\n",
    "                    print \"%.5f, \" % abs_mean(w.data()),\n",
    "                print \") g (\",\n",
    "                for k in w_key:\n",
    "                    w = net.collect_params()[k]\n",
    "                    print \"%.5f, \" % abs_mean(w.grad()),\n",
    "                print \")\"\n",
    "                i += 1\n",
    "            \n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        \n",
    "        train_loss /= len(train_data)\n",
    "        train_acc /= len(train_data)\n",
    "        \n",
    "        if valid_data is not None:\n",
    "            valid_acc = utils.evaluate_accuracy(valid_data, net, ctx)\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f, valid_acc %.4f\" \n",
    "                         % (epoch, train_loss, train_acc, valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"epoch %d, loss %.5f, train_acc %.4f\"\n",
    "                        % (epoch, train_loss, train_acc))\n",
    "        prev_time = cur_time\n",
    "        output_file.write(epoch_str + \", \" + time_str + \",lr \" + str(trainer.learning_rate) + \"\\n\")\n",
    "        output_file.flush()  # to disk only when flush or close\n",
    "    if output_file != stdout:\n",
    "        sys.stdout = stdout\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### note: if you just want to got a submission result, just ignore Exp1~5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp1: res164_v2 + DA1: 0.9529"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [90, 140]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "\n",
    "net.save_params(\"models/shelock_resnet_orign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp2:res164_v2 + DA2: 0.9527"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA2\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train2)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 300\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/resnet164_e300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp3: res164_v2 + focal loss + DA3: 0.9540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "tranform_train = transform_train_DA3\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = FocalLoss()\n",
    "\n",
    "num_epochs = 255\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_valid_data, None, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/res164__2_e255_focal_clip_all_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp4: res164_v2 + focal loss + DA3 + only train_data: 0.9506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA3\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = ResNet164_v2(10)\n",
    "loss_f = FocalLoss()\n",
    "\n",
    "num_epochs = 255\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [150, 225]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.collect_params().initialize(mx.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "net.hybridize()\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, \n",
    "      lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/resnet164_e0-255_focal_clip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp5: sherlock_densenet: 0.9539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform_train = transform_train_DA1\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(batch_size, transform_train)\n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "loss_f = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "num_epochs = 200\n",
    "learning_rate = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_period = [90, 140]\n",
    "lr_decay=0.1\n",
    "log_file = None\n",
    "\n",
    "net.hybridize()\n",
    "net.initialize(ctx=ctx)\n",
    "w_key = []\n",
    "train(net, train_data, valid_data, num_epochs, learning_rate, lr_period, lr_decay, weight_decay, ctx, w_key, log_file, False, loss_f)\n",
    "net.save_params(\"models/shelock_densenet_orign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T03:46:21.596302Z",
     "start_time": "2017-11-05T03:46:21.241490Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "generate CIFAR10 output result\n",
    "\"\"\"\n",
    "\n",
    "def save_net_result(net, filename, test_data, ctx):\n",
    "    output = nd.zeros(shape=(300000, 10), ctx=ctx)\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        output[i*batch_size:i*batch_size+data.shape[0],:] = net(data.as_in_context(ctx))\n",
    "    nd.save(filename, output)\n",
    "\n",
    "def test_net(data):\n",
    "    return data.reshape((data.shape[0], -1))[:, :10]\n",
    "\n",
    "def save_model_result(model_name, ctx):\n",
    "    net.load_params(\"models/\" + model_name, ctx=ctx)\n",
    "    save_net_result(net, \"result/\" + model_name, test_data, ctx)\n",
    "\n",
    "model_list = ['resnet164_e255_focal_clip', 'res164__2_e255_focal_clip_all_data', 'resnet164_e300','resnet164_e0-255_focal_clip',\n",
    "              'res18_9', \n",
    "              'log_shelock_densenet', 'shelock_densenet_orign',\n",
    "              'shelock_resnet_orign']\n",
    "weight_list = [0.9535, 0.9540, 0.95270, 0.95, 0.93230, 0.9346, 0.9539, 0.95]\n",
    "\n",
    "net = ResNet164_v2(10)\n",
    "for model_name in model_list[:4]:\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = ResNet(10)\n",
    "for model_name in model_list[4:5]:\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = DenseNet(growthRate=12, depth=100, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "for model_name in model_list[5:7]:\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)\n",
    "        \n",
    "net = ResNet164_v2(10)\n",
    "for model_name in model_list[7:]:\n",
    "    if not os.path.exists(\"result/\"+model_name):\n",
    "        save_model_result(model_name, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-05T03:47:12.464Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "classfiy test set from generated result\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_data, valid_data, train_valid_data, test_data, test_ds, train_valid_ds = data_loader(128, transform_train_DA1)\n",
    "\n",
    "def mesuare_sum(preds, weight_list=None):\n",
    "    if weight_list is None:\n",
    "        weight_list = [1] * len(preds)\n",
    "    output = preds[0] * weight_list[0]\n",
    "    for i in range(1, len(preds)):\n",
    "        output = output + preds[i] * weight_list[i]\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "def mesuare_softmax_sum(preds, weight_list=None):\n",
    "    if weight_list is None:\n",
    "        weight_list = [1] * len(preds)\n",
    "    output = nd.softmax(preds[0], axis=1) * weight_list[0]\n",
    "    for i in range(1, len(preds)):\n",
    "        output = output + nd.softmax(preds[i], axis=1) * weight_list[i]\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "def mesuare_biggest(preds, weight_list=None):\n",
    "    if weight_list is not None:\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = preds[i] * weight_list[i]\n",
    "    output = nd.concat(*preds, dim=1)\n",
    "    preds = output.argmax(axis=1).astype(int).asnumpy() % 10\n",
    "    return preds\n",
    "\n",
    "model_list = ['res164__2_e255_focal_clip_all_data', 'resnet164_e300', 'resnet164_e0-255_focal_clip',\n",
    "              'shelock_densenet_orign', 'shelock_resnet_orign']\n",
    "weight_list = [0.9540, 0.95270, 0.95, 0.9539, 0.95]\n",
    "#weight_list=None\n",
    "\n",
    "preds = []\n",
    "for result_name in model_list:\n",
    "    preds.append(nd.load(\"result/\"+result_name)[0].as_in_context(ctx))\n",
    "\n",
    "#preds = mesuare_biggest(preds, weight_list)\n",
    "preds = mesuare_sum(preds, weight_list)\n",
    "#preds = mesuare_softmax_sum(preds, weight_list)\n",
    "\n",
    "sorted_ids = list(range(1, 300000 + 1))\n",
    "sorted_ids.sort(key=lambda x: str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission/concat_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "ssap_exp_config": {
   "error_alert": "Error Occurs!",
   "initial": [],
   "max_iteration": 1000,
   "recv_id": "",
   "running": [],
   "summary": [],
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
